# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VVmi0avuVK1007yurc4aqIgcW0x618jH
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
import pandas as pd
import io
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
import os
import shutil
import posixpath
import urllib.request
import datetime
from collections import namedtuple
from sklearn.model_selection import KFold
from xgboost import XGBRegressor
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn import metrics
from matplotlib import pyplot
from numpy import sqrt
from numpy import argmax
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.neural_network import MLPClassifier
from imblearn.pipeline import make_pipeline as make_pipeline_imb
from imblearn.over_sampling import SMOTE
from imblearn.metrics import classification_report_imbalanced
from xgboost import XGBClassifier
import xgboost as xgb
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
import time
from xgboost import plot_importance
from matplotlib import pyplot

from sklearn.feature_selection import SelectFromModel

pd.set_option('display.max_rows', 50000)
pd.set_option('display.max_columns', 50)
pd.set_option('display.width', 1000)
pd.set_option('max_colwidth', 800)

drive.mount('/content/gdrive')

# Download All Time series data for 404 patients.

df_all256_withTemp = pd.read_csv('/content/gdrive/My Drive/Master thesis/df_ts_records_all236SepsisPatients(lessThan20%Missing)_fromSepsisOnset_ShockOnsetPlus10h_or_sepsisOnsetPlus41h_WITH_TEMP_SOFA.csv')
df_all256_withTemp['TIME'] =  pd.to_datetime(df_all256_withTemp['TIME'])
print(df_all256_withTemp.shape)
print(df_all256_withTemp.columns)

#cleaning data :  removing all negative values

df_all256_withTemp.HR = df_all256_withTemp.HR.mask(df_all256_withTemp.HR < 0)

df_all256_withTemp.RESP = df_all256_withTemp.RESP.mask(df_all256_withTemp.RESP < 0)

df_all256_withTemp.ABPSYS = df_all256_withTemp.ABPSYS.mask(df_all256_withTemp.ABPSYS < 0)

df_all256_withTemp.ABPDIAS = df_all256_withTemp.ABPDIAS.mask(df_all256_withTemp.ABPDIAS < 0)

df_all256_withTemp.ABPMEAN = df_all256_withTemp.ABPMEAN.mask(df_all256_withTemp.ABPMEAN < 0)

df_all256_withTemp.SPO2 = df_all256_withTemp.SPO2.mask(df_all256_withTemp.SPO2 < 0)

df_all256_withTemp.TEMP = df_all256_withTemp.TEMP.mask(df_all256_withTemp.TEMP < 0)


df_all256_withTemp.SOFA_SCORE = df_all256_withTemp.SOFA_SCORE.mask(df_all256_withTemp.SOFA_SCORE < 0)


df_all256_withTemp.RESP_SOFA = df_all256_withTemp.RESP_SOFA.mask(df_all256_withTemp.RESP_SOFA < 0)


df_all256_withTemp.LIVER_SOFA = df_all256_withTemp.LIVER_SOFA.mask(df_all256_withTemp.LIVER_SOFA < 0)


df_all256_withTemp.RENAL_SOFA = df_all256_withTemp.RENAL_SOFA.mask(df_all256_withTemp.RENAL_SOFA < 0)


df_all256_withTemp.CARDIO_SOFA = df_all256_withTemp.CARDIO_SOFA.mask(df_all256_withTemp.CARDIO_SOFA < 0)


df_all256_withTemp.CNS_SOFA = df_all256_withTemp.CNS_SOFA.mask(df_all256_withTemp.CNS_SOFA < 0)


df_all256_withTemp.COAG_SOFA = df_all256_withTemp.COAG_SOFA.mask(df_all256_withTemp.COAG_SOFA < 0)

# Missing value imputation by carry forward scheme
df_all256_withTemp_cleaned_MVimputed = df_all256_withTemp.ffill().bfill()

df_all256_withTemp_cleaned_MVimputed.HR = df_all256_withTemp_cleaned_MVimputed.HR.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.RESP = df_all256_withTemp_cleaned_MVimputed.RESP.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.ABPSYS = df_all256_withTemp_cleaned_MVimputed.ABPSYS.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.ABPDIAS = df_all256_withTemp_cleaned_MVimputed.ABPDIAS.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.ABPMEAN = df_all256_withTemp_cleaned_MVimputed.ABPMEAN.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.SPO2 = df_all256_withTemp_cleaned_MVimputed.SPO2.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.TEMP = df_all256_withTemp_cleaned_MVimputed.TEMP.round(decimals=4)


df_all256_withTemp_cleaned_MVimputed.SOFA_SCORE = df_all256_withTemp_cleaned_MVimputed.SOFA_SCORE.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.RENAL_SOFA = df_all256_withTemp_cleaned_MVimputed.RENAL_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.LIVER_SOFA = df_all256_withTemp_cleaned_MVimputed.LIVER_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.COAG_SOFA = df_all256_withTemp_cleaned_MVimputed.COAG_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.CARDIO_SOFA = df_all256_withTemp_cleaned_MVimputed.CARDIO_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.CNS_SOFA = df_all256_withTemp_cleaned_MVimputed.CNS_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.RESP_SOFA = df_all256_withTemp_cleaned_MVimputed.RESP_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed['TIME'] =  pd.to_datetime(df_all256_withTemp_cleaned_MVimputed['TIME'])

print(df_all256_withTemp_cleaned_MVimputed[df_all256_withTemp_cleaned_MVimputed.isnull().any(axis=1)])

subject_ids = df_all256_withTemp_cleaned_MVimputed.SUBJECT_ID.unique()

print(len(subject_ids))


from google.colab import files
uploaded = files.upload()

df_icutime = pd.read_csv(io.BytesIO(uploaded['Only_AllSepsisPatients_with_MissingData_FromSepsisOnset_ToShockOnsetPlus10h_or_SepsisOnset+41h.csv']))
print (df_icutime.columns)
#df_icutime = df_icutime[['subject_id','icustay_id','intime','outtime','sepsis_onsettime']]
#print (df_icutime.columns)


"""
df_icutime['intime'] =  pd.to_datetime(df_icutime['intime'])
df_icutime['outtime'] =  pd.to_datetime(df_icutime['outtime'])
"""

df_test = df_icutime[(df_icutime['Sepsis_SepsisOnset+ShockOnsetOR31h_timeoverlap_exists']== 1) & (df_icutime['Sepsis_SepsisOnset+ShockOnsetOR31h_percentNonMissingData']>=80)]
print(df_test.columns)


#df_test.to_csv('Time_data_for 236_patients.csv')

import seaborn as sns
data_corr = df_all256_withTemp_cleaned_MVimputed[['HR', 'SPO2', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'RESP', 'TEMP','SOFA_SCORE']]
print(data_corr)
corr = data_corr.corr()# calculating the correlation between the above vital signs
sns.heatmap(corr, square=True) # plotting the correlation

import scipy
import collections
from scipy.stats import entropy

feature_cols= ['TIME','ICUSTAY_ID','SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK','HOURS_BETWEEN_SEPSIS_SHOCK','SOFA_START_CURRENT_DIFF',
               'HR',
               'RESP',
               'ABPSYS',
               'ABPDIAS',
               'ABPMEAN',
               'SPO2',
               'TEMP' ,
               'SOFA_SCORE',

               'HR_STD',
               'RESP_STD',
               'ABPSYS_STD',
               'ABPDIAS_STD',
               'ABPMEAN_STD',
               'SPO2_STD',
               'TEMP_STD',
               'SOFA_SCORE_STD',
               
               'HR_ENT',
               'RESP_ENT',
               'ABPSYS_ENT',
               'ABPDIAS_ENT',
               'ABPMEAN_ENT',
               'SPO2_ENT',
               'TEMP_ENT',
               'SOFA_SCORE_ENT',

               'ABPDIAS_HR_CORR',
               'RESP_HR_CORR',
               'ABPDIAS_ABPSYS_CORR',
               'ABPMEAN_ABPSYS_CORR',
               'ABPMEAN_ABPDIAS_CORR',

                'HR_MIN',
                'RESP_MIN',
                'SPO2_MIN',
                'TEMP_MIN',
                'SOFA_SCORE_MIN',
                'ABPSYS_MIN',
                'ABPDIAS_MIN',
                'ABPMEAN_MIN',
                'HR_MAX',
                'RESP_MAX',
                'SPO2_MAX',
                'TEMP_MAX',
                'SOFA_SCORE_MAX',
                'ABPSYS_MAX',
                'ABPDIAS_MAX',
                'ABPMEAN_MAX',

               'HR_DIFF',
               'RESP_DIFF',
               'ABPSYS_DIFF',
               'ABPDIAS_DIFF',
               'ABPMEAN_DIFF',
               'SPO2_DIFF',
               'TEMP_DIFF' ,
               'SOFA_SCORE_DIFF'
               ] 
              



temp_feature_cols= ['TIME',
               'HR',
               'RESP',
               'ABPSYS',
               'ABPDIAS',
               'ABPMEAN',
               'SPO2',
               'TEMP',
               'SOFA_SCORE'
               ] 


try:
  df_features.drop(df_features.index, inplace=True)
except:
  print('df_features does not exists')

df_features  =  pd.DataFrame(columns=feature_cols);

for subject_id in subject_ids: 
  curr_idx = df_features.shape[0]
  

  icustay_id = df_icutime.loc[df_icutime.subject_id==subject_id , 'icustay_id'].values[0]

  icu_intime = df_icutime.loc[df_icutime.subject_id==subject_id , 'intime'].values[0]
  sepsis_onsettime = df_icutime.loc[df_icutime.subject_id==subject_id , 'sepsis_onsettime'].values[0]

  shock_onsetttime = df_icutime.loc[df_icutime.subject_id==subject_id , 'sepstic_shock_onsettime'].values[0]
  print('shock_onsetttime : ', shock_onsetttime)

  if str(shock_onsetttime) == 'nan':
    
    has_shock = 0 ;
    #print('not a shock patient')
    df_tsdata_subjectid_entireTimeBeforeShock = df_all256_withTemp_cleaned_MVimputed[(df_all256_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( ( df_all256_withTemp_cleaned_MVimputed['TIME'] >= datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S')    ) & ( df_all256_withTemp_cleaned_MVimputed['TIME'] <= ( datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=31) ) )  )]
    skip = 31
    #df_tsdata_subjectid_entireTimeBeforeShock = df_all256_withTemp_cleaned_MVimputed[(df_all256_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( ( df_all256_withTemp_cleaned_MVimputed['TIME'] >= (datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=27)   ) ) & ( df_all256_withTemp_cleaned_MVimputed['TIME'] <= ( datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=41) ) )  )]
    #skip = 14

  else:
    has_shock = 1 ;

    datetime_shock = datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S')
    # below added to allocate TS sequences 6 hours before shock as 'shock'
    #datetime_shock_minus_6h = datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S') - datetime.timedelta(hours=7)
    #datetime_shock_minus_4h = datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S') - datetime.timedelta(hours=5)
    #datetime_shock_minus_1h = datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S') - datetime.timedelta(hours=2)
    #

    datetime_shock_date = datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S').date()
    datetime_shock_time_hour  = datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S').time().hour
    df_tsdata_subjectid_entireTimeBeforeShock = df_all256_withTemp_cleaned_MVimputed[(df_all256_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( ( df_all256_withTemp_cleaned_MVimputed['TIME'] >= datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S')    ) & ( df_all256_withTemp_cleaned_MVimputed['TIME'] <= (datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S') ) )  )]
    #df_tsdata_subjectid_entireTimeBeforeShock = df_all256_withTemp_cleaned_MVimputed[(df_all256_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( ( df_all256_withTemp_cleaned_MVimputed['TIME'] >= (datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S') - datetime.timedelta(hours=4) )    ) & ( df_all256_withTemp_cleaned_MVimputed['TIME'] <= (datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=10) ) )  )]
    
    hours_between = ( datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S') ) - datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S') 
    #hours_between = ( datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S')  + datetime.timedelta(hours=10) ) -  ( datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S')  - datetime.timedelta(hours=4) )
    
    duration_in_s = hours_between.total_seconds()
    q, r = divmod(duration_in_s, 3600)
    skip = int(q + int(bool(r)))

  
  
  first_row = df_tsdata_subjectid_entireTimeBeforeShock.iloc[0,:]
  
  base_min = int(first_row['TIME'].strftime('%M') )

  df_tsdata_subjectid_entireTimeBeforeShock = df_tsdata_subjectid_entireTimeBeforeShock[temp_feature_cols]
  df_features_temp =pd.DataFrame(columns=temp_feature_cols)

  
  
  """

  #for mean per hour
  df_features_temp = df_tsdata_subjectid_entireTimeBeforeShock.resample('60min',base=base_min,  on='TIME').mean()
  print('after aggregating mean : ', df_features_temp.shape[0])
  


  # for standard deviation
  df_features_temp_std = df_tsdata_subjectid_entireTimeBeforeShock.resample('60min',base=base_min,  on='TIME').std()

  df_features_temp_std =  df_features_temp_std[['HR','RESP','ABPSYS', 'ABPDIAS','ABPMEAN','SPO2','TEMP', 'SOFA_SCORE']]

  df_features_temp_std.columns = ['HR_STD','RESP_STD','ABPSYS_STD','ABPDIAS_STD','ABPMEAN_STD','SPO2_STD','TEMP_STD','SOFA_SCORE_STD']
  df_features_temp = pd.concat([df_features_temp, df_features_temp_std], axis=1)

  print('after aggregating mean annd std : ', df_features_temp.shape[0])
  print(df_features_temp)

  #print(' df_features_temp after concatenation with std')
  # for calculation of entropy
  """
  #q, r = divmod(df_tsdata_subjectid_entireTimeBeforeShock.shape[0], 60)
  #skip = q + int(bool(r)) # rounds to next greater integer (always ceiling)
  #print(skip)
  """
  skip = df_features_temp.shape[0]
  #print('printing skip : ', skip)

  start_window_idx = 0
  end_window_idx = 60
  """
  #For mean calculation
  hr_mean_list = []
  resp_mean_list = []
  spo2_mean_list = []
  temp_mean_list = []
  sofa_mean_list = []
  abpsys_mean_list = []
  abpdias_mean_list = []
  abpmean_mean_list = []

  hr_mean = '';
  resp_mean = '';
  spo2_mean = '';
  temp_mean = '';
  sofa_mean = '';
  abpsys_mean = '';
  abpdias_mean = '';
  abpmean_mean = '';

  # for std calculation
  hr_std_list = []
  resp_std_list = []
  spo2_std_list = []
  temp_std_list = []
  sofa_std_list = []
  abpsys_std_list = []
  abpdias_std_list = []
  abpmean_std_list = []

  hr_std = '';
  resp_std = '';
  spo2_std = '';
  temp_std = '';
  sofa_std = '';
  abpsys_std = '';
  abpdias_std = '';
  abpmean_std = '';


  #for entropy calculation
  hr_entropy_list=[]
  resp_entropy_list = []
  spo2_entropy_list = []
  temp_entropy_list = []
  sofa_entropy_list = []
  abpsys_entropy_list = []
  abpdias_entropy_list = []
  abpmean_entropy_list = []

  hr_entropy = '';
  resp_entropy = '';
  spo2_entropy = '';
  temp_entropy = '';
  sofa_entropy = '';
  abpsys_entropy = '';
  abpdias_entropy = '';
  abpmean_entropy = '';

  # for correlation calculation
  abpdias_hr_corr_list = [];
  resp_hr_corr_list = [];
  abpdias_abpsys_corr_list = [];
  abpmean_abpsys_corr_list = [];
  abpmean_abpdias_corr_list = [];


  # for min calculation
  hr_min_list=[]
  resp_min_list = []
  spo2_min_list = []
  temp_min_list = []
  sofa_min_list = []
  abpsys_min_list = []
  abpdias_min_list = []
  abpmean_min_list = []

  hr_min = '';
  resp_min = '';
  spo2_min = '';
  temp_min = '';
  sofa_min = '';
  abpsys_min = '';
  abpdias_min = '';
  abpmean_min = '';

  # for max calculation
  hr_max_list=[]
  resp_max_list = []
  spo2_max_list = []
  temp_max_list = []
  sofa_max_list = []
  abpsys_max_list = []
  abpdias_max_list = []
  abpmean_max_list = []

  hr_max = '';
  resp_max = '';
  spo2_max = '';
  temp_max = '';
  sofa_max = '';
  abpsys_max = '';
  abpdias_max = '';
  abpmean_max = '';

  #for time
  time_list = [];
  has_shock_list = [] ;

  sofa_diff_start_list=[];

  init_time = first_row['TIME']
  end_time = first_row['TIME'] + datetime.timedelta(minutes = 60)


  for i in range(skip):
    
    #print('start_window_time : ', init_time)
    #print('end_window_time : ', end_time)

    #df_subset = df_tsdata_subjectid_entireTimeBeforeShock.iloc[start_window_idx:end_window_idx,:]
    df_subset = df_tsdata_subjectid_entireTimeBeforeShock[(df_tsdata_subjectid_entireTimeBeforeShock['TIME'] >= init_time) & (df_tsdata_subjectid_entireTimeBeforeShock['TIME'] < end_time) ]
  
    #if (has_shock == 1 ) & (init_time >= datetime_shock_minus_4h ) : 
    #if (has_shock == 1 ) & ( ( (init_time + datetime.timedelta(hours =1) )  <= datetime_shock )&  ( ( end_time + datetime.timedelta(hours =1) ) >= datetime_shock)) :
    """
    if (has_shock == 1 ) and ( ( (init_time + datetime.timedelta(hours =1) )  <= datetime_shock )&  ( ( end_time + datetime.timedelta(hours =1) ) >= datetime_shock)) :
      time_has_shock = 1
    elif (has_shock == 1 ) and ( ( init_time    <= datetime_shock )&  (  end_time  >= datetime_shock)) :
      time_has_shock = 1
    else:
      time_has_shock = 0
    
    has_shock_list.append(time_has_shock)
    """
    
    hr_mean = '';
    resp_mean = '';
    spo2_mean = '';
    temp_mean = '';
    sofa_mean = '';
    abpsys_mean = '';
    abpdias_mean = '';
    abpmean_mean = '';


    hr_std = '';
    resp_std = '';
    spo2_std = '';
    temp_std = '';
    sofa_std = '';
    abpsys_std = '';
    abpdias_std = '';
    abpmean_std = '';


    hr_entropy = '';
    resp_entropy = '';
    spo2_entropy = '';
    temp_entropy = '';
    sofa_entropy = '';
    abpsys_entropy = '';
    abpdias_entropy = '';
    abpmean_entropy = '';

    abpdias_hr_corr = '';
    resp_hr_corr= '';
    abpdias_abpsys_corr= '';
    abpmean_abpsys_corr= '';
    abpmean_abpdias_corr= '';


    hr_min = '';
    resp_min = '';
    spo2_min = '';
    temp_min = '';
    sofa_min = '';
    abpsys_min = '';
    abpdias_min = '';
    abpmean_min = '';


    hr_max = '';
    resp_max = '';
    spo2_max = '';
    temp_max = '';
    sofa_max = '';
    abpsys_max = '';
    abpdias_max = '';
    abpmean_max = '';



    #extracting series for all vitals + sofa
    hr_series = df_subset.HR
    resp_series = df_subset.RESP
    spo2_series = df_subset.SPO2
    sofa_series =df_subset.SOFA_SCORE
    temp_series = df_subset.TEMP
    abpsys_series = df_subset.ABPSYS
    abpdias_series = df_subset.ABPDIAS
    abpmean_series = df_subset.ABPMEAN

    # other way to calculate the entropy
    #hr_entropy = sample_entropy(hr_series) # from tsfresh package
    #hr_entropy_list.append(hr_entropy)

    
    hr_data = hr_series.value_counts()           # counts occurrence of each value
    hr_entropy = scipy.stats.entropy(hr_data)  # get entropy from counts
    hr_entropy_list.append(hr_entropy) 
    hr_min = hr_series.min()
    hr_min_list.append(hr_min)
    hr_max = hr_series.max()
    hr_max_list.append(hr_max)
    ##### for mean and std
    hr_mean = hr_series.mean()
    hr_mean_list.append(hr_mean)
    hr_std = hr_series.std()
    hr_std_list.append(hr_std)





    resp_data = resp_series.value_counts()           # counts occurrence of each value
    resp_entropy = scipy.stats.entropy(resp_data)  # get entropy from counts
    resp_entropy_list.append(resp_entropy)
    resp_min = resp_series.min()
    resp_min_list.append(resp_min)
    resp_max = resp_series.max()
    resp_max_list.append(resp_max)
    ### for mmean and std
    resp_mean = resp_series.mean()
    resp_mean_list.append(resp_mean)
    resp_std = resp_series.std()
    resp_std_list.append(resp_std)


    spo2_data = spo2_series.value_counts()           # counts occurrence of each value
    spo2_entropy = scipy.stats.entropy(spo2_data)  # get entropy from counts
    spo2_entropy_list.append(spo2_entropy)
    spo2_min = spo2_series.min()
    spo2_min_list.append(spo2_min)
    spo2_max = spo2_series.max()
    spo2_max_list.append(spo2_max)
    ### for mmean and std
    spo2_mean = spo2_series.mean()
    spo2_mean_list.append(spo2_mean)
    spo2_std = spo2_series.std()
    spo2_std_list.append(spo2_std)


    temp_data = temp_series.value_counts()           # counts occurrence of each value
    temp_entropy = scipy.stats.entropy(temp_data)  # get entropy from counts
    temp_entropy_list.append(temp_entropy)
    temp_min = temp_series.min()
    temp_min_list.append(temp_min)
    temp_max = temp_series.max()
    temp_max_list.append(temp_max)
    ### for mmean and std
    temp_mean = temp_series.mean()
    temp_mean_list.append(temp_mean)
    temp_std = temp_series.std()
    temp_std_list.append(temp_std)


    sofa_data = sofa_series.value_counts()           # counts occurrence of each value
    sofa_entropy = scipy.stats.entropy(sofa_data)  # get entropy from counts
    sofa_entropy_list.append(sofa_entropy)
    sofa_min = sofa_series.min()
    sofa_min_list.append(sofa_min)
    sofa_max = sofa_series.max()
    sofa_max_list.append(sofa_max)
    ### for mmean and std
    sofa_mean = sofa_series.mean()
    sofa_mean_list.append(sofa_mean)
    sofa_std = sofa_series.std()
    sofa_std_list.append(sofa_std)

    abpsys_data = abpsys_series.value_counts()           # counts occurrence of each value
    abpsys_entropy = scipy.stats.entropy(abpsys_data)  # get entropy from counts
    abpsys_entropy_list.append(abpsys_entropy)
    abpsys_min = abpsys_series.min()
    abpsys_min_list.append(abpsys_min)
    abpsys_max = abpsys_series.max()
    abpsys_max_list.append(abpsys_max)
    ### for mmean and std
    abpsys_mean = abpsys_series.mean()
    abpsys_mean_list.append(abpsys_mean)
    abpsys_std = abpsys_series.std()
    abpsys_std_list.append(abpsys_std)


    abpdias_data = abpdias_series.value_counts()           # counts occurrence of each value
    abpdias_entropy = scipy.stats.entropy(abpdias_data)  # get entropy from counts
    abpdias_entropy_list.append(abpdias_entropy)
    abpdias_min = abpdias_series.min()
    abpdias_min_list.append(abpdias_min)
    abpdias_max = abpdias_series.max()
    abpdias_max_list.append(abpdias_max)
    ### for mmean and std
    abpdias_mean = abpdias_series.mean()
    abpdias_mean_list.append(abpdias_mean)
    abpdias_std = abpdias_series.std()
    abpdias_std_list.append(abpdias_std)


    abpmean_data = abpmean_series.value_counts()           # counts occurrence of each value
    abpmean_entropy = scipy.stats.entropy(abpmean_data)  # get entropy from counts
    abpmean_entropy_list.append(abpmean_entropy)
    abpmean_min = abpmean_series.min()
    abpmean_min_list.append(abpmean_min)
    abpmean_max = abpmean_series.max()
    abpmean_max_list.append(abpmean_max)
    ### for mmean and std
    abpmean_mean = abpmean_series.mean()
    abpmean_mean_list.append(abpmean_mean)
    abpmean_std = abpmean_series.std()
    abpmean_std_list.append(abpmean_std)

    


    abpdias_hr_corr = abpdias_series.corr(hr_series) 
    abpdias_hr_corr_list.append(abpdias_hr_corr);
    
    resp_hr_corr = resp_series.corr(hr_series) 
    resp_hr_corr_list.append(resp_hr_corr);
    

    abpdias_abpsys_corr = abpdias_series.corr(abpsys_series) 
    abpdias_abpsys_corr_list.append(abpdias_abpsys_corr);
    
    abpmean_abpsys_corr = abpmean_series.corr(abpsys_series) 
    abpmean_abpsys_corr_list.append(abpmean_abpsys_corr);
    

    abpmean_abpdias_corr = abpmean_series.corr(abpdias_series) 
    abpmean_abpdias_corr_list.append(abpmean_abpdias_corr);


    ##### ADDING BELOW TO LABEL ACCORDING TO SOFA
    
    if i == 0:
      start_sofa = df_subset['SOFA_SCORE'].iloc[0]
      end_sofa = df_subset['SOFA_SCORE'].iloc[59]

    #print('start_sofa : ', start_sofa)
    #print('end_sofa :', end_sofa)
    #print((sofa_mean - start_sofa  ))
    
    if (sofa_mean - start_sofa) >= 1 :
      time_has_shock = 1
    else:
      time_has_shock = 0


    has_shock_list.append(time_has_shock) 
    sofa_diff_start_list.append(sofa_mean - start_sofa)

    time_list.append(init_time)

    init_time = end_time # incrementing times for the next window
    end_time = init_time + datetime.timedelta(minutes=60)

    #start_window_idx = end_window_idx 
    #end_window_idx = end_window_idx + 60
    

  #['HR_STD','RESP_STD','ABPSYS_STD','ABPDIAS_STD','ABPMEAN_STD','SPO2_STD','TEMP_STD','SOFA_SCORE_STD']
  #print('df_features_temp lenght: ', df_features_temp.shape)
  #print('hr entrpy list lenght : ', len(hr_entropy_list))

  #for mean and std
  df_features_temp['HR'] = hr_mean_list
  df_features_temp['RESP'] = resp_mean_list
  df_features_temp['ABPSYS'] = abpsys_mean_list
  df_features_temp['ABPDIAS'] = abpdias_mean_list
  df_features_temp['ABPMEAN'] = abpmean_mean_list
  df_features_temp['SPO2'] = spo2_mean_list
  df_features_temp['TEMP'] = temp_mean_list
  df_features_temp['SOFA_SCORE'] = sofa_mean_list

  df_features_temp['HR_STD'] = hr_std_list
  df_features_temp['RESP_STD'] = resp_std_list
  df_features_temp['ABPSYS_STD'] = abpsys_std_list
  df_features_temp['ABPDIAS_STD'] = abpdias_std_list
  df_features_temp['ABPMEAN_STD'] = abpmean_std_list
  df_features_temp['SPO2_STD'] = spo2_std_list
  df_features_temp['TEMP_STD'] = temp_std_list
  df_features_temp['SOFA_SCORE_STD'] = sofa_std_list

  df_features_temp['TIME'] = time_list
  df_features_temp['HR_ENT'] = hr_entropy_list
  df_features_temp['RESP_ENT'] = resp_entropy_list
  df_features_temp['ABPSYS_ENT'] = abpsys_entropy_list
  df_features_temp['ABPDIAS_ENT'] = abpdias_entropy_list
  df_features_temp['ABPMEAN_ENT'] = abpmean_entropy_list
  df_features_temp['SPO2_ENT'] = spo2_entropy_list
  df_features_temp['TEMP_ENT'] = temp_entropy_list
  df_features_temp['SOFA_SCORE_ENT'] = sofa_entropy_list


  df_features_temp['ABPDIAS_HR_CORR'] = abpdias_hr_corr_list
  df_features_temp['RESP_HR_CORR'] = resp_hr_corr_list
  df_features_temp['ABPDIAS_ABPSYS_CORR'] = abpdias_abpsys_corr_list
  df_features_temp['ABPMEAN_ABPSYS_CORR'] = abpmean_abpsys_corr_list
  df_features_temp['ABPMEAN_ABPDIAS_CORR'] = abpmean_abpdias_corr_list
  

  #min 

  df_features_temp['HR_MIN'] = hr_min_list
  df_features_temp['RESP_MIN'] = resp_min_list 
  df_features_temp['SPO2_MIN'] = spo2_min_list 
  df_features_temp['TEMP_MIN'] = temp_min_list 
  df_features_temp['SOFA_SCORE_MIN'] = sofa_min_list 
  df_features_temp['ABPSYS_MIN'] = abpsys_min_list
  df_features_temp['ABPDIAS_MIN'] = abpdias_min_list 
  df_features_temp['ABPMEAN_MIN'] = abpmean_min_list

  #MAX
  df_features_temp['HR_MAX'] = hr_max_list
  df_features_temp['RESP_MAX'] = resp_max_list 
  df_features_temp['SPO2_MAX'] = spo2_max_list 
  df_features_temp['TEMP_MAX'] = temp_max_list 
  df_features_temp['SOFA_SCORE_MAX'] = sofa_max_list 
  df_features_temp['ABPSYS_MAX'] = abpsys_max_list
  df_features_temp['ABPDIAS_MAX'] = abpdias_max_list 
  df_features_temp['ABPMEAN_MAX'] = abpmean_max_list

  
  
  df_features_temp['HR_DIFF']=df_features_temp['HR'] -df_features_temp['HR'].shift(1)
  df_features_temp['RESP_DIFF']=df_features_temp['RESP'] -df_features_temp['RESP'].shift(1)
  df_features_temp['ABPSYS_DIFF']=df_features_temp['ABPSYS'] -df_features_temp['ABPSYS'].shift(1)
  df_features_temp['ABPDIAS_DIFF']=df_features_temp['ABPDIAS'] -df_features_temp['ABPDIAS'].shift(1)
  df_features_temp['ABPMEAN_DIFF']=df_features_temp['ABPMEAN'] -df_features_temp['ABPMEAN'].shift(1)
  df_features_temp['SPO2_DIFF']=df_features_temp['SPO2'] -df_features_temp['SPO2'].shift(1)
  df_features_temp['TEMP_DIFF']=df_features_temp['TEMP'] -df_features_temp['TEMP'].shift(1)
  df_features_temp['SOFA_SCORE_DIFF']=df_features_temp['SOFA_SCORE'] -df_features_temp['SOFA_SCORE'].shift(1)



  df_features_temp['ICUSTAY_ID'] = icustay_id
  df_features_temp['SUBJECT_ID'] = subject_id
  df_features_temp['SEPSIS_ONSETTIME'] = sepsis_onsettime
  df_features_temp['SEPSIS_SHOCK_ONSETTIME'] = shock_onsetttime
  #df_features_temp['HAS_SHOCK'] = has_shock
  df_features_temp['HAS_SHOCK'] = has_shock_list
  df_features_temp['HOURS_BETWEEN_SEPSIS_SHOCK'] = skip
  df_features_temp['SOFA_START_CURRENT_DIFF'] = sofa_diff_start_list
   
  
  
  # forward and backward fill the correlation columns
  corr_cols = ['ABPDIAS_HR_CORR', 'RESP_HR_CORR','ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR','ABPMEAN_ABPDIAS_CORR']

  df_features_temp.loc[:,corr_cols] = df_features_temp.loc[:,corr_cols].ffill().bfill()


  

  
  

  icu_intime='';
  sepsis_onsettime='';
  shock_onsetttime = '';
  has_shock ='';
  base_min = '';
  

  
  df_features = df_features.append(df_features_temp);
  df_tsdata_subjectid_entireTimeBeforeShock.drop(df_tsdata_subjectid_entireTimeBeforeShock.index, inplace = True)
  df_features_temp.drop(df_features_temp.index, inplace=True)

print(df_features[df_features['SUBJECT_ID']==41782])


x = df_features[['HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP',
 'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
 'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
 'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
 'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
 'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']]

scaler = StandardScaler()
#sclaing the training dataset
#x = scaler.fit_transform(x)



y = df_features['SOFA_SCORE']
# define dataset
# define the model
model = XGBRegressor(objective="reg:squarederror")
# fit the model
model.fit(x, y)
# get importance
importance = model.feature_importances_
print(importance)
# summarize feature importance
"""
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
"""
# plot feature importance
pyplot.bar([x for x in range(len(importance))], importance)
pyplot.show()

count_best = 0

for i,v in enumerate(importance):
	if v >= 0.013:
		count_best = count_best+1
		print('Feature: %0d, Score: %.5f' % (i,v))
print(count_best)

k=5 # for 5 fold

for i in range(5):

    # test train split for patients

    try:
      df_final_cohort.drop(df_final_cohort.index, inplace= True)
    except:
      print('df_final_cohort does not exists')

    #extracting patient ids for shock and non-shock group that has less than 20 % mmissing data 
    # from sepsis onset till sepsis onset + 31 hours or from sepsis onset time till shock onset time

    df_cohort_temp = df_icutime[(df_icutime['Sepsis_SepsisOnset+ShockOnsetOR31h_timeoverlap_exists']==1)& (df_icutime['Sepsis_SepsisOnset+ShockOnsetOR31h_percentNonMissingData']>=80) ]
    print(df_cohort_temp.shape[0])


    df_cohort_temp['sepsis_onsettime'] =  pd.to_datetime(df_cohort_temp['sepsis_onsettime'])
    df_cohort_temp['sepstic_shock_onsettime'] =  pd.to_datetime(df_cohort_temp['sepstic_shock_onsettime'])


    # for non-shock
    df_non_shock =  df_cohort_temp[df_cohort_temp['sepstic_shock_onsettime'].isna()]#.head(87) # adding head to have euqal number of shock and non shock patients 
    print('non shock : ', df_non_shock.shape)

    #for shock 
    df_shock =  df_cohort_temp[df_cohort_temp['sepstic_shock_onsettime'].notna()]
    print('shock : ', df_shock.shape)


    df_final_cohort = pd.DataFrame(columns=df_cohort_temp.columns )
    df_final_cohort =  df_final_cohort.append(df_non_shock); # including all non-shock patients
    df_final_cohort =  df_final_cohort.append(df_shock); # including all shock patients 


    print(df_final_cohort.columns)


    df_final_cohort['has_shock'] = np.where(df_final_cohort['sepstic_shock_onsettime'].isna(), 0 , 1 )


    x = df_final_cohort[['icustay_id', 'subject_id','sepsis_onsettime', 'sepstic_shock_onsettime']]

    y = df_final_cohort['has_shock']

    x_train_subjects, x_test_subjects, y_train_class, y_test_class = train_test_split(x, y, test_size=0.3,shuffle=True) # shuffling and splitting

    ############### preparing features for train  ( regression). ######################

    subject_ids_train = x_train_subjects.subject_id.unique()
    print(len(subject_ids_train))

    # taking subset of features only for the train subject ids

    df_features_train_subjects = df_features[df_features.SUBJECT_ID.isin(subject_ids_train)]


    x_train = df_features_train_subjects[['TIME','ICUSTAY_ID','SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK','HOURS_BETWEEN_SEPSIS_SHOCK',
      'HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
      'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
      'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
      'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
      'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
      'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']]

    #y_train = df_features_train_subjects['HAS_SHOCK']

    # x and y for regression
    x_train_reg = x_train[['HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP',
      'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
      'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
      'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
      'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
      'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']]

    y_train_reg = x_train['SOFA_SCORE']



    ############### preparing features for test  (classification and regression). ######################

    subject_ids_test = x_test_subjects.subject_id.unique()
    print(len(subject_ids_test))


    df_features_test_subjects = df_features[df_features.SUBJECT_ID.isin(subject_ids_test)]



    x_test = df_features_test_subjects[['TIME','ICUSTAY_ID','SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK','HOURS_BETWEEN_SEPSIS_SHOCK',
      'HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
      'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
      'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
      'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
      'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
      'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']]

    #y_test = df_features_test_subjects['HAS_SHOCK']

    test_reg_cols = ['HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP',
      'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
      'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
      'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
      'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
      'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']



    #fitting a xgboost Regression model to predict sofa score:



    selection = SelectFromModel(model, threshold=0.013, prefit=True)  # best
    print(selection) 
    select_X_train = selection.transform(x_train_reg)
    print(select_X_train.shape)
    # train model
    selection_model = XGBRegressor()
    selection_model.fit(select_X_train, y_train_reg)



    #selector = SelectKBest(score_func=mutual_info_regression, k=15)
    #selector.fit(x_feat_imp1, y_feat_imp1)
    # Get columns to keep and create new dataframe with those only
    cols = selection.get_support(indices=True)
    features_df_new = x.iloc[:,cols]
    print(features_df_new.columns)



    #performing regression patient by patient on entire test dataset:

    import sklearn
    from sklearn.metrics import mean_squared_error


    rmse_list = [];
    r2_list = [];
    rss_list = [];

    regression_result_cols = ['SUBJECT_ID', 'HOURS_BETWEEN','RMSE','R2','RSS']


    

    try:
      df_regression_result_indv_patients.drop(df_regression_result_indv_patients.index, inplace = True)
      df_features_classification_test_patient_id.drop(df_features_classification_test_patient_id.index, inplace = True)
    except:
      print('df_regression_result_indv_patients.drop(df_regression_result_indv_patients.index, inplace = True) does nnot exist')


    df_regression_result_indv_patients = pd.DataFrame( columns= regression_result_cols)
    df_features_classification_test_patient_id = pd.DataFrame(columns= x_test.columns)
    df_temp_features_classification_test_patient_id = pd.DataFrame(columns= x_test.columns)

    for i in subject_ids_test:

      print('subject :' , i);
      try:
        x_test_reg_patient_id.drop(x_test_reg_patient_id.index, inplace= True)
        
      except:
        print('x_test_reg_patient_id does not exist')


      cur_index_regression_result = df_regression_result_indv_patients.shape[0]


      features_test_patient_id = x_test[x_test['SUBJECT_ID'] == i]               # 50182

      if features_test_patient_id.shape[0] > 0:
        hours_between = features_test_patient_id.HOURS_BETWEEN_SEPSIS_SHOCK.unique()

        df_temp_features_classification_test_patient_id = features_test_patient_id; 
        


        x_test_reg_patient_id =  features_test_patient_id[['HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP',
      'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
      'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
      'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
      'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
      'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']]


        select_X_test = selection.transform(x_test_reg_patient_id)
        

        scaler = StandardScaler()
        #select_X_test = scaler.fit_transform(select_X_test)
        
        reg_predictions = selection_model.predict(select_X_test)

        y_test_reg_patient_id = features_test_patient_id['SOFA_SCORE']


        rms = np.sqrt(mean_squared_error(y_test_reg_patient_id, reg_predictions))
        #print('ROOT MEAN SQUARE ERROR : ',rms )
        
        rss = ((y_test_reg_patient_id - reg_predictions)**2).sum()

        #print("RSS = ", ((y_test_reg_patient_id - reg_predictions)**2).sum())
        #print("R2 = ", glm_regression_model.rsquared)
        R2 = sklearn.metrics.r2_score(y_test_reg_patient_id,reg_predictions)
        #print(R2)
        

        df_regression_result_indv_patients.loc[cur_index_regression_result,'SUBJECT_ID'] = i;
        df_regression_result_indv_patients.loc[cur_index_regression_result,'HOURS_BETWEEN'] = hours_between;
      
        df_regression_result_indv_patients.loc[cur_index_regression_result,'RMSE'] = rms;
        df_regression_result_indv_patients.loc[cur_index_regression_result,'R2'] = R2 ;
        df_regression_result_indv_patients.loc[cur_index_regression_result,'RSS'] = rss ;

        df_temp_features_classification_test_patient_id['PRED_SOFA_SCORE'] = reg_predictions

        df_features_classification_test_patient_id = df_features_classification_test_patient_id.append(df_temp_features_classification_test_patient_id)
        df_temp_features_classification_test_patient_id.drop(df_temp_features_classification_test_patient_id.index, inplace = True)


    rms_overall = np.sqrt(mean_squared_error(df_features_classification_test_patient_id.SOFA_SCORE, df_features_classification_test_patient_id.PRED_SOFA_SCORE))
        
    rss_overall = ((df_features_classification_test_patient_id.SOFA_SCORE - df_features_classification_test_patient_id.PRED_SOFA_SCORE)**2).sum()

    R2_overall = sklearn.metrics.r2_score(df_features_classification_test_patient_id.SOFA_SCORE, df_features_classification_test_patient_id.PRED_SOFA_SCORE)


    print('Mean RMSE : ', rms_overall )
    print('Mean R2 : ', R2_overall  )
    print('Mean RSS : ',  rss_overall )

    ## startclassification

    def gen_sequence(id_df, seq_length, seq_cols):
      data_matrix = id_df[seq_cols].values
      # print(data_matrix)
      num_elements = data_matrix.shape[0]
      #print(num_elements)
      for start, stop in zip(range(0, num_elements - seq_length), range(seq_length, num_elements )):
          #print(start, stop)
          yield data_matrix[start:stop, :]


    # function to generate labels
    def gen_labels(id_df, seq_length, label):
      data_matrix = id_df[label].values
      num_elements = data_matrix.shape[0]
      return data_matrix[seq_length : num_elements , :]


    # Press the green button in the gutter to run the script.
    subject_ids_train = x_train_subjects.subject_id.unique()
    if __name__ == '__main__':

        # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
        scaler = StandardScaler()
        # pick a window size
        sequence_length = 3 
        seq_array_main = []
        seq_array_main = np.array(seq_array_main)

        label_array_main = []
        label_array_main = np.array(label_array_main)

        count_train = 0 ;
            

        # pick the feature columns
        #sequence_cols = ['hr','abpSys','abpDias','abpMean','resp','sp02','SDhr','SDresp','SDsp02']
        sequence_cols = ['HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
          'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
          'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
          'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
          'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
          'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']
                        
        sequence_cols_all = ['TIME','ICUSTAY_ID','SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK','HOURS_BETWEEN_SEPSIS_SHOCK',
          'HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
          'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
          'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
          'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
          'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
          'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']

        for i_train in subject_ids_train : # [41782] : [86984 , 41782]:
          #print(i)
          try:
            train_df.drop(train_df.index, inplace = True)
          except:
            print('train_df does not exist')

          train_df = df_features[df_features['SUBJECT_ID']==i_train]
              
          if train_df.shape[0] >= (sequence_length + 1): 

            # first scale the values we are using as features
            train_df[sequence_cols] = scaler.fit_transform(train_df[sequence_cols])
              
            # here I am adding a label based on the sofa_Score
            #train_df['HAS_SHOCK'] = np.where(train_df.HAS_SHOCK == 1, 'shock','Nonshock')---comented

                
            train_df = train_df.loc[:,sequence_cols_all]
                
            # generate the sequences, of size sequence_length
            seq_gen = list(gen_sequence(train_df, sequence_length, sequence_cols))
            seq_array = np.array(list(seq_gen)).astype(np.float32)
            


            if seq_array_main.shape[0] == 0:
              seq_array_main = seq_array
            else:
              #print('printing for main seq array : ',seq_array_main.shape)
              #print('printing for indv seq array : ',seq_array.shape)
              seq_array_main = np.append(seq_array_main, seq_array, axis = 0) 


            label_gen = gen_labels(train_df, sequence_length, ['HAS_SHOCK'])
            label_array = np.array(label_gen).astype(np.float32)
            

            if label_array_main.shape[0] == 0:
              label_array_main = label_array
            else:
              label_array_main = np.append(label_array_main, label_array, axis = 0) 
                
            count_train = count_train + 1


    # sampling

    # first run till df_Features + cohort selection + extracting train features


    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.pipeline import Pipeline
    from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE , ADASYN
    pd.set_option('display.max_rows', 1200)
    pd.set_option('display.max_columns', 500)
    pd.set_option('display.width', 1000)
    a_og = '';
    b_og = '';
    c_og = '';
    a_og = seq_array_main.shape[0]
    b_og = seq_array_main.shape[1]
    c_og = seq_array_main.shape[2]

    print(a_og)
    print(b_og)
    print(c_og)

    seq_array_main_2d = seq_array_main.reshape(a_og,b_og*c_og)

    print('printing 2D converted array shape ')
    print(seq_array_main_2d.shape)

    #print(label_array_main)
    label_array_main_1d = np.concatenate(label_array_main, axis=0 )

    print('BEFORE OVERSAMPLING')
    print(Counter(label_array_main_1d))  #Counter({0.0: 50, 1.0: 1})


    ##### trying to oversample
    """
    strategy = {0: 1929 ,
                1: 1929,
                2: 1929}

    """
    over = SMOTE(sampling_strategy=1) # 0.01
    #under = RandomUnderSampler(sampling_strategy=1)# 1
    #steps = [('o', over), ('u', under)]
    steps = [('o', over)]
    #steps = [('u', under)]
    pipeline = Pipeline(steps=steps)

    #
    X_oversample , y_oversample =  pipeline.fit_resample(seq_array_main_2d, label_array_main_1d)# oversample.fit_resample(seq_array_main_2d, label_array_main_1d)
    counter = Counter(y_oversample)
    print('AFTER OVERSAMPLING')
    print(Counter(y_oversample)) # Counter({0: 7964, 1: 70})

    print(X_oversample.shape)
    print(y_oversample.shape)

    a_oversmp = X_oversample.shape[0]

    print('CONVERTING BACK TO ORIGINAL SHAPE')
    seq_array_main_back_to_3d = X_oversample.reshape(a_oversmp,3,41) # 3, 41

    print(seq_array_main_back_to_3d.shape)



    print(label_array_main_1d.shape)
    """
    alg = XGBClassifier(learning_rate = 0.02,n_estimators=1000, max_depth=2,subsample= 0.4, colsample_bytree= 0.4, gamma= 0 ,min_child_weight=0.2, 
                        scale_pos_weight = 0.3, reg_alpha=0.001, booster='gbtree',objective='binary:logistic')
    """


    eval_metrics= ['auc']
    eval_set = [(X_oversample, y_oversample)]


    """

    xgb_class = XGBClassifier(learning_rate=0.02, n_estimators=160, max_depth=7, # <- best
                            min_child_weight=3, gamma=0.2, subsample=1, colsample_bytree = 1,
                            objective='binary:logistic', nthread=7, scale_pos_weight= 7, seed=27)
    """
    alg = XGBClassifier(learning_rate=0.02, n_estimators=160, max_depth=7, # <- best
                            min_child_weight=1, gamma=0.2, subsample= 1 ,colsample_bytree = 1,
                            objective='binary:logistic', nthread=7, scale_pos_weight= 0.8, seed=27)

    xgb_model = alg.fit(X_oversample, y_oversample, eval_metric=eval_metrics,eval_set=eval_set) 


    # testing
    df_features_classification_test_patient_id.SOFA_SCORE = df_features_classification_test_patient_id.PRED_SOFA_SCORE
    subject_ids_test = x_test_subjects.subject_id.unique()
        #print(len(subject_ids_test)) # 65 for 3 sequence length

    if __name__ == '__main__':

      # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
      scaler = StandardScaler()
      # pick a window size
      sequence_length = 3
                
      subject_id_list = []
      has_shock_list = []
      number_hours_between_Sepsis_shock_list = []
      precision_list = []
      recall_list = []
      accuracy_list = []
      fscore_list = []
      y_true_list= []
      y_pred_list = []

      balanced_accuracy_list = []
      roc_auc_score_list= []
      specificity_list= []

      count_test = 0 ;

      tn_list = []
      fn_list=[]
      tp_list = []
      fp_list =[]
      first_predicted_time_list = []
                
                

      # pick the feature columns
      #sequence_cols = ['hr','abpSys','abpDias','abpMean','resp','sp02','SDhr','SDresp','SDsp02']
      sequence_cols = ['HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
          'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
          'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
          'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
          'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
          'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']
                        
      sequence_cols_all = ['TIME','ICUSTAY_ID','SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK','HOURS_BETWEEN_SEPSIS_SHOCK',
          'HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
          'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
          'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
          'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
          'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
          'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']

      for i in subject_ids_test : #[87481] : #
        #print(i)
        try:
          test_df.drop(test_df.index, inplace = True)
        except:
          print('traintest_df_df does not exist')


        #test_df = x_test[x_test['SUBJECT_ID']==i]
        test_df = df_features[df_features['SUBJECT_ID']==i]
        print('printing entire feature df')
        #print(test_df)
                  
                  
        if test_df.shape[0] >= (sequence_length + 1): 

          number_hours_between_Sepsis_shock = test_df.HOURS_BETWEEN_SEPSIS_SHOCK.unique()
          #has_shock_indv = test_df.HAS_SHOCK.unique()

          # first scale the values we are using as features
          test_df[sequence_cols] = scaler.fit_transform(test_df[sequence_cols])
                  
          
          test_df = test_df.loc[:,sequence_cols_all]
                    
          has_shock_indv = 1
          # generate the sequences, of size sequence_length
          seq_gen_test = list(gen_sequence(test_df, sequence_length, sequence_cols))
          seq_array_test = np.array(list(seq_gen_test)).astype(np.float32)

          shock_onset = np.asscalar(test_df.SEPSIS_SHOCK_ONSETTIME.unique())              
          if str(shock_onset) != 'nan': 
            print('IS SHOCK')
            has_shock_indv = 1
          else:
            print('NOT SHOCK')
            has_shock_indv = 0

          label_gen_test = gen_labels(test_df, sequence_length, ['HAS_SHOCK'])
          #print('printing after gen labels')
          #print(label_gen_test)
          label_array_test = np.array(label_gen_test).astype(np.float32)
          #print('after conversion, label array')
          #print(label_array_test)
          count_test = count_test + 1

          # reshape seq array test
          a_og_test = '';
          b_og_test = '';
          c_og_test = '';
          a_og_test = seq_array_test.shape[0]
          b_og_test = seq_array_test.shape[1]
          c_og_test = seq_array_test.shape[2]

          

          seq_array_test_2d = seq_array_test.reshape(a_og_test,b_og_test*c_og_test)

          # make predictions and compute confusion matrix
          y_pred = xgb_model.predict(seq_array_test_2d)
          y_pred_probab = xgb_model.predict_proba(seq_array_test_2d)
          y_true = label_array_test
          #print('printing prediction')
          #print(y_pred)
                    

          #y_true = np.argmax(y_true, axis=1)--COMMENTED

          y_true_list.append(y_true)
          y_pred_list.append(y_pred)
              
          
          y_true = np.concatenate(y_true , axis=0 )
          y_true = y_true.astype(int)
          #y_pred  = np.concatenate( y_pred, axis=0 )
          y_pred = y_pred.astype(int)
          
          if has_shock_indv == 1  :
            array_get_index = np.asarray(y_pred)
            itemindex = np.where(array_get_index==1)
            try:
              index = itemindex[0][0]
              first_predicted_time = test_df.loc[index,'TIME']
              print(first_predicted_time)
            except :
              first_predicted_time = '';
          else:
            first_predicted_time = '';
          
          print(y_true)
          print(y_pred)
          
          CM = confusion_matrix(y_true, y_pred)
          try:
            tn = CM[0][0]
          except:
            tn = 0;

          try:
            fn = CM[1][0]
          except:
            fn =0 ;

          try:
          tp = CM[1][1]
          except:
            tp = 0

          try:
            fp = CM[0][1]
          except:
            fp = 0 ;

          

          # compute precision and recall
          precision = precision_score(y_true, y_pred, average='micro')
          recall = recall_score(y_true, y_pred, average='micro')
          accuracy = accuracy_score(y_true, y_pred)
          fscore =  f1_score(y_true, y_pred, average='micro')


          balanced_accuracy = balanced_accuracy_score(y_true, y_pred)
          try:
            roc_auc_score= roc_auc_score(y_true, y_pred_probab)
          except:
            roc_auc_score = 0
            
          specificity = tn / (tn + fp)

          #print('PRECISION : ', precision)
          cm = confusion_matrix(y_true, y_pred)
          subject_id_list.append(i) ;
          has_shock_list.append(has_shock_indv);
          number_hours_between_Sepsis_shock_list.append(number_hours_between_Sepsis_shock);
          recall_list.append(recall);
          precision_list.append(precision);
          accuracy_list.append(accuracy);
          fscore_list.append(fscore)
          balanced_accuracy_list.append(balanced_accuracy)
          roc_auc_score_list.append(roc_auc_score)
          specificity_list.append(specificity)
          tn_list.append(tn)
          fn_list.append(fn)
          fp_list.append(fp)
          tp_list.append(tp)
          first_predicted_time_list.append(first_predicted_time)
              
              
    # evaluation
    #printing confusion matrix for all the patients overall
    import numpy 

    y_true_list_new = numpy.concatenate( y_true_list, axis=0 )
    #print(y_true_list)
    y_true_list_new = y_true_list_new.astype(int)
    y_true_list_new = np.asarray(y_true_list_new)



    y_pred_list_new  = np.concatenate( y_pred_list, axis=0 )
    #print(y_pred_list_new)
    y_pred_list_new = y_pred_list_new.astype(int)
    y_pred_list_new = np.asarray(y_pred_list_new)


    print('Count of labels for true labels')
    print(np.array(np.unique(y_true_list_new, return_counts=True)).T)

    cm = confusion_matrix(y_true_list_new, y_pred_list_new)
    print(cm)
    tn, fp, fn, tp = confusion_matrix(y_true_list_new, y_pred_list_new).ravel()


    print('TN : ', tn)
    print('FP : ', fp)
    print('FN : ', fn)
    print('TP : ', tp)

    precision_overall = precision_score(y_true_list_new, y_pred_list_new, average='micro')
    recall_overall = recall_score(y_true_list_new, y_pred_list_new, average='micro')
    accuracy_overall = accuracy_score(y_true_list_new, y_pred_list_new)
    fscore_overall =  f1_score(y_true_list_new, y_pred_list_new, average='micro')
    balanced_accuracy_overall = balanced_accuracy_score(y_true_list_new, y_pred_list_new)
    specificity_overall = tn / (tn + fp)
    

    print('RECALL : ', recall_overall)
    print('SPECIFICITY: ', specificity_overall)

    print('ACCURACY : ', accuracy_overall )
    print('PRECISION : ', precision_overall)
    print('F-SCORE : ', fscore_overall)
    print('BALANCED ACCURACY : ', balanced_accuracy_overall)


