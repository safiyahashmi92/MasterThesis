# -*- coding: utf-8 -*-
"""SMOTE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tRz9WmFlCs6Zjy4nUyV4cEEtb8BDTXYp
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
import pandas as pd
import io
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
import os
import shutil
import posixpath
import urllib.request
import datetime
from collections import namedtuple

pd.set_option('display.max_rows', 50000)
pd.set_option('display.max_columns', 50)
pd.set_option('display.width', 1000)
pd.set_option('max_colwidth', 800)



drive.mount('/content/gdrive')

# Download All Time series data for 404 patients.

df_all256_withTemp = pd.read_csv('/content/gdrive/My Drive/Master thesis/df_ts_records_all236SepsisPatients(lessThan20%Missing)_fromSepsisOnset_ShockOnsetPlus10h_or_sepsisOnsetPlus41h_WITH_TEMP_SOFA.csv')
df_all256_withTemp['TIME'] =  pd.to_datetime(df_all256_withTemp['TIME'])
print(df_all256_withTemp.shape)
print(df_all256_withTemp.columns)

#cleaning data :  removing all negative values

df_all256_withTemp.HR = df_all256_withTemp.HR.mask(df_all256_withTemp.HR < 0)

df_all256_withTemp.RESP = df_all256_withTemp.RESP.mask(df_all256_withTemp.RESP < 0)

df_all256_withTemp.ABPSYS = df_all256_withTemp.ABPSYS.mask(df_all256_withTemp.ABPSYS < 0)

df_all256_withTemp.ABPDIAS = df_all256_withTemp.ABPDIAS.mask(df_all256_withTemp.ABPDIAS < 0)

df_all256_withTemp.ABPMEAN = df_all256_withTemp.ABPMEAN.mask(df_all256_withTemp.ABPMEAN < 0)

df_all256_withTemp.SPO2 = df_all256_withTemp.SPO2.mask(df_all256_withTemp.SPO2 < 0)

df_all256_withTemp.TEMP = df_all256_withTemp.TEMP.mask(df_all256_withTemp.TEMP < 0)


df_all256_withTemp.SOFA_SCORE = df_all256_withTemp.SOFA_SCORE.mask(df_all256_withTemp.SOFA_SCORE < 0)


df_all256_withTemp.RESP_SOFA = df_all256_withTemp.RESP_SOFA.mask(df_all256_withTemp.RESP_SOFA < 0)


df_all256_withTemp.LIVER_SOFA = df_all256_withTemp.LIVER_SOFA.mask(df_all256_withTemp.LIVER_SOFA < 0)


df_all256_withTemp.RENAL_SOFA = df_all256_withTemp.RENAL_SOFA.mask(df_all256_withTemp.RENAL_SOFA < 0)


df_all256_withTemp.CARDIO_SOFA = df_all256_withTemp.CARDIO_SOFA.mask(df_all256_withTemp.CARDIO_SOFA < 0)


df_all256_withTemp.CNS_SOFA = df_all256_withTemp.CNS_SOFA.mask(df_all256_withTemp.CNS_SOFA < 0)


df_all256_withTemp.COAG_SOFA = df_all256_withTemp.COAG_SOFA.mask(df_all256_withTemp.COAG_SOFA < 0)

# Missing value imputation by carry forward scheme
df_all256_withTemp_cleaned_MVimputed = df_all256_withTemp.ffill().bfill()

df_all256_withTemp_cleaned_MVimputed.HR = df_all256_withTemp_cleaned_MVimputed.HR.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.RESP = df_all256_withTemp_cleaned_MVimputed.RESP.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.ABPSYS = df_all256_withTemp_cleaned_MVimputed.ABPSYS.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.ABPDIAS = df_all256_withTemp_cleaned_MVimputed.ABPDIAS.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.ABPMEAN = df_all256_withTemp_cleaned_MVimputed.ABPMEAN.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.SPO2 = df_all256_withTemp_cleaned_MVimputed.SPO2.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.TEMP = df_all256_withTemp_cleaned_MVimputed.TEMP.round(decimals=4)


df_all256_withTemp_cleaned_MVimputed.SOFA_SCORE = df_all256_withTemp_cleaned_MVimputed.SOFA_SCORE.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.RENAL_SOFA = df_all256_withTemp_cleaned_MVimputed.RENAL_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.LIVER_SOFA = df_all256_withTemp_cleaned_MVimputed.LIVER_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.COAG_SOFA = df_all256_withTemp_cleaned_MVimputed.COAG_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.CARDIO_SOFA = df_all256_withTemp_cleaned_MVimputed.CARDIO_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.CNS_SOFA = df_all256_withTemp_cleaned_MVimputed.CNS_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.RESP_SOFA = df_all256_withTemp_cleaned_MVimputed.RESP_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed['TIME'] =  pd.to_datetime(df_all256_withTemp_cleaned_MVimputed['TIME'])

print(df_all256_withTemp_cleaned_MVimputed[df_all256_withTemp_cleaned_MVimputed.isnull().any(axis=1)])

subject_ids = df_all256_withTemp_cleaned_MVimputed.SUBJECT_ID.unique()

print(len(subject_ids))

for i in [69272, 84063, 86144, 82055, 58483 ,98769, 77070, 85895, 85541, 52710, 44827, 88111 , 95603, 62835, 50182, 69903, 42621]:
    subject_ids = np.delete(subject_ids,np.where(subject_ids==i))

print(subject_ids)
print(len(subject_ids))

from google.colab import files
uploaded = files.upload()

df_icutime = pd.read_csv(io.BytesIO(uploaded['Only_AllSepsisPatients_with_MissingData_FromSepsisOnset_ToShockOnsetPlus10h_or_SepsisOnset+41h.csv']))
print (df_icutime.columns)
#df_icutime = df_icutime[['subject_id','icustay_id','intime','outtime','sepsis_onsettime']]
#print (df_icutime.columns)
"""
df_icutime['intime'] =  pd.to_datetime(df_icutime['intime'])
df_icutime['outtime'] =  pd.to_datetime(df_icutime['outtime'])
"""

df_test = df_icutime[(df_icutime['Sepsis_SepsisOnset+ShockOnsetOR31h_timeoverlap_exists']== 1) & (df_icutime['Sepsis_SepsisOnset+ShockOnsetOR31h_percentNonMissingData']>=80)]
print(df_test.columns)


#df_test.to_csv('Time_data_for 236_patients.csv')

import seaborn as sns
data_corr = df_all256_withTemp_cleaned_MVimputed[['HR', 'SPO2', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'RESP', 'TEMP','SOFA_SCORE']]
print(data_corr)
corr = data_corr.corr()# calculating the correlation between the above vital signs
sns.heatmap(corr, square=True) # plotting the correlation

import scipy
import collections
from scipy.stats import entropy

feature_cols= ['TIME','ICUSTAY_ID','SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK','HOURS_BETWEEN_SEPSIS_SHOCK',
               'HR',
               'RESP',
               'ABPSYS',
               'ABPDIAS',
               'ABPMEAN',
               'SPO2',
               'TEMP' ,
               'SOFA_SCORE',

               'HR_STD',
               'RESP_STD',
               'ABPSYS_STD',
               'ABPDIAS_STD',
               'ABPMEAN_STD',
               'SPO2_STD',
               'TEMP_STD',
               'SOFA_SCORE_STD',
               
               'HR_ENT',
               'RESP_ENT',
               'ABPSYS_ENT',
               'ABPDIAS_ENT',
               'ABPMEAN_ENT',
               'SPO2_ENT',
               'TEMP_ENT',
               'SOFA_SCORE_ENT',

               'ABPDIAS_HR_CORR',
               'RESP_HR_CORR',
               'ABPDIAS_ABPSYS_CORR',
               'ABPMEAN_ABPSYS_CORR',
               'ABPMEAN_ABPDIAS_CORR',

                'HR_MIN',
                'RESP_MIN',
                'SPO2_MIN',
                'TEMP_MIN',
                'SOFA_SCORE_MIN',
                'ABPSYS_MIN',
                'ABPDIAS_MIN',
                'ABPMEAN_MIN',
                'HR_MAX',
                'RESP_MAX',
                'SPO2_MAX',
                'TEMP_MAX',
                'SOFA_SCORE_MAX',
                'ABPSYS_MAX',
                'ABPDIAS_MAX',
                'ABPMEAN_MAX',

               'HR_DIFF',
               'RESP_DIFF',
               'ABPSYS_DIFF',
               'ABPDIAS_DIFF',
               'ABPMEAN_DIFF',
               'SPO2_DIFF',
               'TEMP_DIFF' ,
               'SOFA_SCORE_DIFF'
               ] 
              



temp_feature_cols= ['TIME',
               'HR',
               'RESP',
               'ABPSYS',
               'ABPDIAS',
               'ABPMEAN',
               'SPO2',
               'TEMP',
               'SOFA_SCORE'
               ] 


try:
  df_features.drop(df_features.index, inplace=True)
except:
  print('df_features does not exists')

df_features  =  pd.DataFrame(columns=feature_cols);

for subject_id in subject_ids:
  curr_idx = df_features.shape[0]
  

  icustay_id = df_icutime.loc[df_icutime.subject_id==subject_id , 'icustay_id'].values[0]

  icu_intime = df_icutime.loc[df_icutime.subject_id==subject_id , 'intime'].values[0]
  sepsis_onsettime = df_icutime.loc[df_icutime.subject_id==subject_id , 'sepsis_onsettime'].values[0]

  shock_onsetttime = df_icutime.loc[df_icutime.subject_id==subject_id , 'sepstic_shock_onsettime'].values[0]
  print('shock_onsetttime : ', shock_onsetttime)

  if str(shock_onsetttime) == 'nan':
    
    has_shock = 0 ;
    #print('not a shock patient')
    df_tsdata_subjectid_entireTimeBeforeShock = df_all256_withTemp_cleaned_MVimputed[(df_all256_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( ( df_all256_withTemp_cleaned_MVimputed['TIME'] >= datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S')    ) & ( df_all256_withTemp_cleaned_MVimputed['TIME'] <= ( datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=31) ) )  )]
    skip = 31
    #df_tsdata_subjectid_entireTimeBeforeShock = df_all256_withTemp_cleaned_MVimputed[(df_all256_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( ( df_all256_withTemp_cleaned_MVimputed['TIME'] >= (datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=27)   ) ) & ( df_all256_withTemp_cleaned_MVimputed['TIME'] <= ( datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=41) ) )  )]
    #skip = 14

  else:
    has_shock = 1 ;

    datetime_shock = datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S')
    # below added to allocate TS sequences 6 hours before shock as 'shock'
    #datetime_shock_minus_6h = datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S') - datetime.timedelta(hours=7)
    #datetime_shock_minus_4h = datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S') - datetime.timedelta(hours=5)
    #datetime_shock_minus_1h = datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S') - datetime.timedelta(hours=2)
    #

    datetime_shock_date = datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S').date()
    datetime_shock_time_hour  = datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S').time().hour
    df_tsdata_subjectid_entireTimeBeforeShock = df_all256_withTemp_cleaned_MVimputed[(df_all256_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( ( df_all256_withTemp_cleaned_MVimputed['TIME'] >= datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S')    ) & ( df_all256_withTemp_cleaned_MVimputed['TIME'] <= (datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S') ) )  )]
    #df_tsdata_subjectid_entireTimeBeforeShock = df_all256_withTemp_cleaned_MVimputed[(df_all256_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( ( df_all256_withTemp_cleaned_MVimputed['TIME'] >= (datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S') - datetime.timedelta(hours=4) )    ) & ( df_all256_withTemp_cleaned_MVimputed['TIME'] <= (datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=10) ) )  )]
    
    hours_between = ( datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S') ) - datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S') 
    #hours_between = ( datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S')  + datetime.timedelta(hours=10) ) -  ( datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S')  - datetime.timedelta(hours=4) )
    
    duration_in_s = hours_between.total_seconds()
    q, r = divmod(duration_in_s, 3600)
    skip = int(q + int(bool(r)))

  
  
  first_row = df_tsdata_subjectid_entireTimeBeforeShock.iloc[0,:]
  
  base_min = int(first_row['TIME'].strftime('%M') )

  df_tsdata_subjectid_entireTimeBeforeShock = df_tsdata_subjectid_entireTimeBeforeShock[temp_feature_cols]
  df_features_temp =pd.DataFrame(columns=temp_feature_cols)

  
  
  """

  #for mean per hour
  df_features_temp = df_tsdata_subjectid_entireTimeBeforeShock.resample('60min',base=base_min,  on='TIME').mean()
  print('after aggregating mean : ', df_features_temp.shape[0])
  


  # for standard deviation
  df_features_temp_std = df_tsdata_subjectid_entireTimeBeforeShock.resample('60min',base=base_min,  on='TIME').std()

  df_features_temp_std =  df_features_temp_std[['HR','RESP','ABPSYS', 'ABPDIAS','ABPMEAN','SPO2','TEMP', 'SOFA_SCORE']]

  df_features_temp_std.columns = ['HR_STD','RESP_STD','ABPSYS_STD','ABPDIAS_STD','ABPMEAN_STD','SPO2_STD','TEMP_STD','SOFA_SCORE_STD']
  df_features_temp = pd.concat([df_features_temp, df_features_temp_std], axis=1)

  print('after aggregating mean annd std : ', df_features_temp.shape[0])
  print(df_features_temp)

  #print(' df_features_temp after concatenation with std')
  # for calculation of entropy
  """
  #q, r = divmod(df_tsdata_subjectid_entireTimeBeforeShock.shape[0], 60)
  #skip = q + int(bool(r)) # rounds to next greater integer (always ceiling)
  #print(skip)
  """
  skip = df_features_temp.shape[0]
  #print('printing skip : ', skip)

  start_window_idx = 0
  end_window_idx = 60
  """
  #For mean calculation
  hr_mean_list = []
  resp_mean_list = []
  spo2_mean_list = []
  temp_mean_list = []
  sofa_mean_list = []
  abpsys_mean_list = []
  abpdias_mean_list = []
  abpmean_mean_list = []

  hr_mean = '';
  resp_mean = '';
  spo2_mean = '';
  temp_mean = '';
  sofa_mean = '';
  abpsys_mean = '';
  abpdias_mean = '';
  abpmean_mean = '';

  # for std calculation
  hr_std_list = []
  resp_std_list = []
  spo2_std_list = []
  temp_std_list = []
  sofa_std_list = []
  abpsys_std_list = []
  abpdias_std_list = []
  abpmean_std_list = []

  hr_std = '';
  resp_std = '';
  spo2_std = '';
  temp_std = '';
  sofa_std = '';
  abpsys_std = '';
  abpdias_std = '';
  abpmean_std = '';


  #for entropy calculation
  hr_entropy_list=[]
  resp_entropy_list = []
  spo2_entropy_list = []
  temp_entropy_list = []
  sofa_entropy_list = []
  abpsys_entropy_list = []
  abpdias_entropy_list = []
  abpmean_entropy_list = []

  hr_entropy = '';
  resp_entropy = '';
  spo2_entropy = '';
  temp_entropy = '';
  sofa_entropy = '';
  abpsys_entropy = '';
  abpdias_entropy = '';
  abpmean_entropy = '';

  # for correlation calculation
  abpdias_hr_corr_list = [];
  resp_hr_corr_list = [];
  abpdias_abpsys_corr_list = [];
  abpmean_abpsys_corr_list = [];
  abpmean_abpdias_corr_list = [];


  # for min calculation
  hr_min_list=[]
  resp_min_list = []
  spo2_min_list = []
  temp_min_list = []
  sofa_min_list = []
  abpsys_min_list = []
  abpdias_min_list = []
  abpmean_min_list = []

  hr_min = '';
  resp_min = '';
  spo2_min = '';
  temp_min = '';
  sofa_min = '';
  abpsys_min = '';
  abpdias_min = '';
  abpmean_min = '';

  # for max calculation
  hr_max_list=[]
  resp_max_list = []
  spo2_max_list = []
  temp_max_list = []
  sofa_max_list = []
  abpsys_max_list = []
  abpdias_max_list = []
  abpmean_max_list = []

  hr_max = '';
  resp_max = '';
  spo2_max = '';
  temp_max = '';
  sofa_max = '';
  abpsys_max = '';
  abpdias_max = '';
  abpmean_max = '';

  #for time
  time_list = [];
  has_shock_list = [] ;

  init_time = first_row['TIME']
  end_time = first_row['TIME'] + datetime.timedelta(minutes =60)


  for i in range(skip):
    
    #print('start_window_time : ', init_time)
    #print('end_window_time : ', end_time)

    #df_subset = df_tsdata_subjectid_entireTimeBeforeShock.iloc[start_window_idx:end_window_idx,:]
    df_subset = df_tsdata_subjectid_entireTimeBeforeShock[(df_tsdata_subjectid_entireTimeBeforeShock['TIME'] >= init_time) & (df_tsdata_subjectid_entireTimeBeforeShock['TIME'] < end_time) ]
  
    #if (has_shock == 1 ) & (init_time >= datetime_shock_minus_4h ) : 
    #if (has_shock == 1 ) & ( ( (init_time + datetime.timedelta(hours =1) )  <= datetime_shock )&  ( ( end_time + datetime.timedelta(hours =1) ) >= datetime_shock)) :
    if (has_shock == 1 ) and ( ( init_time    <= datetime_shock )&  (  end_time  >= datetime_shock)) :
      time_has_shock = 1
    else:
      time_has_shock = 0
    
    has_shock_list.append(time_has_shock)

    
    hr_mean = '';
    resp_mean = '';
    spo2_mean = '';
    temp_mean = '';
    sofa_mean = '';
    abpsys_mean = '';
    abpdias_mean = '';
    abpmean_mean = '';


    hr_std = '';
    resp_std = '';
    spo2_std = '';
    temp_std = '';
    sofa_std = '';
    abpsys_std = '';
    abpdias_std = '';
    abpmean_std = '';


    hr_entropy = '';
    resp_entropy = '';
    spo2_entropy = '';
    temp_entropy = '';
    sofa_entropy = '';
    abpsys_entropy = '';
    abpdias_entropy = '';
    abpmean_entropy = '';

    abpdias_hr_corr = '';
    resp_hr_corr= '';
    abpdias_abpsys_corr= '';
    abpmean_abpsys_corr= '';
    abpmean_abpdias_corr= '';


    hr_min = '';
    resp_min = '';
    spo2_min = '';
    temp_min = '';
    sofa_min = '';
    abpsys_min = '';
    abpdias_min = '';
    abpmean_min = '';


    hr_max = '';
    resp_max = '';
    spo2_max = '';
    temp_max = '';
    sofa_max = '';
    abpsys_max = '';
    abpdias_max = '';
    abpmean_max = '';


    #extracting series for all vitals + sofa
    hr_series = df_subset.HR
    resp_series = df_subset.RESP
    spo2_series = df_subset.SPO2
    sofa_series =df_subset.SOFA_SCORE
    temp_series = df_subset.TEMP
    abpsys_series = df_subset.ABPSYS
    abpdias_series = df_subset.ABPDIAS
    abpmean_series = df_subset.ABPMEAN

    # other way to calculate the entropy
    #hr_entropy = sample_entropy(hr_series) # from tsfresh package
    #hr_entropy_list.append(hr_entropy)

    
    hr_data = hr_series.value_counts()           # counts occurrence of each value
    hr_entropy = scipy.stats.entropy(hr_data)  # get entropy from counts
    hr_entropy_list.append(hr_entropy) 
    hr_min = hr_series.min()
    hr_min_list.append(hr_min)
    hr_max = hr_series.max()
    hr_max_list.append(hr_max)
    ##### for mean and std
    hr_mean = hr_series.mean()
    hr_mean_list.append(hr_mean)
    hr_std = hr_series.std()
    hr_std_list.append(hr_std)





    resp_data = resp_series.value_counts()           # counts occurrence of each value
    resp_entropy = scipy.stats.entropy(resp_data)  # get entropy from counts
    resp_entropy_list.append(resp_entropy)
    resp_min = resp_series.min()
    resp_min_list.append(resp_min)
    resp_max = resp_series.max()
    resp_max_list.append(resp_max)
    ### for mmean and std
    resp_mean = resp_series.mean()
    resp_mean_list.append(resp_mean)
    resp_std = resp_series.std()
    resp_std_list.append(resp_std)


    spo2_data = spo2_series.value_counts()           # counts occurrence of each value
    spo2_entropy = scipy.stats.entropy(spo2_data)  # get entropy from counts
    spo2_entropy_list.append(spo2_entropy)
    spo2_min = spo2_series.min()
    spo2_min_list.append(spo2_min)
    spo2_max = spo2_series.max()
    spo2_max_list.append(spo2_max)
    ### for mmean and std
    spo2_mean = spo2_series.mean()
    spo2_mean_list.append(spo2_mean)
    spo2_std = spo2_series.std()
    spo2_std_list.append(spo2_std)


    temp_data = temp_series.value_counts()           # counts occurrence of each value
    temp_entropy = scipy.stats.entropy(temp_data)  # get entropy from counts
    temp_entropy_list.append(temp_entropy)
    temp_min = temp_series.min()
    temp_min_list.append(temp_min)
    temp_max = temp_series.max()
    temp_max_list.append(temp_max)
    ### for mmean and std
    temp_mean = temp_series.mean()
    temp_mean_list.append(temp_mean)
    temp_std = temp_series.std()
    temp_std_list.append(temp_std)


    sofa_data = sofa_series.value_counts()           # counts occurrence of each value
    sofa_entropy = scipy.stats.entropy(sofa_data)  # get entropy from counts
    sofa_entropy_list.append(sofa_entropy)
    sofa_min = sofa_series.min()
    sofa_min_list.append(sofa_min)
    sofa_max = sofa_series.max()
    sofa_max_list.append(sofa_max)
    ### for mmean and std
    sofa_mean = sofa_series.mean()
    sofa_mean_list.append(sofa_mean)
    sofa_std = sofa_series.std()
    sofa_std_list.append(sofa_std)

    abpsys_data = abpsys_series.value_counts()           # counts occurrence of each value
    abpsys_entropy = scipy.stats.entropy(abpsys_data)  # get entropy from counts
    abpsys_entropy_list.append(abpsys_entropy)
    abpsys_min = abpsys_series.min()
    abpsys_min_list.append(abpsys_min)
    abpsys_max = abpsys_series.max()
    abpsys_max_list.append(abpsys_max)
    ### for mmean and std
    abpsys_mean = abpsys_series.mean()
    abpsys_mean_list.append(abpsys_mean)
    abpsys_std = abpsys_series.std()
    abpsys_std_list.append(abpsys_std)


    abpdias_data = abpdias_series.value_counts()           # counts occurrence of each value
    abpdias_entropy = scipy.stats.entropy(abpdias_data)  # get entropy from counts
    abpdias_entropy_list.append(abpdias_entropy)
    abpdias_min = abpdias_series.min()
    abpdias_min_list.append(abpdias_min)
    abpdias_max = abpdias_series.max()
    abpdias_max_list.append(abpdias_max)
    ### for mmean and std
    abpdias_mean = abpdias_series.mean()
    abpdias_mean_list.append(abpdias_mean)
    abpdias_std = abpdias_series.std()
    abpdias_std_list.append(abpdias_std)


    abpmean_data = abpmean_series.value_counts()           # counts occurrence of each value
    abpmean_entropy = scipy.stats.entropy(abpmean_data)  # get entropy from counts
    abpmean_entropy_list.append(abpmean_entropy)
    abpmean_min = abpmean_series.min()
    abpmean_min_list.append(abpmean_min)
    abpmean_max = abpmean_series.max()
    abpmean_max_list.append(abpmean_max)
    ### for mmean and std
    abpmean_mean = abpmean_series.mean()
    abpmean_mean_list.append(abpmean_mean)
    abpmean_std = abpmean_series.std()
    abpmean_std_list.append(abpmean_std)

    


    abpdias_hr_corr = abpdias_series.corr(hr_series) 
    abpdias_hr_corr_list.append(abpdias_hr_corr);
    
    resp_hr_corr = resp_series.corr(hr_series) 
    resp_hr_corr_list.append(resp_hr_corr);
    

    abpdias_abpsys_corr = abpdias_series.corr(abpsys_series) 
    abpdias_abpsys_corr_list.append(abpdias_abpsys_corr);
    
    abpmean_abpsys_corr = abpmean_series.corr(abpsys_series) 
    abpmean_abpsys_corr_list.append(abpmean_abpsys_corr);
    

    abpmean_abpdias_corr = abpmean_series.corr(abpdias_series) 
    abpmean_abpdias_corr_list.append(abpmean_abpdias_corr);
   
    time_list.append(init_time)

    init_time = end_time # incrementing times for the next window
    end_time = init_time + datetime.timedelta(minutes=60)

    #start_window_idx = end_window_idx 
    #end_window_idx = end_window_idx + 60
    

  #['HR_STD','RESP_STD','ABPSYS_STD','ABPDIAS_STD','ABPMEAN_STD','SPO2_STD','TEMP_STD','SOFA_SCORE_STD']
  #print('df_features_temp lenght: ', df_features_temp.shape)
  #print('hr entrpy list lenght : ', len(hr_entropy_list))

  #for mean and std
  df_features_temp['HR'] = hr_mean_list
  df_features_temp['RESP'] = resp_mean_list
  df_features_temp['ABPSYS'] = abpsys_mean_list
  df_features_temp['ABPDIAS'] = abpdias_mean_list
  df_features_temp['ABPMEAN'] = abpmean_mean_list
  df_features_temp['SPO2'] = spo2_mean_list
  df_features_temp['TEMP'] = temp_mean_list
  df_features_temp['SOFA_SCORE'] = sofa_mean_list

  df_features_temp['HR_STD'] = hr_std_list
  df_features_temp['RESP_STD'] = resp_std_list
  df_features_temp['ABPSYS_STD'] = abpsys_std_list
  df_features_temp['ABPDIAS_STD'] = abpdias_std_list
  df_features_temp['ABPMEAN_STD'] = abpmean_std_list
  df_features_temp['SPO2_STD'] = spo2_std_list
  df_features_temp['TEMP_STD'] = temp_std_list
  df_features_temp['SOFA_SCORE_STD'] = sofa_std_list

  df_features_temp['TIME'] = time_list
  df_features_temp['HR_ENT'] = hr_entropy_list
  df_features_temp['RESP_ENT'] = resp_entropy_list
  df_features_temp['ABPSYS_ENT'] = abpsys_entropy_list
  df_features_temp['ABPDIAS_ENT'] = abpdias_entropy_list
  df_features_temp['ABPMEAN_ENT'] = abpmean_entropy_list
  df_features_temp['SPO2_ENT'] = spo2_entropy_list
  df_features_temp['TEMP_ENT'] = temp_entropy_list
  df_features_temp['SOFA_SCORE_ENT'] = sofa_entropy_list


  df_features_temp['ABPDIAS_HR_CORR'] = abpdias_hr_corr_list
  df_features_temp['RESP_HR_CORR'] = resp_hr_corr_list
  df_features_temp['ABPDIAS_ABPSYS_CORR'] = abpdias_abpsys_corr_list
  df_features_temp['ABPMEAN_ABPSYS_CORR'] = abpmean_abpsys_corr_list
  df_features_temp['ABPMEAN_ABPDIAS_CORR'] = abpmean_abpdias_corr_list
  

  #min 

  df_features_temp['HR_MIN'] = hr_min_list
  df_features_temp['RESP_MIN'] = resp_min_list 
  df_features_temp['SPO2_MIN'] = spo2_min_list 
  df_features_temp['TEMP_MIN'] = temp_min_list 
  df_features_temp['SOFA_SCORE_MIN'] = sofa_min_list 
  df_features_temp['ABPSYS_MIN'] = abpsys_min_list
  df_features_temp['ABPDIAS_MIN'] = abpdias_min_list 
  df_features_temp['ABPMEAN_MIN'] = abpmean_min_list

  #MAX
  df_features_temp['HR_MAX'] = hr_max_list
  df_features_temp['RESP_MAX'] = resp_max_list 
  df_features_temp['SPO2_MAX'] = spo2_max_list 
  df_features_temp['TEMP_MAX'] = temp_max_list 
  df_features_temp['SOFA_SCORE_MAX'] = sofa_max_list 
  df_features_temp['ABPSYS_MAX'] = abpsys_max_list
  df_features_temp['ABPDIAS_MAX'] = abpdias_max_list 
  df_features_temp['ABPMEAN_MAX'] = abpmean_max_list

  
  
  df_features_temp['HR_DIFF']=df_features_temp['HR'] -df_features_temp['HR'].shift(1)
  df_features_temp['RESP_DIFF']=df_features_temp['RESP'] -df_features_temp['RESP'].shift(1)
  df_features_temp['ABPSYS_DIFF']=df_features_temp['ABPSYS'] -df_features_temp['ABPSYS'].shift(1)
  df_features_temp['ABPDIAS_DIFF']=df_features_temp['ABPDIAS'] -df_features_temp['ABPDIAS'].shift(1)
  df_features_temp['ABPMEAN_DIFF']=df_features_temp['ABPMEAN'] -df_features_temp['ABPMEAN'].shift(1)
  df_features_temp['SPO2_DIFF']=df_features_temp['SPO2'] -df_features_temp['SPO2'].shift(1)
  df_features_temp['TEMP_DIFF']=df_features_temp['TEMP'] -df_features_temp['TEMP'].shift(1)
  df_features_temp['SOFA_SCORE_DIFF']=df_features_temp['SOFA_SCORE'] -df_features_temp['SOFA_SCORE'].shift(1)



  df_features_temp['ICUSTAY_ID'] = icustay_id
  df_features_temp['SUBJECT_ID'] = subject_id
  df_features_temp['SEPSIS_ONSETTIME'] = sepsis_onsettime
  df_features_temp['SEPSIS_SHOCK_ONSETTIME'] = shock_onsetttime
  #df_features_temp['HAS_SHOCK'] = has_shock
  df_features_temp['HAS_SHOCK'] = has_shock_list
  df_features_temp['HOURS_BETWEEN_SEPSIS_SHOCK'] = skip
  
  
  
  # forward and backward fill the correlation columns
  corr_cols = ['ABPDIAS_HR_CORR', 'RESP_HR_CORR','ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR','ABPMEAN_ABPDIAS_CORR']

  df_features_temp.loc[:,corr_cols] = df_features_temp.loc[:,corr_cols].ffill().bfill()


  

  
  

  icu_intime='';
  sepsis_onsettime='';
  shock_onsetttime = '';
  has_shock ='';
  base_min = '';
  

  
  df_features = df_features.append(df_features_temp);
  df_tsdata_subjectid_entireTimeBeforeShock.drop(df_tsdata_subjectid_entireTimeBeforeShock.index, inplace = True)
  df_features_temp.drop(df_features_temp.index, inplace=True)

print(df_features[df_features['SUBJECT_ID']==86984])

# generating 3 hours sequences for all data
from sklearn.preprocessing import StandardScaler

def gen_sequence(id_df, seq_length, seq_cols):
  data_matrix = id_df[seq_cols].values
  # print(data_matrix)
  num_elements = data_matrix.shape[0]
  #print(num_elements)
  for start, stop in zip(range(0, num_elements - seq_length), range(seq_length, num_elements )):
      #print(start, stop)
      yield data_matrix[start:stop, :]


# function to generate labels
def gen_labels(id_df, seq_length, label):
  data_matrix = id_df[label].values
  num_elements = data_matrix.shape[0]
  return data_matrix[seq_length : num_elements , :]


# Press the green button in the gutter to run the script.
subject_ids_train = subject_ids
if __name__ == '__main__':

    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
    scaler = StandardScaler()
    # pick a window size
    sequence_length = 3 
    seq_array_main = []
    seq_array_main = np.array(seq_array_main)

    label_array_main = []
    label_array_main = np.array(label_array_main)

    count_train = 0 ;
    count_train_shock = 0;
    count_train_non_shock = 0;
        

    # pick the feature columns
    #sequence_cols = ['hr','abpSys','abpDias','abpMean','resp','sp02','SDhr','SDresp','SDsp02']
    sequence_cols = ['HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
      'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
      'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
      'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
      'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
      'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']
                    
    sequence_cols_all = ['TIME','ICUSTAY_ID','SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK','HOURS_BETWEEN_SEPSIS_SHOCK',
      'HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
      'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
      'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
      'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
      'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
      'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']

    for i_train in subject_ids_train : # [41782] : [86984 , 41782]:
      #print(i)
      try:
        train_df.drop(train_df.index, inplace = True)
      except:
        print('train_df does not exist')

      train_df = df_features[df_features['SUBJECT_ID']==i_train]
          
      if train_df.shape[0] >= (sequence_length + 1): 

        # first scale the values we are using as features
        train_df[sequence_cols] = scaler.fit_transform(train_df[sequence_cols])
          
        # here I am adding a label based on the sofa_Score
        #train_df['HAS_SHOCK'] = np.where(train_df.HAS_SHOCK == 1, 'shock','Nonshock')---comented

            
        train_df = train_df.loc[:,sequence_cols_all]

        #print('\nLabels distribution file: ', i)
        #print(train_df['HAS_SHOCK'].value_counts())
        shock_onset = np.asscalar(train_df.SEPSIS_SHOCK_ONSETTIME.unique())   

        if str(shock_onset) != 'nan': 
          print('IS SHOCK')
          count_train_shock = count_train_shock + 1
        else:
          print('NOT SHOCK')
          count_train_non_shock = count_train_non_shock + 1

            
        # generate the sequences, of size sequence_length
        seq_gen = list(gen_sequence(train_df, sequence_length, sequence_cols))
        seq_array = np.array(list(seq_gen)).astype(np.float32)
        #print(seq_array)
        #print('individual sequence shape')
        #print('printing for main seq array : ',seq_array_main.shape)
        #print('printing for indv seq array : ',seq_array.shape)


        if seq_array_main.shape[0] == 0:
          seq_array_main = seq_array
        else:
          #print('printing for main seq array : ',seq_array_main.shape)
          #print('printing for indv seq array : ',seq_array.shape)
          seq_array_main = np.append(seq_array_main, seq_array, axis = 0) 


        # generate labels
        #print('before gen labels')
        #print(train_df)
        #label_gen = gen_labels(train_df, sequence_length, ['Nonshock', 'shock'])---commented
        label_gen = gen_labels(train_df, sequence_length, ['HAS_SHOCK'])
        label_array = np.array(label_gen).astype(np.float32)
        #print(label_array)
        #print('individual label shape')
        #print(label_array.shape)

        if label_array_main.shape[0] == 0:
          label_array_main = label_array
        else:
          label_array_main = np.append(label_array_main, label_array, axis = 0) 
            
        count_train = count_train + 1

from collections import Counter
from matplotlib import pyplot
import imblearn

x_scatter = df_features [['HR',
               'RESP']]

x_scatter = np.asarray(x_scatter)

y_scatter = label_array_main #df_features['HAS_SHOCK']
label_array_main_1d = np.concatenate(label_array_main, axis=0 )
label_array_main_1d_changed = np.where(label_array_main_1d==1,'shock','non-shock')
print(label_array_main_1d_changed)
counter = Counter(label_array_main_1d)
print(counter) #Counter({0: 7964, 1: 70})

import numpy as np    

# scatter plot of examples by class label
for label, _ in counter.items():
	row_ix = np.where(y_scatter == label)[0]
	pyplot.scatter(x_scatter[row_ix, 0], x_scatter[row_ix, 1], label=str(label))
 
pyplot.legend()
pyplot.show()

"""# **TO TEST SMOTE -- DO NOT RUN**"""

df_test= df_features[['SUBJECT_ID',
                      'HR',
               'RESP',
               'ABPSYS',
               'ABPDIAS',
               'ABPMEAN',
               'SPO2',
               'TEMP' ,
               'SOFA_SCORE',

               'HR_STD',
               'RESP_STD',
               'ABPSYS_STD',
               'ABPDIAS_STD',
               'ABPMEAN_STD',
               'SPO2_STD',
               'TEMP_STD',
               'SOFA_SCORE_STD',
               
               'HR_ENT',
               'RESP_ENT',
               'ABPSYS_ENT',
               'ABPDIAS_ENT',
               'ABPMEAN_ENT',
               'SPO2_ENT',
               'TEMP_ENT',
               'SOFA_SCORE_ENT',

               'ABPDIAS_HR_CORR',
               'RESP_HR_CORR',
               'ABPDIAS_ABPSYS_CORR',
               'ABPMEAN_ABPSYS_CORR',
               'ABPMEAN_ABPDIAS_CORR',

                'HR_MIN',
                'RESP_MIN',
                'SPO2_MIN',
                'TEMP_MIN',
                'SOFA_SCORE_MIN',
                'ABPSYS_MIN',
                'ABPDIAS_MIN',
                'ABPMEAN_MIN',
                'HR_MAX',
                'RESP_MAX',
                'SPO2_MAX',
                'TEMP_MAX',
                'SOFA_SCORE_MAX',
                'ABPSYS_MAX',
                'ABPDIAS_MAX',
                'ABPMEAN_MAX']]
print(df_test[df_test.isnull().any(axis=1)].SUBJECT_ID.unique())

from collections import Counter
from matplotlib import pyplot
import imblearn

x_scatter = df_features [['HR',
               'RESP']]

x_scatter = np.asarray(x_scatter)

y_scatter = df_features['HAS_SHOCK']

counter = Counter(y_scatter)
print(counter) #Counter({0: 7964, 1: 70})

import numpy as np    

# scatter plot of examples by class label
for label, _ in counter.items():
	row_ix = np.where(y_scatter == label)[0]
	pyplot.scatter(x_scatter[row_ix, 0], x_scatter[row_ix, 1], label=str(label))
 
pyplot.legend()
pyplot.show()

X_features = df_features[['HR',
               'RESP',
               'ABPSYS',
               'ABPDIAS',
               'ABPMEAN',
               'SPO2',
               'TEMP' ,
               'SOFA_SCORE',

               'HR_STD',
               'RESP_STD',
               'ABPSYS_STD',
               'ABPDIAS_STD',
               'ABPMEAN_STD',
               'SPO2_STD',
               'TEMP_STD',
               'SOFA_SCORE_STD',
               
               'HR_ENT',
               'RESP_ENT',
               'ABPSYS_ENT',
               'ABPDIAS_ENT',
               'ABPMEAN_ENT',
               'SPO2_ENT',
               'TEMP_ENT',
               'SOFA_SCORE_ENT',

               'ABPDIAS_HR_CORR',
               'RESP_HR_CORR',
               'ABPDIAS_ABPSYS_CORR',
               'ABPMEAN_ABPSYS_CORR',
               'ABPMEAN_ABPDIAS_CORR',

                'HR_MIN',
                'RESP_MIN',
                'SPO2_MIN',
                'TEMP_MIN',
                'SOFA_SCORE_MIN',
                'ABPSYS_MIN',
                'ABPDIAS_MIN',
                'ABPMEAN_MIN',
                'HR_MAX',
                'RESP_MAX',
                'SPO2_MAX',
                'TEMP_MAX',
                'SOFA_SCORE_MAX',
                'ABPSYS_MAX',
                'ABPDIAS_MAX',
                'ABPMEAN_MAX']]

y = df_features['HAS_SHOCK']

print(X_features.shape)
print(y.shape)

"""# **OVER SAMPLING OF SHOCK CLASS**"""

from imblearn.over_sampling import SMOTE

oversample = SMOTE()
X_oversample , y_oversample = oversample.fit_resample(X_features, y)

counter = Counter(y_oversample)
print(counter) # Counter({0: 7964, 1: 70})

print(X_oversample.shape)
print(y.shape)


x_scatter_oversample = X_oversample

x_scatter_oversample = np.asarray(x_scatter_oversample)

y_scatter_oversample = y_oversample

counter = Counter(y_scatter_oversample)
print(counter) #Counter({0: 7964, 1: 70})

import numpy as np    

# scatter plot of examples by class label
for label, _ in counter.items():
	row_ix = np.where(y_scatter_oversample == label)[0]
	pyplot.scatter(x_scatter_oversample[row_ix, 0], x_scatter_oversample[row_ix, 1], label=str(label))
 
pyplot.legend()
pyplot.show()

"""#**UNDER SAMPLING THE MAJORITY CLASS + OVER SAMPLING OF THE MINORITY CALSS**"""

from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

# original : Counter({0: 7964, 1: 70})

over = SMOTE(sampling_strategy=0.15) # 0.1
under = RandomUnderSampler(sampling_strategy=1)# 0.5
steps = [('o', over), ('u', under)]
pipeline = Pipeline(steps=steps)

y = y.astype(int)

# transform the dataset
X_both, y_both = pipeline.fit_resample(X_features, y)
# summarize the new class distribution
counter_both = Counter(y_both)
print(counter_both)
# scatter plot of examples by class label
for label, _ in counter.items():
	row_ix = np.where(y_both == label)[0]
	pyplot.scatter(X_both[row_ix, 0], X_both[row_ix, 1], label=str(label))
pyplot.legend()
pyplot.show()

"""# **k-fold LSTM classificaiton with SMOTE + RANDOMM UNDER SAMPLING**




"""



# test train spolit for patients

from sklearn.model_selection import train_test_split

try:
  df_final_cohort.drop(df_final_cohort.index, inplace= True)
except:
  print('df_final_cohort does not exists')

#extracting patient ids for shock and non-shock group that has less than 20 % mmissing data 
# from sepsis onset till sepsis onset + 31 hours or from sepsis onset time till shock onset time

df_cohort_temp = df_icutime[(df_icutime['Sepsis_SepsisOnset+ShockOnsetOR31h_timeoverlap_exists']==1)& (df_icutime['Sepsis_SepsisOnset+ShockOnsetOR31h_percentNonMissingData']>=80) ]
print(df_cohort_temp.shape[0])

"""
df_shock = df_sepsisOnset_SepsisOnsetPlus31h_csvdata[df_sepsisOnset_SepsisOnsetPlus31h_csvdata['sepstic_shock_onsettime'].notna()]
print(df_shock.shape[0])
print(df_shock.columns )
"""

df_cohort_temp['sepsis_onsettime'] =  pd.to_datetime(df_cohort_temp['sepsis_onsettime'])
df_cohort_temp['sepstic_shock_onsettime'] =  pd.to_datetime(df_cohort_temp['sepstic_shock_onsettime'])


# for non-shock
df_non_shock =  df_cohort_temp[df_cohort_temp['sepstic_shock_onsettime'].isna()]#.head(87) # adding head to have euqal number of shock and non shock patients 
print('non shock : ', df_non_shock.shape)

#for shock 
df_shock =  df_cohort_temp[df_cohort_temp['sepstic_shock_onsettime'].notna()]
print('shock : ', df_shock.shape)
"""
#for patients that got shock after sepsis onset + 1hour
df_shock_post1hourAfterSepsisOnset = df_shock[(df_shock['sepstic_shock_onsettime'] >= ( df_shock['sepsis_onsettime'] + datetime.timedelta(hours=1) )) ]
print(df_shock_post1hourAfterSepsisOnset.shape)
"""
df_final_cohort = pd.DataFrame(columns=df_cohort_temp.columns )
df_final_cohort =  df_final_cohort.append(df_non_shock); # including all non-shock patients
#df_final_cohort =  df_final_cohort.append(df_shock_post1hourAfterSepsisOnset); # including only shock patients who got shock after sepsis onset + 1 hour
df_final_cohort =  df_final_cohort.append(df_shock); # including all shock patients 


print(df_final_cohort.columns)
#print(df_final_cohort)

df_final_cohort = df_final_cohort[df_final_cohort['subject_id']!= 52875]

#x = x.apply(pd.to_numeric)

df_final_cohort['has_shock'] = np.where(df_final_cohort['sepstic_shock_onsettime'].isna(), 0 , 1 )


x_cohort = df_final_cohort['subject_id']

y_cohort = df_final_cohort['has_shock']


"""
x = df_final_cohort[['icustay_id', 'subject_id','sepsis_onsettime', 'sepstic_shock_onsettime']]

y = df_final_cohort['has_shock']




#print(x)
#print(y)
x_train_subjects, x_test_subjects, y_train_class, y_test_class = train_test_split(x, y, test_size=0.3,random_state=42)
"""

from sklearn.model_selection import KFold
import pandas as pd
import numpy as np
from collections import Counter


# Setting seed for reproducibility
np.random.seed(1234)
PYTHONHASHSEED = 0

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, recall_score, precision_score, accuracy_score, f1_score, balanced_accuracy_score, roc_auc_score
from tensorflow.python.keras.models import Sequential, load_model
import tensorflow as tf

from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE , ADASYN


df_kfold_class_results = pd.DataFrame(columns = ['TN' , 'FP', 'FN','TP','SENSITIVITY', 'SPECIFICITY',
                                                  'ROC_AUC','PRECISION' ,'ACCURACY',  'FSCORE','BAL_ACCURACY' ] )



kf = KFold(n_splits=5)
#print(kf)  
"""
x_cohort = x_cohort.apply(pd.to_numeric)
x_cohort = np.asarray(x_cohort)

y_cohort = y_cohort.apply(pd.to_numeric)
y_cohort = np.asarray(y_cohort)

"""
for train_index, test_index in kf.split(x_cohort):
    cur_kfold_idx = df_kfold_class_results.shape[0]
    x_train_subjects, x_test_subjects = x_cohort[train_index], x_cohort[test_index]
    y_train_class, y_test_class = y_cohort[train_index], y_cohort[test_index]

    print('Training data set size: ', x_train_subjects.shape[0])
    print('Test data set size: ', x_test_subjects.shape[0])
    print('-------------------------------------------------------')

    #subject_ids_train = x_train_subjects.subject_id.unique()
  
    #----------------

    def gen_sequence(id_df, seq_length, seq_cols):
      data_matrix = id_df[seq_cols].values
      # print(data_matrix)
      num_elements = data_matrix.shape[0]
      #print(num_elements)
      for start, stop in zip(range(0, num_elements - seq_length), range(seq_length, num_elements )):
          #print(start, stop)
          yield data_matrix[start:stop, :]


    # function to generate labels
    def gen_labels(id_df, seq_length, label):
        data_matrix = id_df[label].values
        num_elements = data_matrix.shape[0]
        return data_matrix[seq_length : num_elements , :]


    # Press the green button in the gutter to run the script.
    subject_ids_train = x_train_subjects
    if __name__ == '__main__':

        # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
        scaler = StandardScaler()
        # pick a window size
        sequence_length = 3 
        seq_array_main = []
        seq_array_main = np.array(seq_array_main)

        label_array_main = []
        label_array_main = np.array(label_array_main)

        count_train = 0 ;
        

      # pick the feature columns
        #sequence_cols = ['hr','abpSys','abpDias','abpMean','resp','sp02','SDhr','SDresp','SDsp02']
        sequence_cols = ['HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
      'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
      'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
      'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
      'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
      'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']
                    
        sequence_cols_all = ['TIME','ICUSTAY_ID','SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK','HOURS_BETWEEN_SEPSIS_SHOCK',
      'HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
      'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
      'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
      'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
      'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
      'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']

        for i_train in subject_ids_train : # [41782] : [86984 , 41782]:
          #print(i)
          try:
            train_df.drop(train_df.index, inplace = True)
          except:
            print('train_df does not exist')

          train_df = df_features[df_features['SUBJECT_ID']==i_train]
          
          if train_df.shape[0] >= (sequence_length + 1): 



            # first scale the values we are using as features
            train_df[sequence_cols] = scaler.fit_transform(train_df[sequence_cols])
          
            # here I am adding a label based on the sofa_Score
            #train_df['HAS_SHOCK'] = np.where(train_df.HAS_SHOCK == 1, 'shock','Nonshock')---comented

            
            train_df = train_df.loc[:,sequence_cols_all]

            #print('\nLabels distribution file: ', i)
            #print(train_df['HAS_SHOCK'].value_counts())

            
            # generate the sequences, of size sequence_length
            seq_gen = list(gen_sequence(train_df, sequence_length, sequence_cols))
            seq_array = np.array(list(seq_gen)).astype(np.float32)
            #print(seq_array)
            #print('individual sequence shape')
            #print('printing for main seq array : ',seq_array_main.shape)
            #print('printing for indv seq array : ',seq_array.shape)


            if seq_array_main.shape[0] == 0:
              seq_array_main = seq_array
            else:
              #print('printing for main seq array : ',seq_array_main.shape)
              #print('printing for indv seq array : ',seq_array.shape)
              seq_array_main = np.append(seq_array_main, seq_array, axis = 0) 


            # generate labels
            #print('before gen labels')
            #print(train_df)
            #label_gen = gen_labels(train_df, sequence_length, ['Nonshock', 'shock'])---commented
            label_gen = gen_labels(train_df, sequence_length, ['HAS_SHOCK'])
            label_array = np.array(label_gen).astype(np.float32)
            #print(label_array)
            #print('individual label shape')
            #print(label_array.shape)

            if label_array_main.shape[0] == 0:
              label_array_main = label_array
            else:
              label_array_main = np.append(label_array_main, label_array, axis = 0) 
            
            count_train = count_train + 1


            #appending to the main array for all patients
            #label_array_main = np.append(label_array_main, label_array)
          
          
        #print('printing for all patients')
        #print(seq_array_main.shape)
        #print(label_array_main.shape)
        #print('Number of train patients : ', count_train)
        #print(train_df['HAS_SHOCK'].value_counts())

        #----

        
    a_og = '';
    b_og = '';
    c_og = '';
    a_og = seq_array_main.shape[0]
    b_og = seq_array_main.shape[1]
    c_og = seq_array_main.shape[2]

    #print(a_og)
    #print(b_og)
    #print(c_og)

    seq_array_main_2d = seq_array_main.reshape(a_og,b_og*c_og)
    print('Printing original shape of seq array')
    print(seq_array_main.shape)

    print('printing 2D converted array shape ')
    print(seq_array_main_2d.shape)

    print('porinting label array shape before oversampleing')
    print(label_array_main.shape)
    label_array_main_1d = np.concatenate(label_array_main, axis=0 )
    

    #print('BEFORE OVERSAMPLING')
    #print(Counter(label_array_main_1d))  #Counter({0.0: 50, 1.0: 1})



    ##### trying to oversample

    over = SMOTE(sampling_strategy= 0.2) # 0.1
    under = RandomUnderSampler(sampling_strategy=1)# 0.5
    steps = [('o', over), ('u', under)]
    #steps = [('o', over)]
    #steps = [('u', under)]
    pipeline = Pipeline(steps=steps)

    print(seq_array_main_2d.shape)
    print(label_array_main_1d.shape)
    X_oversample , y_oversample =  pipeline.fit_resample(seq_array_main_2d, label_array_main_1d)
    counter = Counter(y_oversample)
    #print('AFTER OVERSAMPLING')
    #print(Counter(y_oversample)) # Counter({0: 7964, 1: 70})

    #print(X_oversample.shape)
    #print(y_oversample.shape)

    a_oversmp = X_oversample.shape[0]

    #print('CONVERTING BACK TO ORIGINAL SHAPE')
    seq_array_main_back_to_3d = X_oversample.reshape(a_oversmp,3,41)

    #print(seq_array_main_back_to_3d.shape)

    #-----------start commenting -------#
    """

    # Create model
    # number of features
    nb_features = seq_array_main_back_to_3d.shape[2]
    #print('nb_features : ' , nb_features)
    # number of classes
    nb_out = 1 #y_oversample.shape[1]


    # create model
    # model = vi.create_bi_model(nb_features, nb_out)
    model = Sequential()
    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=64, input_shape=(nb_features, nb_out))))
    model.add(tf.keras.layers.Dropout(rate=0.4))
    model.add(tf.keras.layers.Dense(units=nb_out))
        # opt = tf.keras.optimizers.SGD( 0.001)

    #model.compile(loss='mean_squared_error', optimizer='Adam', metrics=['accuracy']) --COMMENTED 
    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy']) 


    class_weight = {1.0: 0.85,
                      0.0: 0.15}


        # fit the network
    history = model.fit(seq_array_main_back_to_3d, y_oversample, epochs=100, batch_size=10, verbose=1, shuffle=False , class_weight=class_weight
                                # ,validation_split=0.05,
                                # callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='min')]
                                # ,tf.keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]
                                )

        # list all data in history
    #print(history.history.keys())

    scores = model.evaluate(seq_array_main, label_array_main, verbose=1, batch_size=16)
    #print('Accurracy: {}'.format(scores[1]))

    """
    ###-------------------end commenting---------------#
    
    alg = XGBClassifier(learning_rate=0.1, n_estimators=140, max_depth=5,
                        min_child_weight=3, gamma=0.2, subsample=0.6, colsample_bytree=1.0,
                        objective='binary:logistic', nthread=7, scale_pos_weight=4,   seed=27) 
    xgb_model = alg.fit(X_oversample,y_oversample)


    #---------------
    
    #-------------------

    # for testing

    subject_ids_test = x_test_subjects
    #print(len(subject_ids_test)) # 65 for 3 sequence length

    if __name__ == '__main__':

      # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
      scaler = StandardScaler()
      # pick a window size
      sequence_length = 3 
            
      subject_id_list = []
      has_shock_list = []
      number_hours_between_Sepsis_shock_list = []
      precision_list = []
      recall_list = []
      accuracy_list = []
      fscore_list = []
      y_true_list= []
      y_pred_list = []


      count_test = 0 ;
            

      # pick the feature columns
      #sequence_cols = ['hr','abpSys','abpDias','abpMean','resp','sp02','SDhr','SDresp','SDsp02']
      sequence_cols = ['HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
            'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
            'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
            'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
            'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
            'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']
                        
      sequence_cols_all = ['TIME','ICUSTAY_ID','SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK','HOURS_BETWEEN_SEPSIS_SHOCK',
            'HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
            'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
            'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
            'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
            'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
            'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']

      for i in subject_ids_test : #[87481] : #
        #print(i)
        try:
          test_df.drop(test_df.index, inplace = True)
        except:
          print('traintest_df_df does not exist')


        #test_df = x_test[x_test['SUBJECT_ID']==i]
        test_df = df_features[df_features['SUBJECT_ID']==i]
        #print('printing entire feature df')
        #print(test_df)
              
              
        if test_df.shape[0] >= (sequence_length + 1): 

          number_hours_between_Sepsis_shock = test_df.HOURS_BETWEEN_SEPSIS_SHOCK.unique()
          #has_shock_indv = test_df.HAS_SHOCK.unique()

          # first scale the values we are using as features
          test_df[sequence_cols] = scaler.fit_transform(test_df[sequence_cols])
              
          # here I am adding a label based on the sofa_Score
          #test_df['HAS_SHOCK'] = np.where(test_df.HAS_SHOCK == 1, 'shock','Nonshock')--commmmented

                
          test_df = test_df.loc[:,sequence_cols_all]

          #print('before dummies : ' ,train_df.columns )

                
          #label_encoding = pd.get_dummies(test_df.HAS_SHOCK)--commmmented
                
          #print(label_encoding)
          #test_df = pd.concat([test_df, label_encoding], axis=1)--commmmented
                
                
          has_shock_indv = 1
          # generate the sequences, of size sequence_length
          seq_gen_test = list(gen_sequence(test_df, sequence_length, sequence_cols))
          seq_array_test = np.array(list(seq_gen_test)).astype(np.float32)

          label_gen_test = gen_labels(test_df, sequence_length, ['HAS_SHOCK'])
          #print('printing after gen labels')
          #print(label_gen_test)
          label_array_test = np.array(label_gen_test).astype(np.float32)
          #print('after conversion, label array')
          #print(label_array_test)
          count_test = count_test + 1

          a_og_test = '';
          b_og_test = '';
          c_og_test = '';
          a_og_test = seq_array_test.shape[0]
          b_og_test = seq_array_test.shape[1]
          c_og_test = seq_array_test.shape[2]

          

          seq_array_test_2d = seq_array_test.reshape(a_og_test,b_og_test*c_og_test)

          # make predictions and compute confusion matrix
          y_pred = xgb_model.predict(seq_array_test_2d)

          # make predictions and compute confusion matrix
          #y_pred = model.predict_classes(seq_array_test, verbose=1, batch_size=16) -----commented
          
          y_true = label_array_test
          #print('printing prediction')
          #print(y_pred)
                

          #y_true = np.argmax(y_true, axis=1)--COMMENTED

          y_true_list.append(y_true)
          y_pred_list.append(y_pred)
          """
          cm = confusion_matrix(y_true, y_pred)

          # compute precision and recall
          precision = precision_score(y_true, y_pred)
          recall = recall_score(y_true, y_pred)
          accuracy = accuracy_score(y_true, y_pred)
          fscore =  f1_score(y_true, y_pred)

          #print('PRECISION : ', precision)
          cm = confusion_matrix(y_true, y_pred)
          subject_id_list.append(i) ;
          has_shock_list.append(has_shock_indv);
          number_hours_between_Sepsis_shock_list.append(number_hours_between_Sepsis_shock);
          recall_list.append(recall);
          precision_list.append(precision);
          accuracy_list.append(accuracy);
          fscore_list.append(fscore)
          """
           


      y_true_list_new = np.concatenate( y_true_list, axis=0 )
      #print(y_true_list)

      y_pred_list_new  = np.concatenate( y_pred_list, axis=0 )
      #print(y_pred_list_new)

      cm = confusion_matrix(y_true_list_new, y_pred_list_new)
      tn, fp, fn, tp = confusion_matrix(y_true_list_new, y_pred_list_new).ravel()

      precision_overall = precision_score(y_true_list_new, y_pred_list_new)
      recall_overall = recall_score(y_true_list_new, y_pred_list_new)
      accuracy_overall = accuracy_score(y_true_list_new, y_pred_list_new)
      fscore_overall =  f1_score(y_true_list_new, y_pred_list_new)
      balanced_accuracy_overall = balanced_accuracy_score(y_true_list_new, y_pred_list_new)
      #roc_auc_score_overall = roc_auc_score(y_true_list_new, y_pred_list_new) 
      specificity_overall = tn / (tn + fp)
    

      df_kfold_class_results.loc[cur_kfold_idx, 'TN'] = tn
      df_kfold_class_results.loc[cur_kfold_idx, 'FP'] = fp
      df_kfold_class_results.loc[cur_kfold_idx, 'FN'] = fn
      df_kfold_class_results.loc[cur_kfold_idx, 'TP'] =  tp

      df_kfold_class_results.loc[cur_kfold_idx, 'SENSITIVITY'] = recall_overall
      df_kfold_class_results.loc[cur_kfold_idx, 'SPECIFICITY'] = specificity_overall
      #df_kfold_class_results.loc[cur_kfold_idx, 'ROC_AUC'] = roc_auc_score_overall
      df_kfold_class_results.loc[cur_kfold_idx, 'PRECISION'] = precision_overall
      df_kfold_class_results.loc[cur_kfold_idx, 'ACCURACY'] = accuracy_overall
      df_kfold_class_results.loc[cur_kfold_idx, 'FSCORE'] = fscore_overall
      df_kfold_class_results.loc[cur_kfold_idx, 'BAL_ACCURACY'] = balanced_accuracy_overall

print(df_kfold_class_results)

"""# **for 70-30 set**"""

# first run till df_Features


# test train spolit for patients
#
from sklearn.model_selection import KFold
import pandas as pd
import numpy as np
from collections import Counter


# Setting seed for reproducibility
np.random.seed(1234)
PYTHONHASHSEED = 0

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, recall_score, precision_score, accuracy_score, f1_score, balanced_accuracy_score, roc_auc_score
from tensorflow.python.keras.models import Sequential, load_model
import tensorflow as tf

from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE , ADASYN


from sklearn.model_selection import train_test_split

try:
  df_final_cohort.drop(df_final_cohort.index, inplace= True)
except:
  print('df_final_cohort does not exists')

#extracting patient ids for shock and non-shock group that has less than 20 % mmissing data 
# from sepsis onset till sepsis onset + 31 hours or from sepsis onset time till shock onset time

df_cohort_temp = df_icutime[(df_icutime['Sepsis_SepsisOnset+ShockOnsetOR31h_timeoverlap_exists']==1)& (df_icutime['Sepsis_SepsisOnset+ShockOnsetOR31h_percentNonMissingData']>=80) ]
print(df_cohort_temp.shape[0])

"""
df_shock = df_sepsisOnset_SepsisOnsetPlus31h_csvdata[df_sepsisOnset_SepsisOnsetPlus31h_csvdata['sepstic_shock_onsettime'].notna()]
print(df_shock.shape[0])
print(df_shock.columns )
"""

df_cohort_temp['sepsis_onsettime'] =  pd.to_datetime(df_cohort_temp['sepsis_onsettime'])
df_cohort_temp['sepstic_shock_onsettime'] =  pd.to_datetime(df_cohort_temp['sepstic_shock_onsettime'])


# for non-shock
df_non_shock =  df_cohort_temp[df_cohort_temp['sepstic_shock_onsettime'].isna()]#.head(87) # adding head to have euqal number of shock and non shock patients 
print('non shock : ', df_non_shock.shape)

#for shock 
df_shock =  df_cohort_temp[df_cohort_temp['sepstic_shock_onsettime'].notna()]
print('shock : ', df_shock.shape)
"""
#for patients that got shock after sepsis onset + 1hour
df_shock_post1hourAfterSepsisOnset = df_shock[(df_shock['sepstic_shock_onsettime'] >= ( df_shock['sepsis_onsettime'] + datetime.timedelta(hours=1) )) ]
print(df_shock_post1hourAfterSepsisOnset.shape)
"""
df_final_cohort = pd.DataFrame(columns=df_cohort_temp.columns )
df_final_cohort =  df_final_cohort.append(df_non_shock); # including all non-shock patients
#df_final_cohort =  df_final_cohort.append(df_shock_post1hourAfterSepsisOnset); # including only shock patients who got shock after sepsis onset + 1 hour
df_final_cohort =  df_final_cohort.append(df_shock); # including all shock patients 


print(df_final_cohort.columns)
#print(df_final_cohort)

#df_final_cohort = df_final_cohort[df_final_cohort['subject_id']!= 52875]

#x = x.apply(pd.to_numeric)

df_final_cohort['has_shock'] = np.where(df_final_cohort['sepstic_shock_onsettime'].isna(), 0 , 1 )


x = df_final_cohort[['icustay_id', 'subject_id','sepsis_onsettime', 'sepstic_shock_onsettime']]

y = df_final_cohort['has_shock']




#print(x)
#print(y)
x_train_subjects, x_test_subjects, y_train_class, y_test_class = train_test_split(x, y, test_size=0.3,random_state=42)

def gen_sequence(id_df, seq_length, seq_cols):
  data_matrix = id_df[seq_cols].values
  # print(data_matrix)
  num_elements = data_matrix.shape[0]
  #print(num_elements)
  for start, stop in zip(range(0, num_elements - seq_length), range(seq_length, num_elements )):
      #print(start, stop)
      yield data_matrix[start:stop, :]


# function to generate labels
def gen_labels(id_df, seq_length, label):
  data_matrix = id_df[label].values
  num_elements = data_matrix.shape[0]
  return data_matrix[seq_length : num_elements , :]


# Press the green button in the gutter to run the script.
subject_ids_train = x_train_subjects.subject_id.unique()
if __name__ == '__main__':

    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
    scaler = StandardScaler()
    # pick a window size
    sequence_length = 3 
    seq_array_main = []
    seq_array_main = np.array(seq_array_main)

    label_array_main = []
    label_array_main = np.array(label_array_main)

    count_train = 0 ;
        

    # pick the feature columns
    #sequence_cols = ['hr','abpSys','abpDias','abpMean','resp','sp02','SDhr','SDresp','SDsp02']
    sequence_cols = ['HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
      'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
      'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
      'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
      'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
      'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']
                    
    sequence_cols_all = ['TIME','ICUSTAY_ID','SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK','HOURS_BETWEEN_SEPSIS_SHOCK',
      'HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
      'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
      'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
      'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
      'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
      'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']

    for i_train in subject_ids_train : # [41782] : [86984 , 41782]:
      #print(i)
      try:
        train_df.drop(train_df.index, inplace = True)
      except:
        print('train_df does not exist')

      train_df = df_features[df_features['SUBJECT_ID']==i_train]
          
      if train_df.shape[0] >= (sequence_length + 1): 

        # first scale the values we are using as features
        train_df[sequence_cols] = scaler.fit_transform(train_df[sequence_cols])
          
        # here I am adding a label based on the sofa_Score
        #train_df['HAS_SHOCK'] = np.where(train_df.HAS_SHOCK == 1, 'shock','Nonshock')---comented

            
        train_df = train_df.loc[:,sequence_cols_all]

        #print('\nLabels distribution file: ', i)
        #print(train_df['HAS_SHOCK'].value_counts())

            
        # generate the sequences, of size sequence_length
        seq_gen = list(gen_sequence(train_df, sequence_length, sequence_cols))
        seq_array = np.array(list(seq_gen)).astype(np.float32)
        #print(seq_array)
        #print('individual sequence shape')
        #print('printing for main seq array : ',seq_array_main.shape)
        #print('printing for indv seq array : ',seq_array.shape)


        if seq_array_main.shape[0] == 0:
          seq_array_main = seq_array
        else:
          #print('printing for main seq array : ',seq_array_main.shape)
          #print('printing for indv seq array : ',seq_array.shape)
          seq_array_main = np.append(seq_array_main, seq_array, axis = 0) 


        # generate labels
        #print('before gen labels')
        #print(train_df)
        #label_gen = gen_labels(train_df, sequence_length, ['Nonshock', 'shock'])---commented
        label_gen = gen_labels(train_df, sequence_length, ['HAS_SHOCK'])
        label_array = np.array(label_gen).astype(np.float32)
        #print(label_array)
        #print('individual label shape')
        #print(label_array.shape)

        if label_array_main.shape[0] == 0:
          label_array_main = label_array
        else:
          label_array_main = np.append(label_array_main, label_array, axis = 0) 
            
        count_train = count_train + 1

print('Number of train patients : ', count_train)
print(seq_array_main.shape)
print(label_array_main.shape)

print(df_features[df_features['SUBJECT_ID']==86984])

from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE , ADASYN
pd.set_option('display.max_rows', 1200)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
a_og = '';
b_og = '';
c_og = '';
a_og = seq_array_main.shape[0]
b_og = seq_array_main.shape[1]
c_og = seq_array_main.shape[2]

print(a_og)
print(b_og)
print(c_og)

seq_array_main_2d = seq_array_main.reshape(a_og,b_og*c_og)

print('printing 2D converted array shape ')
print(seq_array_main_2d.shape)

#print(label_array_main)
label_array_main_1d = np.concatenate(label_array_main, axis=0 )

print('BEFORE OVERSAMPLING')
print(Counter(label_array_main_1d))  #Counter({0.0: 50, 1.0: 1})


##### trying to oversample

over = SMOTE(sampling_strategy=0.02) # 0.01
under = RandomUnderSampler(sampling_strategy=1)# 1
steps = [('o', over), ('u', under)]
#steps = [('o', over)]
#steps = [('u', under)]
pipeline = Pipeline(steps=steps)

#
X_oversample , y_oversample =  pipeline.fit_resample(seq_array_main_2d, label_array_main_1d)# oversample.fit_resample(seq_array_main_2d, label_array_main_1d)
counter = Counter(y_oversample)
print('AFTER OVERSAMPLING')
print(Counter(y_oversample)) # Counter({0: 7964, 1: 70})

print(X_oversample.shape)
print(y_oversample.shape)

a_oversmp = X_oversample.shape[0]

print('CONVERTING BACK TO ORIGINAL SHAPE')
seq_array_main_back_to_3d = X_oversample.reshape(a_oversmp,5,41)

print(seq_array_main_back_to_3d.shape)

# Create model
# number of features
nb_features = seq_array_main_back_to_3d.shape[2]
#print('nb_features : ' , nb_features)
# number of classes
nb_out = 1 #y_oversample.shape[1]

# create model
# model = vi.create_bi_model(nb_features, nb_out)
model = Sequential()
model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=64, input_shape=(nb_features, nb_out))))
model.add(tf.keras.layers.Dropout(rate=0.4))
model.add(tf.keras.layers.Dense(units=nb_out))
# opt = tf.keras.optimizers.SGD( 0.001)

#model.compile(loss='mean_squared_error', optimizer='Adam', metrics=['accuracy']) --COMMENTED 
model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy']) 

class_weight = {1.0: 90,
                0.0: 10}


# fit the network
history = model.fit(seq_array_main_back_to_3d, y_oversample, epochs=100, batch_size=10, verbose=1, shuffle=False , class_weight=class_weight
                                # ,validation_split=0.05,
                                # callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='min')]
                                # ,tf.keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]
                                )

# list all data in history
#print(history.history.keys())

scores = model.evaluate(seq_array_main, label_array_main, verbose=1, batch_size=16)
print('Accurracy: {}'.format(scores[1]))

subject_ids_test = x_test_subjects.subject_id.unique()
    #print(len(subject_ids_test)) # 65 for 3 sequence length

if __name__ == '__main__':

  # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
  scaler = StandardScaler()
  # pick a window size
  sequence_length = 5 
            
  subject_id_list = []
  has_shock_list = []
  number_hours_between_Sepsis_shock_list = []
  precision_list = []
  recall_list = []
  accuracy_list = []
  fscore_list = []
  y_true_list= []
  y_pred_list = []

  balanced_accuracy_list = []
  roc_auc_score_list= []
  specificity_list= []

  count_test = 0 ;
            

  # pick the feature columns
  #sequence_cols = ['hr','abpSys','abpDias','abpMean','resp','sp02','SDhr','SDresp','SDsp02']
  sequence_cols = ['HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
            'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
            'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
            'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
            'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
            'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']
                        
  sequence_cols_all = ['TIME','ICUSTAY_ID','SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK','HOURS_BETWEEN_SEPSIS_SHOCK',
            'HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
            'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
            'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
            'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
            'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
            'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']

  for i in subject_ids_test : #[87481] : #
    #print(i)
    try:
      test_df.drop(test_df.index, inplace = True)
    except:
      print('traintest_df_df does not exist')


    #test_df = x_test[x_test['SUBJECT_ID']==i]
    test_df = df_features[df_features['SUBJECT_ID']==i]
    print('printing entire feature df')
    #print(test_df)
              
              
    if test_df.shape[0] >= (sequence_length + 1): 

      number_hours_between_Sepsis_shock = test_df.HOURS_BETWEEN_SEPSIS_SHOCK.unique()
      #has_shock_indv = test_df.HAS_SHOCK.unique()

      # first scale the values we are using as features
      test_df[sequence_cols] = scaler.fit_transform(test_df[sequence_cols])
              
      # here I am adding a label based on the sofa_Score
      #test_df['HAS_SHOCK'] = np.where(test_df.HAS_SHOCK == 1, 'shock','Nonshock')--commmmented

                
      test_df = test_df.loc[:,sequence_cols_all]

      #print('before dummies : ' ,train_df.columns )

                
      #label_encoding = pd.get_dummies(test_df.HAS_SHOCK)--commmmented
                
      #print(label_encoding)
      #test_df = pd.concat([test_df, label_encoding], axis=1)--commmmented
                
                
      has_shock_indv = 1
      # generate the sequences, of size sequence_length
      seq_gen_test = list(gen_sequence(test_df, sequence_length, sequence_cols))
      seq_array_test = np.array(list(seq_gen_test)).astype(np.float32)

      label_gen_test = gen_labels(test_df, sequence_length, ['HAS_SHOCK'])
      #print('printing after gen labels')
      #print(label_gen_test)
      label_array_test = np.array(label_gen_test).astype(np.float32)
      #print('after conversion, label array')
      #print(label_array_test)
      count_test = count_test + 1

      # make predictions and compute confusion matrix
      y_pred = model.predict_classes(seq_array_test, verbose=1, batch_size=16)
      y_true = label_array_test
      #print('printing prediction')
      #print(y_pred)
                

      #y_true = np.argmax(y_true, axis=1)--COMMENTED

      y_true_list.append(y_true)
      y_pred_list.append(y_pred)
          
      
      y_true = np.concatenate(y_true , axis=0 )
      y_true = y_true.astype(int)
      y_pred  = np.concatenate( y_pred, axis=0 )
      y_pred = y_pred.astype(int)

      print(y_true)
      print(y_pred)
      
      CM = confusion_matrix(y_true, y_pred)
      try:
        tn = CM[0][0]
      except:
        tn = 0;

      try:
        fn = CM[1][0]
      except:
        fn =0 ;

      try:
       tp = CM[1][1]
      except:
        tp = 0

      try:
        fp = CM[0][1]
      except:
        fp = 0 ;

      

      # compute precision and recall
      precision = precision_score(y_true, y_pred)
      recall = recall_score(y_true, y_pred)
      accuracy = accuracy_score(y_true, y_pred)
      fscore =  f1_score(y_true, y_pred)


      balanced_accuracy = balanced_accuracy_score(y_true, y_pred)
      try:
        roc_auc_score= roc_auc_score(y_true, y_pred)
      except:
        roc_auc_score = 0

      specificity = tn / (tn + fp)

      #print('PRECISION : ', precision)
      cm = confusion_matrix(y_true, y_pred)
      subject_id_list.append(i) ;
      has_shock_list.append(has_shock_indv);
      number_hours_between_Sepsis_shock_list.append(number_hours_between_Sepsis_shock);
      recall_list.append(recall);
      precision_list.append(precision);
      accuracy_list.append(accuracy);
      fscore_list.append(fscore)
      balanced_accuracy_list.append(balanced_accuracy)
      roc_auc_score_list.append(roc_auc_score)
      specificity_list.append(specificity)

#printing confusion matrix for all the patients overall
import numpy 

y_true_list_new = numpy.concatenate( y_true_list, axis=0 )
#print(y_true_list)
y_true_list_new = y_true_list_new.astype(int)
y_true_list_new = np.asarray(y_true_list_new)



y_pred_list_new  = np.concatenate( y_pred_list, axis=0 )
#print(y_pred_list_new)
y_pred_list_new = y_pred_list_new.astype(int)
y_pred_list_new = np.asarray(y_pred_list_new)


print('Count of labels for true labels')
print(np.array(np.unique(y_true_list_new, return_counts=True)).T)

cm = confusion_matrix(y_true_list_new, y_pred_list_new)
print(cm)
tn, fp, fn, tp = confusion_matrix(y_true_list_new, y_pred_list_new).ravel()


print('TN : ', tn)
print('FP : ', fp)
print('FN : ', fn)
print('TP : ', tp)

precision_overall = precision_score(y_true_list_new, y_pred_list_new)
recall_overall = recall_score(y_true_list_new, y_pred_list_new)
accuracy_overall = accuracy_score(y_true_list_new, y_pred_list_new)
fscore_overall =  f1_score(y_true_list_new, y_pred_list_new)
balanced_accuracy_overall = balanced_accuracy_score(y_true_list_new, y_pred_list_new)
specificity_overall = tn / (tn + fp)
try:
  roc_auc_score_overall = roc_auc_score(y_true, y_pred)
except:
  roc_auc_score_overall = 0

print('RECALL : ', recall_overall)
print('SPECIFICITY: ', specificity_overall)

print('ACCURACY : ', accuracy_overall )
print('PRECISION : ', precision_overall)
print('F-SCORE : ', fscore_overall)
print('BALANCED ACCURACY : ', balanced_accuracy_overall)
print('ROC AUC : ', roc_auc_score_overall  )

df_test_results_indiv = pd.DataFrame(columns= ['SUBJECT_ID','HAS_SHOCK', 'HOURS_BETWEEN_SEPSIS_SHOCK','ACCURACY','FSCORE','PRECISION','RECALL','SPECIFICITY','BALANCED_ACCURACY'])
df_test_results_indiv['SUBJECT_ID'] = subject_id_list
#df_test_results_indiv['HAS_SHOCK'] = has_shock_list
df_test_results_indiv['HOURS_BETWEEN_SEPSIS_SHOCK'] = number_hours_between_Sepsis_shock_list
df_test_results_indiv['ACCURACY'] = accuracy_list
df_test_results_indiv['PRECISION'] = precision_list
df_test_results_indiv['RECALL'] = recall_list
df_test_results_indiv['FSCORE'] = fscore_list
df_test_results_indiv['SPECIFICITY'] = specificity_list
df_test_results_indiv['BALANCED_ACCURACY'] = balanced_accuracy_list
df_test_results_indiv['HAS_SHOCK'] = np.where(df_test_results_indiv.HOURS_BETWEEN_SEPSIS_SHOCK == 31, 'Nonshock','Shock')


print(df_test_results_indiv.shape)
print(df_test_results_indiv.sort_values("RECALL"))

"""# **TRYING CLASSIFICATION WITH XGBOOST**"""

def gen_sequence(id_df, seq_length, seq_cols):
  data_matrix = id_df[seq_cols].values
  # print(data_matrix)
  num_elements = data_matrix.shape[0]
  #print(num_elements)
  for start, stop in zip(range(0, num_elements - seq_length), range(seq_length, num_elements )):
      #print(start, stop)
      yield data_matrix[start:stop, :]


# function to generate labels
def gen_labels(id_df, seq_length, label):
  data_matrix = id_df[label].values
  num_elements = data_matrix.shape[0]
  return data_matrix[seq_length : num_elements , :]


# Press the green button in the gutter to run the script.
subject_ids_train = x_train_subjects.subject_id.unique()
if __name__ == '__main__':

    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
    scaler = StandardScaler()
    # pick a window size
    sequence_length = 3 
    seq_array_main = []
    seq_array_main = np.array(seq_array_main)

    label_array_main = []
    label_array_main = np.array(label_array_main)

    count_train = 0 ;
        

    # pick the feature columns
    #sequence_cols = ['hr','abpSys','abpDias','abpMean','resp','sp02','SDhr','SDresp','SDsp02']
    sequence_cols = ['HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
      'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
      'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
      'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
      'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
      'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']
                    
    sequence_cols_all = ['TIME','ICUSTAY_ID','SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK','HOURS_BETWEEN_SEPSIS_SHOCK',
      'HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
      'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
      'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
      'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
      'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
      'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']

    for i_train in subject_ids_train : # [41782] : [86984 , 41782]:
      #print(i)
      try:
        train_df.drop(train_df.index, inplace = True)
      except:
        print('train_df does not exist')

      train_df = df_features[df_features['SUBJECT_ID']==i_train]
          
      if train_df.shape[0] >= (sequence_length + 1): 

        # first scale the values we are using as features
        train_df[sequence_cols] = scaler.fit_transform(train_df[sequence_cols])
          
        # here I am adding a label based on the sofa_Score
        #train_df['HAS_SHOCK'] = np.where(train_df.HAS_SHOCK == 1, 'shock','Nonshock')---comented

            
        train_df = train_df.loc[:,sequence_cols_all]

        #print('\nLabels distribution file: ', i)
        #print(train_df['HAS_SHOCK'].value_counts())

            
        # generate the sequences, of size sequence_length
        seq_gen = list(gen_sequence(train_df, sequence_length, sequence_cols))
        seq_array = np.array(list(seq_gen)).astype(np.float32)
        #print(seq_array)
        #print('individual sequence shape')
        #print('printing for main seq array : ',seq_array_main.shape)
        #print('printing for indv seq array : ',seq_array.shape)


        if seq_array_main.shape[0] == 0:
          seq_array_main = seq_array
        else:
          #print('printing for main seq array : ',seq_array_main.shape)
          #print('printing for indv seq array : ',seq_array.shape)
          seq_array_main = np.append(seq_array_main, seq_array, axis = 0) 


        # generate labels
        #print('before gen labels')
        #print(train_df)
        #label_gen = gen_labels(train_df, sequence_length, ['Nonshock', 'shock'])---commented
        label_gen = gen_labels(train_df, sequence_length, ['HAS_SHOCK'])
        label_array = np.array(label_gen).astype(np.float32)
        #print(label_array)
        #print('individual label shape')
        #print(label_array.shape)

        if label_array_main.shape[0] == 0:
          label_array_main = label_array
        else:
          label_array_main = np.append(label_array_main, label_array, axis = 0) 
            
        count_train = count_train + 1

# first run till df_Features + cohort selection + extracting train features


from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE , ADASYN
pd.set_option('display.max_rows', 1200)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
a_og = '';
b_og = '';
c_og = '';
a_og = seq_array_main.shape[0]
b_og = seq_array_main.shape[1]
c_og = seq_array_main.shape[2]

print(a_og)
print(b_og)
print(c_og)

seq_array_main_2d = seq_array_main.reshape(a_og,b_og*c_og)

print('printing 2D converted array shape ')
print(seq_array_main_2d.shape)

#print(label_array_main)
label_array_main_1d = np.concatenate(label_array_main, axis=0 )

print('BEFORE OVERSAMPLING')
print(Counter(label_array_main_1d))  #Counter({0.0: 50, 1.0: 1})


##### trying to oversample

over = SMOTE(sampling_strategy=0.018) # 0.01
under = RandomUnderSampler(sampling_strategy=1)# 1
steps = [('o', over), ('u', under)]
#steps = [('o', over)]
#steps = [('u', under)]
pipeline = Pipeline(steps=steps)

#
X_oversample , y_oversample =  pipeline.fit_resample(seq_array_main_2d, label_array_main_1d)# oversample.fit_resample(seq_array_main_2d, label_array_main_1d)
counter = Counter(y_oversample)
print('AFTER OVERSAMPLING')
print(Counter(y_oversample)) # Counter({0: 7964, 1: 70})

print(X_oversample.shape)
print(y_oversample.shape)

a_oversmp = X_oversample.shape[0]

print('CONVERTING BACK TO ORIGINAL SHAPE')
seq_array_main_back_to_3d = X_oversample.reshape(a_oversmp,3,41) # 3, 41

print(seq_array_main_back_to_3d.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.neural_network import MLPClassifier
from imblearn.pipeline import make_pipeline as make_pipeline_imb
from imblearn.over_sampling import SMOTE
from imblearn.metrics import classification_report_imbalanced
from xgboost import XGBClassifier
import xgboost as xgb
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
import time
from xgboost import plot_importance
from matplotlib import pyplot

"""
alg = XGBClassifier(learning_rate = 0.02,n_estimators=1000, max_depth=2,subsample= 0.4, colsample_bytree= 0.4, gamma= 0 ,min_child_weight=0.2, 
                    scale_pos_weight = 0.3, reg_alpha=0.001, booster='gbtree',objective='binary:logistic')
"""

#alg = XGBClassifier(max_depth=5,)

eval_metrics= ['auc']
eval_set = [(X_oversample, y_oversample)]


"""
alg = XGBClassifier(learning_rate=0.1, n_estimators=140, max_depth=5,
                        min_child_weight=3, gamma=0.2, subsample=0.6, colsample_bytree=1.0,
                        objective='binary:logistic', nthread=7, scale_pos_weight=4,   seed=27) #  scale_pos_weight=58,scale_pos_weight=4,

"""

alg = XGBClassifier()
xgb_model = alg.fit(X_oversample, y_oversample, eval_metric=eval_metrics,eval_set=eval_set, verbose=False)

#subject_ids_test = x_test_subjects.subject_id.unique()
    #print(len(subject_ids_test)) # 65 for 3 sequence length

if __name__ == '__main__':

  # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
  scaler = StandardScaler()
  # pick a window size
  sequence_length = 3 
            
  subject_id_list = []
  has_shock_list = []
  number_hours_between_Sepsis_shock_list = []
  precision_list = []
  recall_list = []
  accuracy_list = []
  fscore_list = []
  y_true_list= []
  y_pred_list = []

  balanced_accuracy_list = []
  roc_auc_score_list= []
  specificity_list= []

  count_test = 0 ;
            

  # pick the feature columns
  #sequence_cols = ['hr','abpSys','abpDias','abpMean','resp','sp02','SDhr','SDresp','SDsp02']
  sequence_cols = ['HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
      'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
      'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
      'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
      'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
      'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']
                    
  sequence_cols_all = ['TIME','ICUSTAY_ID','SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK','HOURS_BETWEEN_SEPSIS_SHOCK',
      'HR', 'RESP', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'SPO2', 'TEMP','SOFA_SCORE',
      'HR_STD', 'RESP_STD', 'ABPSYS_STD', 'ABPDIAS_STD', 'ABPMEAN_STD', 'SPO2_STD', 'TEMP_STD', 
      'HR_ENT', 'RESP_ENT', 'ABPSYS_ENT', 'ABPDIAS_ENT', 'ABPMEAN_ENT', 'SPO2_ENT', 'TEMP_ENT',  
      'ABPDIAS_HR_CORR', 'RESP_HR_CORR', 'ABPDIAS_ABPSYS_CORR', 'ABPMEAN_ABPSYS_CORR', 'ABPMEAN_ABPDIAS_CORR', 
      'HR_MIN', 'RESP_MIN', 'SPO2_MIN', 'TEMP_MIN','ABPSYS_MIN', 'ABPDIAS_MIN', 'ABPMEAN_MIN', 
      'HR_MAX', 'RESP_MAX', 'SPO2_MAX', 'TEMP_MAX', 'ABPSYS_MAX', 'ABPDIAS_MAX', 'ABPMEAN_MAX']

  for i in [87481] : #subject_ids_test : #[87481] : #
    #print(i)
    try:
      test_df.drop(test_df.index, inplace = True)
    except:
      print('traintest_df_df does not exist')


    #test_df = x_test[x_test['SUBJECT_ID']==i]
    test_df = df_features[df_features['SUBJECT_ID']==i]
    print('printing entire feature df')
    #print(test_df)
              
              
    if test_df.shape[0] >= (sequence_length + 1): 

      number_hours_between_Sepsis_shock = test_df.HOURS_BETWEEN_SEPSIS_SHOCK.unique()
      #has_shock_indv = test_df.HAS_SHOCK.unique()

      # first scale the values we are using as features
      test_df[sequence_cols] = scaler.fit_transform(test_df[sequence_cols])
              
      # here I am adding a label based on the sofa_Score
      #test_df['HAS_SHOCK'] = np.where(test_df.HAS_SHOCK == 1, 'shock','Nonshock')--commmmented

                
      test_df = test_df.loc[:,sequence_cols_all]

      #print('before dummies : ' ,train_df.columns )

                
      #label_encoding = pd.get_dummies(test_df.HAS_SHOCK)--commmmented
                
      #print(label_encoding)
      #test_df = pd.concat([test_df, label_encoding], axis=1)--commmmented
                
                
      has_shock_indv = 1
      # generate the sequences, of size sequence_length
      seq_gen_test = list(gen_sequence(test_df, sequence_length, sequence_cols))
      seq_array_test = np.array(list(seq_gen_test)).astype(np.float32)

      label_gen_test = gen_labels(test_df, sequence_length, ['HAS_SHOCK'])
      #print('printing after gen labels')
      #print(label_gen_test)
      label_array_test = np.array(label_gen_test).astype(np.float32)
      #print('after conversion, label array')
      #print(label_array_test)
      count_test = count_test + 1

      # reshape seq array test
      a_og_test = '';
      b_og_test = '';
      c_og_test = '';
      a_og_test = seq_array_test.shape[0]
      b_og_test = seq_array_test.shape[1]
      c_og_test = seq_array_test.shape[2]

      

      seq_array_test_2d = seq_array_test.reshape(a_og_test,b_og_test*c_og_test)

      # make predictions and compute confusion matrix
      y_pred = xgb_model.predict(seq_array_test_2d)
      y_pred_probab = xgb_model.predict_proba(seq_array_test_2d)
      y_true = label_array_test
      #print('printing prediction')
      #print(y_pred)
                

      #y_true = np.argmax(y_true, axis=1)--COMMENTED

      y_true_list.append(y_true)
      y_pred_list.append(y_pred)
          
      
      y_true = np.concatenate(y_true , axis=0 )
      y_true = y_true.astype(int)
      #y_pred  = np.concatenate( y_pred, axis=0 )
      y_pred = y_pred.astype(int)

      print(y_true)
      print(y_pred)
      
      CM = confusion_matrix(y_true, y_pred)
      try:
        tn = CM[0][0]
      except:
        tn = 0;

      try:
        fn = CM[1][0]
      except:
        fn =0 ;

      try:
       tp = CM[1][1]
      except:
        tp = 0

      try:
        fp = CM[0][1]
      except:
        fp = 0 ;

      

      # compute precision and recall
      precision = precision_score(y_true, y_pred)
      recall = recall_score(y_true, y_pred)
      accuracy = accuracy_score(y_true, y_pred)
      fscore =  f1_score(y_true, y_pred)


      balanced_accuracy = balanced_accuracy_score(y_true, y_pred)
      try:
        roc_auc_score= roc_auc_score(y_true, y_pred_probab)
      except:
        roc_auc_score = 0
        
      specificity = tn / (tn + fp)

      #print('PRECISION : ', precision)
      cm = confusion_matrix(y_true, y_pred)
      subject_id_list.append(i) ;
      has_shock_list.append(has_shock_indv);
      number_hours_between_Sepsis_shock_list.append(number_hours_between_Sepsis_shock);
      recall_list.append(recall);
      precision_list.append(precision);
      accuracy_list.append(accuracy);
      fscore_list.append(fscore)
      balanced_accuracy_list.append(balanced_accuracy)
      roc_auc_score_list.append(roc_auc_score)
      specificity_list.append(specificity)

#printing confusion matrix for all the patients overall
import numpy 

y_true_list_new = numpy.concatenate( y_true_list, axis=0 )
#print(y_true_list)
y_true_list_new = y_true_list_new.astype(int)
y_true_list_new = np.asarray(y_true_list_new)



y_pred_list_new  = np.concatenate( y_pred_list, axis=0 )
#print(y_pred_list_new)
y_pred_list_new = y_pred_list_new.astype(int)
y_pred_list_new = np.asarray(y_pred_list_new)


print('Count of labels for true labels')
print(np.array(np.unique(y_true_list_new, return_counts=True)).T)

cm = confusion_matrix(y_true_list_new, y_pred_list_new)
print(cm)
tn, fp, fn, tp = confusion_matrix(y_true_list_new, y_pred_list_new).ravel()


print('TN : ', tn)
print('FP : ', fp)
print('FN : ', fn)
print('TP : ', tp)

precision_overall = precision_score(y_true_list_new, y_pred_list_new)
recall_overall = recall_score(y_true_list_new, y_pred_list_new)
accuracy_overall = accuracy_score(y_true_list_new, y_pred_list_new)
fscore_overall =  f1_score(y_true_list_new, y_pred_list_new)
balanced_accuracy_overall = balanced_accuracy_score(y_true_list_new, y_pred_list_new)
specificity_overall = tn / (tn + fp)
try:
  roc_auc_score_overall = roc_auc_score(y_true, y_pred)
except:
  roc_auc_score_overall = 0

print('RECALL : ', recall_overall)
print('SPECIFICITY: ', specificity_overall)

print('ACCURACY : ', accuracy_overall )
print('PRECISION : ', precision_overall)
print('F-SCORE : ', fscore_overall)
print('BALANCED ACCURACY : ', balanced_accuracy_overall)
print('ROC AUC : ', roc_auc_score_overall  )
"""

"""

df_test_results_indiv = pd.DataFrame(columns= ['SUBJECT_ID','HAS_SHOCK', 'HOURS_BETWEEN_SEPSIS_SHOCK',
                                               'ACCURACY','FSCORE','PRECISION','RECALL','SPECIFICITY','BALANCED_ACCURACY'])
df_test_results_indiv['SUBJECT_ID'] = subject_id_list
#df_test_results_indiv['HAS_SHOCK'] = has_shock_list
df_test_results_indiv['HOURS_BETWEEN_SEPSIS_SHOCK'] = number_hours_between_Sepsis_shock_list
df_test_results_indiv['ACCURACY'] = accuracy_list
df_test_results_indiv['PRECISION'] = precision_list
df_test_results_indiv['RECALL'] = recall_list
df_test_results_indiv['FSCORE'] = fscore_list
df_test_results_indiv['SPECIFICITY'] = specificity_list
df_test_results_indiv['BALANCED_ACCURACY'] = balanced_accuracy_list
df_test_results_indiv['HAS_SHOCK'] = np.where(df_test_results_indiv.HOURS_BETWEEN_SEPSIS_SHOCK == 31, 'Nonshock','Shock')




print(df_test_results_indiv.shape)
print(df_test_results_indiv.sort_values("BALANCED_ACCURACY"))


df_test_results_indiv.to_csv('best_xgboost_3hSEQ_OVER0.018_UNDER1.csv')