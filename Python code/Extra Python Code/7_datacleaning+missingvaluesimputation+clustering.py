# -*- coding: utf-8 -*-
"""7_DataCleaning+MissingValuesImputation+clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LfHNIWQcRsNnJ0ZYREnqFjH_w_x5dJKx
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
import pandas as pd
import io
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
import os
import shutil
import posixpath
import urllib.request
import datetime
from collections import namedtuple

drive.mount('/content/gdrive')

# Download All Time series data for 404 patients.
df_all404_withTemp = pd.read_csv('/content/gdrive/My Drive/Master thesis/df_ts_records_all404_patients_withTEMP.csv')
df_all404_withTemp['TIME'] =  pd.to_datetime(df_all404_withTemp['TIME'])
print(df_all404_withTemp.shape)

# Test code for cleaning data i.e. replace -ver values with NaN
# and Missing values imoputation by last observation carry forward scheme.
"""
df_mv_test.drop(df_mv_test.index ,inplace=True)
df_mv_test = pd.DataFrame()
df_mv_test['Sales'] = [np.nan,np.nan,6,-1,2,np.nan,np.nan,-5,8, np.nan]
df_mv_test['Accounts'] = [np.nan,np.nan,6,-1,2.0,np.nan,np.nan,-5,8, np.nan]
print(df_mv_test)
#df_mv_test = df_mv_test.where(df_mv_test > 0, np.nan) # one of the ways to replace -ve values with NaN
print(df_mv_test.mask(df_mv_test < 0)) 
#df.mask(df < 0, 0)  # to replace negative values with 0
print(df_mv_test.mask(df_mv_test < 0).ffill().bfill())
"""

#cleaning data :  removing all negative values

df_all404_withTemp.HR = df_all404_withTemp.HR.mask(df_all404_withTemp.HR < 0)

df_all404_withTemp.RESP = df_all404_withTemp.RESP.mask(df_all404_withTemp.RESP < 0)

df_all404_withTemp.ABPSYS = df_all404_withTemp.ABPSYS.mask(df_all404_withTemp.ABPSYS < 0)

df_all404_withTemp.ABPDIAS = df_all404_withTemp.ABPDIAS.mask(df_all404_withTemp.ABPDIAS < 0)

df_all404_withTemp.ABPMEAN = df_all404_withTemp.ABPMEAN.mask(df_all404_withTemp.ABPMEAN < 0)

df_all404_withTemp.SPO2 = df_all404_withTemp.SPO2.mask(df_all404_withTemp.SPO2 < 0)

df_all404_withTemp.TEMP = df_all404_withTemp.TEMP.mask(df_all404_withTemp.TEMP < 0)

# Missing value imputation by carry forward scheme
df_all404_withTemp_cleaned_MVimputed = df_all404_withTemp.ffill().bfill()

df_all404_withTemp_cleaned_MVimputed.HR = df_all404_withTemp_cleaned_MVimputed.HR.round(decimals=4)
df_all404_withTemp_cleaned_MVimputed.RESP = df_all404_withTemp_cleaned_MVimputed.RESP.round(decimals=4)
df_all404_withTemp_cleaned_MVimputed.ABPSYS = df_all404_withTemp_cleaned_MVimputed.ABPSYS.round(decimals=4)
df_all404_withTemp_cleaned_MVimputed.ABPDIAS = df_all404_withTemp_cleaned_MVimputed.ABPDIAS.round(decimals=4)
df_all404_withTemp_cleaned_MVimputed.ABPMEAN = df_all404_withTemp_cleaned_MVimputed.ABPMEAN.round(decimals=4)
df_all404_withTemp_cleaned_MVimputed.SPO2 = df_all404_withTemp_cleaned_MVimputed.SPO2.round(decimals=4)
df_all404_withTemp_cleaned_MVimputed.TEMP = df_all404_withTemp_cleaned_MVimputed.TEMP.round(decimals=4)

df_all404_withTemp_cleaned_MVimputed['TIME'] =  pd.to_datetime(df_all404_withTemp_cleaned_MVimputed['TIME'])

# Check if any null values exists in the final dataframe after cleaning and imputing missing values.
print(df_all404_withTemp_cleaned_MVimputed[df_all404_withTemp_cleaned_MVimputed.isnull().any(axis=1)])

from google.colab import files
uploaded = files.upload()

df_icutime = pd.read_csv(io.BytesIO(uploaded['df_TS_exists_withoutTEMP_overlapcount.csv']))
print (df_icutime.columns)
df_icutime = df_icutime[['subject_id','icustay_id','intime','outtime','sepsis_onsettime']]
print (df_icutime.columns)
"""
df_icutime['intime'] =  pd.to_datetime(df_icutime['intime'])
df_icutime['outtime'] =  pd.to_datetime(df_icutime['outtime'])
"""

# test
pd.set_option('display.max_rows', 5000)
pd.set_option('display.max_columns', 50)
pd.set_option('display.width', 1000)
pd.set_option('max_colwidth', 800)
df_saf_test = df_all404_withTemp_cleaned_MVimputed[df_all404_withTemp_cleaned_MVimputed['SUBJECT_ID'] == 52264]
df_saf_test= df_saf_test.loc[ 2521656:2521667 ,['SUBJECT_ID','TIME','HR','RESP','ABPSYS','ABPMEAN','ABPDIAS','SPO2','TEMP'] ] 
"""
df_saf_test.HR = df_saf_test.HR.round(decimals=4)
df_saf_test.RESP = df_saf_test.RESP.round(decimals=4)
df_saf_test.ABPSYS = df_saf_test.ABPSYS.round(decimals=4)
df_saf_test.ABPDIAS = df_saf_test.ABPDIAS.round(decimals=4)
df_saf_test.ABPMEAN = df_saf_test.ABPMEAN.round(decimals=4)
df_saf_test.SPO2 = df_saf_test.SPO2.round(decimals=4)
df_saf_test.TEMP = df_saf_test.TEMP.round(decimals=4)
"""
"""
print(df_saf_test)
print(df_saf_test.groupby('SUBJECT_ID').mean()) #[np.size, np.mean]
df = df_all404_withTemp_cleaned_MVimputed.groupby('SUBJECT_ID').agg( {'HR': [ np.mean,  np.min, np.std, np.var,  np.max], 'RESP':np.mean }  )
print(df_all404_withTemp_cleaned_MVimputed.groupby('SUBJECT_ID').agg( {'HR': [ np.mean,  np.min, np.std, np.var,  np.max], 'RESP':np.mean }  ))
print(df['SUBJECT_ID'])
"""

#print( df_saf_test.describe().round(decimals = 4).to_numpy() )
#print('printing individual')
#print( df_saf_test['HR'].mean()) 
"""
feature_single_patient=[]
feature_single_patient.append(df_saf_test['HR'].min())
feature_single_patient.append(df_saf_test['HR'].max() )
feature_single_patient.append(df_saf_test['HR'].mean())
feature_single_patient.append(df_saf_test['HR'].std() )
feature_single_patient.append(df_saf_test['HR'].var() )
"""
"""
feature_single_patient=''
feature_single_patient = str(df_saf_test['HR'].min()) + ' ' + str(df_saf_test['HR'].max()) + ' ' + str(df_saf_test['HR'].mean()) + ' ' +  str(df_saf_test['HR'].std()) + ' ' +  str(df_saf_test['HR'].var() )



print(feature_single_patient)

new_df= pd.DataFrame(columns=['SUBJECT_ID','FEATURE_VECTOR'])
new_df['FEATURE_VECTOR']= new_df['FEATURE_VECTOR'].astype('float')
new_df.loc[0,'SUBJECT_ID'] = 9
print(new_df.dtypes)


new_df.at[0,'FEATURE_VECTOR'] = feature_single_patient
print(new_df)
from sklearn.preprocessing import StandardScaler
print(new_df['FEATURE_VECTOR'])
x = StandardScaler().fit_transform(new_df['FEATURE_VECTOR'])
print(x)
"""

new_df= pd.DataFrame(columns=['SUBJECT_ID','HR_MIN','HR_MAX','HR_MEAN'])
new_df.loc[0,'SUBJECT_ID'] = 9
new_df.loc[0,'HR_MIN'] = 121.4492
new_df.loc[0,'HR_MAX'] = 124
new_df.loc[0,'HR_MEAN'] = 118

new_df.loc[1,'SUBJECT_ID'] = 10
new_df.loc[1,'HR_MIN'] = 122
new_df.loc[1,'HR_MAX'] = 125
new_df.loc[1,'HR_MEAN'] = 112

print(new_df)
print(new_df.columns)

from sklearn.preprocessing import StandardScaler, MinMaxScaler
cols= ['HR_MIN','HR_MAX','HR_MEAN'];
data = new_df[cols]
print(data.values)
x = MinMaxScaler().fit_transform(data)
print(x)

subject_ids = df_all404_withTemp_cleaned_MVimputed.SUBJECT_ID.unique()
print((subject_ids))

"""SIMPLE KMEANS CLUSTERING"""

feature_cols= ['SUBJECT_ID','SEPSIS_ONSETTIME',
               'HR_MIN','HR_MAX','HR_MEAN','HR_STD','HR_VAR',
               'RESP_MIN','RESP_MAX','RESP_MEAN','RESP_STD','RESP_VAR',
               'ABPSYS_MIN','ABPSYS_MAX','ABPSYS_MEAN','ABPSYS_STD','ABPSYS_VAR',
               'ABPDIAS_MIN','ABPDIAS_MAX','ABPDIAS_MEAN','ABPDIAS_STD','ABPDIAS_VAR',
               'ABPMEAN_MIN','ABPMEAN_MAX','ABPMEAN_MEAN','ABPMEAN_STD','ABPMEAN_VAR',
               'SPO2_MIN','SPO2_MAX','SPO2_MEAN','SPO2_STD','SPO2_VAR',
               'TEMP_MIN','TEMP_MAX','TEMP_MEAN','TEMP_STD','TEMP_VAR' ] 

df_features =pd.DataFrame(columns=feature_cols);

for subject_id in subject_ids:
  curr_idx = df_features.shape[0]
  #df_icuintime_subjectid = df_icutime[df_icutime['subject_id'] == subject_id] 

  icu_intime = df_icutime.loc[df_icutime.subject_id==subject_id , 'intime'].values[0]
  sepsis_onsettime = df_icutime.loc[df_icutime.subject_id==subject_id , 'sepsis_onsettime'].values[0]
  print(icu_intime)

  df_tsdata_subjectid = df_all404_withTemp_cleaned_MVimputed[(df_all404_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( df_all404_withTemp_cleaned_MVimputed['TIME'] <= (datetime.datetime.strptime(icu_intime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=24) )  )]
  
  data_test = df_all404_withTemp_cleaned_MVimputed[(df_all404_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) ]
  #print(data_test[['TIME','HR','RESP','ABPSYS','TEMP']])
  df_features.loc[curr_idx,'SUBJECT_ID'] = subject_id
  df_features.loc[curr_idx,'SEPSIS_ONSETTIME'] = sepsis_onsettime
  df_features.loc[curr_idx,'HR_MIN'] = df_tsdata_subjectid['HR'].min()
  df_features.loc[curr_idx,'HR_MAX'] = df_tsdata_subjectid['HR'].max()
  df_features.loc[curr_idx,'HR_MEAN'] = df_tsdata_subjectid['HR'].mean()
  df_features.loc[curr_idx,'HR_STD'] = df_tsdata_subjectid['HR'].std()
  df_features.loc[curr_idx,'HR_VAR'] = df_tsdata_subjectid['HR'].var()

  df_features.loc[curr_idx,'RESP_MIN'] = df_tsdata_subjectid['RESP'].min()
  df_features.loc[curr_idx,'RESP_MAX']= df_tsdata_subjectid['RESP'].max()
  df_features.loc[curr_idx,'RESP_MEAN'] = df_tsdata_subjectid['RESP'].mean()
  df_features.loc[curr_idx,'RESP_STD'] = df_tsdata_subjectid['RESP'].std()
  df_features.loc[curr_idx,'RESP_VAR'] = df_tsdata_subjectid['RESP'].var()

  df_features.loc[curr_idx,'ABPSYS_MIN'] = df_tsdata_subjectid['ABPSYS'].min()
  df_features.loc[curr_idx,'ABPSYS_MAX'] = df_tsdata_subjectid['ABPSYS'].max()
  df_features.loc[curr_idx,'ABPSYS_MEAN'] = df_tsdata_subjectid['ABPSYS'].mean()
  df_features.loc[curr_idx,'ABPSYS_STD'] = df_tsdata_subjectid['ABPSYS'].std()
  df_features.loc[curr_idx,'ABPSYS_VAR']  = df_tsdata_subjectid['ABPSYS'].var()

  df_features.loc[curr_idx,'ABPDIAS_MIN']  = df_tsdata_subjectid['ABPDIAS'].min()
  df_features.loc[curr_idx,'ABPDIAS_MAX'] = df_tsdata_subjectid['ABPDIAS'].max()
  df_features.loc[curr_idx,'ABPDIAS_MEAN'] = df_tsdata_subjectid['ABPDIAS'].mean()
  df_features.loc[curr_idx,'ABPDIAS_STD'] = df_tsdata_subjectid['ABPDIAS'].std()
  df_features.loc[curr_idx,'ABPDIAS_VAR'] = df_tsdata_subjectid['ABPDIAS'].var()

  df_features.loc[curr_idx,'ABPMEAN_MIN']= df_tsdata_subjectid['ABPMEAN'].min()
  df_features.loc[curr_idx,'ABPMEAN_MAX']= df_tsdata_subjectid['ABPMEAN'].max()
  df_features.loc[curr_idx,'ABPMEAN_MEAN']= df_tsdata_subjectid['ABPMEAN'].mean()
  df_features.loc[curr_idx,'ABPMEAN_STD']= df_tsdata_subjectid['ABPMEAN'].std()
  df_features.loc[curr_idx,'ABPMEAN_VAR']= df_tsdata_subjectid['ABPMEAN'].var()

  df_features.loc[curr_idx,'SPO2_MIN']= df_tsdata_subjectid['SPO2'].min()
  df_features.loc[curr_idx,'SPO2_MAX']= df_tsdata_subjectid['SPO2'].max()
  df_features.loc[curr_idx,'SPO2_MEAN']= df_tsdata_subjectid['SPO2'].mean()
  df_features.loc[curr_idx,'SPO2_STD']= df_tsdata_subjectid['SPO2'].std()
  df_features.loc[curr_idx,'SPO2_VAR']= df_tsdata_subjectid['SPO2'].var()

  df_features.loc[curr_idx,'TEMP_MIN'] = df_tsdata_subjectid['TEMP'].min()
  df_features.loc[curr_idx,'TEMP_MAX']= df_tsdata_subjectid['TEMP'].max()
  df_features.loc[curr_idx,'TEMP_MEAN']= df_tsdata_subjectid['TEMP'].mean()
  df_features.loc[curr_idx,'TEMP_STD']= df_tsdata_subjectid['TEMP'].std()
  df_features.loc[curr_idx,'TEMP_VAR']= df_tsdata_subjectid['TEMP'].var()

  icu_intime='';
  sepsis_onsettime='';
  df_tsdata_subjectid.drop(df_tsdata_subjectid.index, inplace=True) 
  #print(df_features)

print(df_features)

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA

cols_of_interest = ['HR_MIN','HR_MAX','HR_MEAN','HR_STD','HR_VAR',
               'RESP_MIN','RESP_MAX','RESP_MEAN','RESP_STD','RESP_VAR',
               'ABPSYS_MIN','ABPSYS_MAX','ABPSYS_MEAN','ABPSYS_STD','ABPSYS_VAR',
               'ABPDIAS_MIN','ABPDIAS_MAX','ABPDIAS_MEAN','ABPDIAS_STD','ABPDIAS_VAR',
               'ABPMEAN_MIN','ABPMEAN_MAX','ABPMEAN_MEAN','ABPMEAN_STD','ABPMEAN_VAR',
               'SPO2_MIN','SPO2_MAX','SPO2_MEAN','SPO2_STD','SPO2_VAR',
               'TEMP_MIN','TEMP_MAX','TEMP_MEAN','TEMP_STD','TEMP_VAR' ];

data = df_features[cols_of_interest]
scaled_data = MinMaxScaler().fit_transform(data)
print(scaled_data)

pca = PCA(n_components=2)
principalComponents = pca.fit_transform(scaled_data)

# Plot the explained variances
features_pca = range(pca.n_components_)
plt.bar(features_pca, pca.explained_variance_ratio_, color='black')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features_pca)
# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)


plt.scatter(PCA_components[0], PCA_components[1], alpha=.1, color='black')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')

from sklearn.cluster import KMeans
sum_of_squared_distances = []
K = range(2,15)
for k in K:
    k_means = KMeans(n_clusters=k)
    model = k_means.fit(scaled_data)
    sum_of_squared_distances.append(k_means.inertia_)
    print(metrics.silhouette_score(scaled_data, k_means.labels_, metric = 'euclidean'))


plt.plot(K, sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('sum_of_squared_distances')
plt.title('elbow method for optimal k')
plt.show()

import seaborn as sns
import sklearn
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn import metrics
from sklearn.cluster import AgglomerativeClustering


"""
k_means_6 = KMeans(n_clusters=6)
#Run the clustering algorithm
model = k_means_6.fit(scaled_data)
#Generate cluster predictions and store in y_hat
y_hat = k_means_6.predict(scaled_data)
labels_6 = k_means_6.labels_

df_features['CLUSTER'] =  pd.DataFrame(k_means_6.labels_)
print(len(k_means_6.labels_))
print(df_features)

#####

k_means_7 = KMeans(n_clusters=7)
#Run the clustering algorithm
model = k_means_7.fit(scaled_data)
#Generate cluster predictions and store in y_hat
y_hat = k_means_7.predict(scaled_data)
labels_7 = k_means_7.labels_


#####

"""
k_means_8 = KMeans(n_clusters=8)
#Run the clustering algorithm
model = k_means_8.fit(scaled_data)
#Generate cluster predictions and store in y_hat
y_hat = k_means_8.predict(scaled_data)
labels_8 = k_means_8.labels_
df_features['CLUSTER'] = labels_8
print(PCA_components)
result = pd.concat([df_features, PCA_components], axis=1, sort=False)
print(result)


"""
print('Printing metircs for 6 clusters')
print( metrics.silhouette_score(scaled_data, labels_6, metric = 'euclidean'))
print(metrics.calinski_harabasz_score(scaled_data, labels_6) ) 

print('Printing metircs for 7 clusters')
print(metrics.silhouette_score(scaled_data, lables_7, metric = 'euclidean'))
print( metrics.calinski_harabasz_score(scaled_data, labels_7))
"""
print('Printing metircs for 8 clusters')
print(metrics.silhouette_score(scaled_data, labels_8, metric = 'euclidean'))
print(metrics.calinski_harabasz_score(scaled_data, labels_8))

centroids = k_means_8.cluster_centers_

# Plot the clustered data
fig, ax = plt.subplots(figsize=(6, 6))
"""
plt.scatter(scaled_data[k_means_8.labels_ == 0, 0], scaled_data[k_means_8.labels_ == 0, 1],
            c='green', label='cluster 1')
plt.scatter(scaled_data[k_means_8.labels_ == 1, 0], scaled_data[k_means_8.labels_ == 1, 1],
            c='blue', label='cluster 2')

plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=300,
            c='r', label='centroid')

plt.legend()
plt.xlim([-2, 2])
plt.ylim([-2, 2])
plt.xlabel('Eruption time in mins')
plt.ylabel('Waiting time to next eruption')
plt.title('Visualization of clustered data', fontweight='bold')
ax.set_aspect('equal');
"""
print(result.columns)

print(result.iloc[:,38].values)
plt.scatter(result.iloc[:,38].values,result.iloc[:,39].values, c=k_means_8.labels_, cmap='rainbow')

plt.legend()
plt.xlim([-2, 2])
plt.ylim([-2, 2])
plt.xlabel('Component 1')
plt.ylabel('Component 2')
plt.title('Visualization of clustered data', fontweight='bold')
ax.set_aspect('equal');

print(df_features[df_features['SEPSIS_ONSETTIME'].notna()].groupby('CLUSTER').count()) #septic patients
print(df_features[df_features['SEPSIS_ONSETTIME'].isna()].groupby('CLUSTER').count()) # non- septic patients

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

clustering_result_cols = ['SEPSIS_ONSETTIME','HR_MIN','HR_MAX','HR_MEAN','HR_STD','HR_VAR',
               'RESP_MIN','RESP_MAX','RESP_MEAN','RESP_STD','RESP_VAR',
               'ABPSYS_MIN','ABPSYS_MAX','ABPSYS_MEAN','ABPSYS_STD','ABPSYS_VAR',
               'ABPDIAS_MIN','ABPDIAS_MAX','ABPDIAS_MEAN','ABPDIAS_STD','ABPDIAS_VAR',
               'ABPMEAN_MIN','ABPMEAN_MAX','ABPMEAN_MEAN','ABPMEAN_STD','ABPMEAN_VAR',
               'SPO2_MIN','SPO2_MAX','SPO2_MEAN','SPO2_STD','SPO2_VAR',
               'TEMP_MIN','TEMP_MAX','TEMP_MEAN','TEMP_STD','TEMP_VAR' ];

df_clutering_Result = df_features[df_features['CLUSTER']==1 ]

print(df_clutering_Result[clustering_result_cols]) #septic patients

"""TRYING MINI SOM : USED FOR EQUAL LENGTH TS

"""

!pip install MiniSom
from minisom import MiniSom

som_shape = (1, 4)
som = MiniSom(som_shape[0], som_shape[1], 35, sigma=.5, learning_rate=.5,
              neighborhood_function='gaussian', random_seed=10)

som.train_batch(scaled_data, 500, verbose=True)

# each neuron represents a cluster
winner_coordinates = np.array([som.winner(x) for x in scaled_data]).T
# with np.ravel_multi_index we convert the bidimensional
# coordinates to a monodimensional index
cluster_index = np.ravel_multi_index(winner_coordinates, som_shape)
print(winner_coordinates)
print(len(cluster_index))
df_features['CLUSTER'] = cluster_index

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

# plotting the clusters using the first 2 dimentions of the data
for c in np.unique(cluster_index):
    plt.scatter(scaled_data[cluster_index == c, 0],
                scaled_data[cluster_index == c,1], label='cluster='+str(c), alpha=.7)

# plotting centroids
for centroid in som.get_weights():
    plt.scatter(centroid[:, 0], centroid[:, 1], marker='x', 
                s=80, linewidths=35, color='k', label='centroid')
plt.legend();

# Commented out IPython magic to ensure Python compatibility.
# TRYING MINI SOM ON A SINGLE PATIENT Data
df_single_patient =  df_all404_withTemp_cleaned_MVimputed[df_all404_withTemp_cleaned_MVimputed['SUBJECT_ID'] == 98930 ]
cols_interest_single_patient = ['HR','SPO2','ABPSYS','ABPDIAS','ABPMEAN','RESP','TEMP']
print(df_single_patient[cols_interest_single_patient])

data_single_patient = df_single_patient[cols_interest_single_patient]
scaled_data_single= MinMaxScaler().fit_transform(data_single_patient)
print(scaled_data_single)


sum_of_squared_distances = []
K = range(1,15)
for k in K:
    k_means = KMeans(n_clusters=k)
    model = k_means.fit(scaled_data_single)
    sum_of_squared_distances.append(k_means.inertia_)

plt.plot(K, sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('sum_of_squared_distances')
plt.title('elbow method for optimal k')
plt.show()

som_shape = (1, 4)

som = MiniSom(som_shape[0], som_shape[1], 7, sigma=.5, learning_rate=.5,
              neighborhood_function='gaussian', random_seed=10)

som.train_batch(scaled_data_single, 500, verbose=True)
# each neuron represents a cluster
winner_coordinates = np.array([som.winner(x) for x in scaled_data_single]).T
# with np.ravel_multi_index we convert the bidimensional
# coordinates to a monodimensional index
cluster_index = np.ravel_multi_index(winner_coordinates, som_shape)
print(winner_coordinates)
print(len(cluster_index))
df_single_patient['CLUSTER'] = cluster_index

import matplotlib.pyplot as plt
# %matplotlib inline

# plotting the clusters using the first 2 dimentions of the data
for c in np.unique(cluster_index):
    plt.scatter(scaled_data_single[cluster_index == c, 0],
                scaled_data_single[cluster_index == c,1], label='cluster='+str(c), alpha=.7)

# plotting centroids
for centroid in som.get_weights():
    plt.scatter(centroid[:, 0], centroid[:, 1], marker='x', 
                s=80, linewidths=35, color='k', label='centroid')
plt.legend();


print(df_single_patient[df_single_patient['CLUSTER'] ==])

pd.set_option('display.max_rows', 4000)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
print(df_single_patient[df_single_patient['CLUSTER'] == 3]) # 2201-04-02 20:05:06 to 2201-04-05 11:53:06   
# from  2201-04-02 15:41:06 to 2201-04-05 10:53:06, on set : 2201-04-03 06:41:56

"""TRYING TSLEARN KMEANS WITH DTW DISTANCE METRIC FOR CLUSTERING on RAW DATA

WITHOUT FEATURE EXTRACTION
"""

#trying TSLEARN PACKAGE , this package can be used for multivariate and different length time series.

!pip install tslearn
from tslearn.utils import to_time_series_dataset
from tslearn.preprocessing import TimeSeriesScalerMinMax
from tslearn.clustering import TimeSeriesKMeans
from sklearn.preprocessing import MinMaxScaler
#print(df_all404_withTemp_cleaned_MVimputed.columns)
data_tslearn = df_all404_withTemp_cleaned_MVimputed[['SUBJECT_ID','HR', 'SPO2', 'ABPSYS','ABPDIAS', 'ABPMEAN', 'RESP', 'TEMP']]

data_tslearn.SUBJECT_ID.unique()
subject_ids = [98930, 59864, 40227]

data_slice = data_tslearn[data_tslearn.SUBJECT_ID.isin(subject_ids)]
print('before scaling', data_slice)
scaler = MinMaxScaler()
data_slice['HR'] = scaler.fit_transform(data_slice['HR'].values.reshape(-1,1))
data_slice['SPO2'] = scaler.fit_transform(data_slice['SPO2'].values.reshape(-1,1))
data_slice['ABPSYS'] = scaler.fit_transform(data_slice['ABPSYS'].values.reshape(-1,1))
data_slice['ABPDIAS'] = scaler.fit_transform(data_slice['ABPDIAS'].values.reshape(-1,1))
data_slice['ABPMEAN'] = scaler.fit_transform(data_slice['ABPMEAN'].values.reshape(-1,1))
data_slice['RESP'] = scaler.fit_transform(data_slice['RESP'].values.reshape(-1,1))
data_slice['TEMP'] = scaler.fit_transform(data_slice['TEMP'].values.reshape(-1,1))


print('after scaling')
print( data_slice)


arr = data_slice.set_index('SUBJECT_ID').groupby('SUBJECT_ID').apply(pd.DataFrame.to_numpy).to_numpy()
print('AFTER GROUPING')
print(arr)

arr= to_time_series_dataset(arr)

print('AFTER CONVERTING TO TIME SERIES DATASET', arr.shape)

km = TimeSeriesKMeans(n_clusters=2, metric="dtw", max_iter=3,
                       random_state=0).fit(arr)

km.cluster_centers_.shape

"""TRYING TSLEARN KMEANS WITH DTW DISTANCE METRIC FOR CLUSTERING 

WITH FEATURE EXTRACTION USING SLIDING WINDOW OF 1 HOUR AND OVERLAPING OF 10 MINS
"""

#!pip install window_slider
#!pip install tsfresh
from tsfresh import feature_extraction
from window_slider import Slider
import numpy
#df_features.drop(df_features.index,inplace=True) 

#df_clustering_result.drop(df_clustering_result.index,inplace=True) 
feature_cols= ['SUBJECT_ID','SEPSIS_ONSETTIME',
               
               'HR_MIN','HR_MAX','HR_MEAN','HR_STD','HR_VAR','HR_AC','HR_SKEW','HR_KURT',

               'RESP_MIN','RESP_MAX','RESP_MEAN','RESP_STD','RESP_VAR','RESP_AC','RESP_SKEW','RESP_KURT',

               'ABPSYS_MIN','ABPSYS_MAX','ABPSYS_MEAN','ABPSYS_STD','ABPSYS_VAR','ABPSYS_AC','ABPSYS_SKEW','ABPSYS_KURT',

               'ABPDIAS_MIN','ABPDIAS_MAX','ABPDIAS_MEAN','ABPDIAS_STD','ABPDIAS_VAR','ABPDIAS_AC','ABPDIAS_SKEW','ABPDIAS_KURT',

               'ABPMEAN_MIN','ABPMEAN_MAX','ABPMEAN_MEAN','ABPMEAN_STD','ABPMEAN_VAR','ABPMEAN_AC','ABPMEAN_SKEW','ABPMEAN_KURT',

               'SPO2_MIN','SPO2_MAX','SPO2_MEAN','SPO2_STD','SPO2_VAR','SPO2_AC','SPO2_SKEW','SPO2_KURT',

               'TEMP_MIN','TEMP_MAX','TEMP_MEAN','TEMP_STD','TEMP_VAR' ,'TEMP_AC','TEMP_SKEW','TEMP_KURT']

df_features =pd.DataFrame(columns=feature_cols);
df_clustering_result = pd.DataFrame(columns=['SUBJECT_ID','SEPSIS_ONSETTIME','ICUSTAY_ID']);
#subject_ids = [98930]
for subject_id in subject_ids:
  curr_idx_clustering_res= df_clustering_result.shape[0]
  df_clustering_result.loc[curr_idx_clustering_res,'SUBJECT_ID'] = subject_id
  df_clustering_result.loc[curr_idx_clustering_res,'ICUSTAY_ID']  = df_icutime.loc[df_icutime.subject_id==subject_id , 'icustay_id'].values[0]

  df_clustering_result.loc[curr_idx_clustering_res,'SEPSIS_ONSETTIME']  = df_icutime.loc[df_icutime.subject_id==subject_id , 'sepsis_onsettime'].values[0]
  try:
    df_features_hr.drop(df_features_hr.index,inplace=True)
    df_features_resp.drop(df_features_resp.index,inplace=True)
    df_features_abpsys.drop(df_features_abpsys.index,inplace=True)
    df_features_abpdias.drop(df_features_abpdias.index,inplace=True)
    df_features_abpmean.drop(df_features_abpmean.index,inplace=True)
    df_features_spo2.drop(df_features_spo2.index,inplace=True)
    df_features_temp.drop(df_features_temp.index,inplace=True)
  except:
    print('initial dfs do not exists')

  df_features_hr = pd.DataFrame(columns=['SUBJECT_ID','SEPSIS_ONSETTIME','HR_MIN', 'HR_MAX','HR_MEAN','HR_STD','HR_VAR','HR_AC','HR_SKEW','HR_KURT'])
  df_features_resp = pd.DataFrame(columns=['RESP_MIN','RESP_MAX','RESP_MEAN','RESP_STD','RESP_VAR','RESP_AC','RESP_SKEW','RESP_KURT'])
  df_features_abpsys = pd.DataFrame(columns=['ABPSYS_MIN','ABPSYS_MAX','ABPSYS_MEAN','ABPSYS_STD','ABPSYS_VAR','ABPSYS_AC','ABPSYS_SKEW','ABPSYS_KURT'])
  df_features_abpdias = pd.DataFrame(columns=['ABPDIAS_MIN','ABPDIAS_MAX','ABPDIAS_MEAN','ABPDIAS_STD','ABPDIAS_VAR','ABPDIAS_AC','ABPDIAS_SKEW','ABPDIAS_KURT']) 
  df_features_abpmean = pd.DataFrame(columns=['ABPMEAN_MIN','ABPMEAN_MAX','ABPMEAN_MEAN','ABPMEAN_STD','ABPMEAN_VAR','ABPMEAN_AC','ABPMEAN_SKEW','ABPMEAN_KURT'])
  df_features_spo2 = pd.DataFrame(columns=['SPO2_MIN','SPO2_MAX','SPO2_MEAN','SPO2_STD','SPO2_VAR','SPO2_AC','SPO2_SKEW','SPO2_KURT'])
  df_features_temp = pd.DataFrame(columns=['TEMP_MIN','TEMP_MAX','TEMP_MEAN','TEMP_STD','TEMP_VAR' ,'TEMP_AC','TEMP_SKEW','TEMP_KURT'])


  icu_intime = df_icutime.loc[df_icutime.subject_id==subject_id , 'intime'].values[0]
  sepsis_onsettime = df_icutime.loc[df_icutime.subject_id==subject_id , 'sepsis_onsettime'].values[0]

  df_tsdata_subjectid = df_all404_withTemp_cleaned_MVimputed[(df_all404_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( df_all404_withTemp_cleaned_MVimputed['TIME'] <= (datetime.datetime.strptime(icu_intime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=24) )  )]
  
  #hear rate data
  hr_data = df_tsdata_subjectid.HR
  bucket_size = 60
  overlap_count = 10
  slider = Slider(bucket_size,overlap_count)
  slider.fit(hr_data)       
 
  while True:
      window_data = slider.slide()
      # do your stuff
      #print(window_data)
      pac = window_data.to_list()
      curr_idx = df_features_hr.shape[0]
      df_features_hr.loc[curr_idx,'SUBJECT_ID'] = subject_id
      df_features_hr.loc[curr_idx,'SEPSIS_ONSETTIME'] = sepsis_onsettime
      df_features_hr.loc[curr_idx,'HR_MIN'] = feature_extraction.feature_calculators.minimum(window_data)
      df_features_hr.loc[curr_idx,'HR_MAX'] = feature_extraction.feature_calculators.maximum(window_data)
      df_features_hr.loc[curr_idx,'HR_MEAN'] = feature_extraction.feature_calculators.mean(window_data)
      df_features_hr.loc[curr_idx,'HR_STD'] = feature_extraction.feature_calculators.standard_deviation(window_data)
      df_features_hr.loc[curr_idx,'HR_VAR'] = feature_extraction.feature_calculators.variance(window_data)
      df_features_hr.loc[curr_idx,'HR_AC'] = feature_extraction.feature_calculators.autocorrelation(window_data,1)
      df_features_hr.loc[curr_idx,'HR_SKEW'] = feature_extraction.feature_calculators.skewness(window_data)
      df_features_hr.loc[curr_idx,'HR_KURT'] = feature_extraction.feature_calculators.kurtosis(window_data)
      #df_features_hr.loc[curr_idx,'HR_PAC'] = feature_extraction.feature_calculators.partial_autocorrelation(window_data,{"lag":1} )
      window_data = ''
      
      if slider.reached_end_of_list(): break
  
  #resp data
  resp_data = df_tsdata_subjectid.RESP
  bucket_size = 60
  overlap_count = 10
  slider = Slider(bucket_size,overlap_count)
  slider.fit(resp_data)       


  while True:
      window_data = slider.slide()
      # do your stuff
      #print(window_data)
      
      curr_idx = df_features_resp.shape[0]
      df_features_resp.loc[curr_idx,'RESP_MIN'] = feature_extraction.feature_calculators.minimum(window_data)
      df_features_resp.loc[curr_idx,'RESP_MAX'] = feature_extraction.feature_calculators.maximum(window_data)
      df_features_resp.loc[curr_idx,'RESP_MEAN'] = feature_extraction.feature_calculators.mean(window_data)
      df_features_resp.loc[curr_idx,'RESP_STD'] = feature_extraction.feature_calculators.standard_deviation(window_data)
      df_features_resp.loc[curr_idx,'RESP_VAR'] = feature_extraction.feature_calculators.variance(window_data)
      df_features_resp.loc[curr_idx,'RESP_AC'] = feature_extraction.feature_calculators.autocorrelation(window_data,1)
      df_features_resp.loc[curr_idx,'RESP_SKEW'] = feature_extraction.feature_calculators.skewness(window_data)
      df_features_resp.loc[curr_idx,'RESP_KURT'] = feature_extraction.feature_calculators.kurtosis(window_data)
      #df_features_resp.loc[curr_idx,'RESP_PAC'] = feature_extraction.feature_calculators.partial_autocorrelation(window_data,1)
      window_data = ''
      if slider.reached_end_of_list(): break

### abpsys
  abpsys_data = df_tsdata_subjectid.ABPSYS
  bucket_size = 60
  overlap_count = 10
  slider = Slider(bucket_size,overlap_count)
  slider.fit(abpsys_data)       


  while True:
      window_data = slider.slide()
      # do your stuff
      #print(window_data)
      
      curr_idx = df_features_abpsys.shape[0]
      df_features_abpsys.loc[curr_idx,'ABPSYS_MIN'] = feature_extraction.feature_calculators.minimum(window_data)
      df_features_abpsys.loc[curr_idx,'ABPSYS_MAX'] = feature_extraction.feature_calculators.maximum(window_data)
      df_features_abpsys.loc[curr_idx,'ABPSYS_MEAN'] = feature_extraction.feature_calculators.mean(window_data)
      df_features_abpsys.loc[curr_idx,'ABPSYS_STD'] = feature_extraction.feature_calculators.standard_deviation(window_data)
      df_features_abpsys.loc[curr_idx,'ABPSYS_VAR'] = feature_extraction.feature_calculators.variance(window_data)
      df_features_abpsys.loc[curr_idx,'ABPSYS_AC'] = feature_extraction.feature_calculators.autocorrelation(window_data,1)
      df_features_abpsys.loc[curr_idx,'ABPSYS_SKEW'] = feature_extraction.feature_calculators.skewness(window_data)
      df_features_abpsys.loc[curr_idx,'ABPSYS_KURT'] = feature_extraction.feature_calculators.kurtosis(window_data)
      #df_features_abpsys.loc[curr_idx,'ABPSYS_PAC'] = feature_extraction.feature_calculators.partial_autocorrelation(window_data,1)
      window_data = ''
      if slider.reached_end_of_list(): break

# ABP DIAS DATA
  abpdias_data = df_tsdata_subjectid.ABPDIAS
  bucket_size = 60
  overlap_count = 10
  slider = Slider(bucket_size,overlap_count)
  slider.fit(abpdias_data)       


  while True:
      window_data = slider.slide()
      # do your stuff
      #print(window_data)
      
      curr_idx = df_features_abpdias.shape[0]
      df_features_abpdias.loc[curr_idx,'ABPDIAS_MIN'] = feature_extraction.feature_calculators.minimum(window_data)
      df_features_abpdias.loc[curr_idx,'ABPDIAS_MAX'] = feature_extraction.feature_calculators.maximum(window_data)
      df_features_abpdias.loc[curr_idx,'ABPDIAS_MEAN'] = feature_extraction.feature_calculators.mean(window_data)
      df_features_abpdias.loc[curr_idx,'ABPDIAS_STD'] = feature_extraction.feature_calculators.standard_deviation(window_data)
      df_features_abpdias.loc[curr_idx,'ABPDIAS_VAR'] = feature_extraction.feature_calculators.variance(window_data)
      df_features_abpdias.loc[curr_idx,'ABPDIAS_AC'] = feature_extraction.feature_calculators.autocorrelation(window_data,1)
      df_features_abpdias.loc[curr_idx,'ABPDIAS_SKEW'] = feature_extraction.feature_calculators.skewness(window_data)
      df_features_abpdias.loc[curr_idx,'ABPDIAS_KURT'] = feature_extraction.feature_calculators.kurtosis(window_data)
      #df_features_abpdias.loc[curr_idx,'ABPDIAS_PAC'] = feature_extraction.feature_calculators.partial_autocorrelation(window_data,1)
      window_data = ''
      if slider.reached_end_of_list(): break
  
#abp mean
  abpmean_data = df_tsdata_subjectid.ABPMEAN
  bucket_size = 60
  overlap_count = 10
  slider = Slider(bucket_size,overlap_count)
  slider.fit(abpmean_data)       


  while True:
      window_data = slider.slide()
      # do your stuff
      #print(window_data)
      
      curr_idx = df_features_abpmean.shape[0]
      df_features_abpmean.loc[curr_idx,'ABPMEAN_MIN'] = feature_extraction.feature_calculators.minimum(window_data)
      df_features_abpmean.loc[curr_idx,'ABPMEAN_MAX'] = feature_extraction.feature_calculators.maximum(window_data)
      df_features_abpmean.loc[curr_idx,'ABPMEAN_MEAN'] = feature_extraction.feature_calculators.mean(window_data)
      df_features_abpmean.loc[curr_idx,'ABPMEAN_STD'] = feature_extraction.feature_calculators.standard_deviation(window_data)
      df_features_abpmean.loc[curr_idx,'ABPMEAN_VAR'] = feature_extraction.feature_calculators.variance(window_data)
      df_features_abpmean.loc[curr_idx,'ABPMEAN_AC'] = feature_extraction.feature_calculators.autocorrelation(window_data,1)
      df_features_abpmean.loc[curr_idx,'ABPMEAN_SKEW'] = feature_extraction.feature_calculators.skewness(window_data)
      df_features_abpmean.loc[curr_idx,'ABPMEAN_KURT'] = feature_extraction.feature_calculators.kurtosis(window_data)
      #df_features_abpmean.loc[curr_idx,'ABPMEAN_PAC'] = feature_extraction.feature_calculators.partial_autocorrelation(window_data,1)
      window_data = ''
      if slider.reached_end_of_list(): break

#spo2
  spo2_data = df_tsdata_subjectid.SPO2
  bucket_size = 60
  overlap_count = 10
  slider = Slider(bucket_size,overlap_count)
  slider.fit(spo2_data)       


  while True:
      window_data = slider.slide()
      # do your stuff
      #print(window_data)
      
      curr_idx = df_features_spo2.shape[0]
      df_features_spo2.loc[curr_idx,'SPO2_MIN'] = feature_extraction.feature_calculators.minimum(window_data)
      df_features_spo2.loc[curr_idx,'SPO2_MAX'] = feature_extraction.feature_calculators.maximum(window_data)
      df_features_spo2.loc[curr_idx,'SPO2_MEAN'] = feature_extraction.feature_calculators.mean(window_data)
      df_features_spo2.loc[curr_idx,'SPO2_STD'] = feature_extraction.feature_calculators.standard_deviation(window_data)
      df_features_spo2.loc[curr_idx,'SPO2_VAR'] = feature_extraction.feature_calculators.variance(window_data)
      df_features_spo2.loc[curr_idx,'SPO2_AC'] = feature_extraction.feature_calculators.autocorrelation(window_data,1)
      df_features_spo2.loc[curr_idx,'SPO2_SKEW'] = feature_extraction.feature_calculators.skewness(window_data)
      df_features_spo2.loc[curr_idx,'SPO2_KURT'] = feature_extraction.feature_calculators.kurtosis(window_data)
      #df_features_spo2.loc[curr_idx,'SPO2_PAC'] = feature_extraction.feature_calculators.partial_autocorrelation(window_data,1)
      window_data = ''
      if slider.reached_end_of_list(): break

#temp

  temp_data = df_tsdata_subjectid.TEMP
  bucket_size = 60
  overlap_count = 10
  slider = Slider(bucket_size,overlap_count)
  slider.fit(temp_data)       


  while True:
      window_data = slider.slide()
      # do your stuff
      #print(window_data)
      
      curr_idx = df_features_temp.shape[0]
      df_features_temp.loc[curr_idx,'TEMP_MIN'] = feature_extraction.feature_calculators.minimum(window_data)
      df_features_temp.loc[curr_idx,'TEMP_MAX'] = feature_extraction.feature_calculators.maximum(window_data)
      df_features_temp.loc[curr_idx,'TEMP_MEAN'] = feature_extraction.feature_calculators.mean(window_data)
      df_features_temp.loc[curr_idx,'TEMP_STD'] = feature_extraction.feature_calculators.standard_deviation(window_data)
      df_features_temp.loc[curr_idx,'TEMP_VAR'] = feature_extraction.feature_calculators.variance(window_data)
      df_features_temp.loc[curr_idx,'TEMP_AC'] = feature_extraction.feature_calculators.autocorrelation(window_data,1)
      df_features_temp.loc[curr_idx,'TEMP_SKEW'] = feature_extraction.feature_calculators.skewness(window_data)
      df_features_temp.loc[curr_idx,'TEMP_KURT'] = feature_extraction.feature_calculators.kurtosis(window_data)
      #df_features_temp.loc[curr_idx,'TEMP_PAC'] = feature_extraction.feature_calculators.partial_autocorrelation(window_data,1)
      window_data = ''
      if slider.reached_end_of_list(): break


  result = pd.concat([df_features_hr, df_features_resp,df_features_abpsys,df_features_abpdias,df_features_abpmean,df_features_spo2,df_features_temp], axis=1, sort=False)
  #print(result)
  df_features = df_features.append(result, ignore_index=True);
  print('inserted for subjectid: ', subject_id)

print(df_features)

!pip install tslearn
from tslearn.utils import to_time_series_dataset
from tslearn.preprocessing import TimeSeriesScalerMinMax
from tslearn.clustering import TimeSeriesKMeans
from sklearn.preprocessing import MinMaxScaler
#print(df_all404_withTemp_cleaned_MVimputed.columns)

#below commented code for selecting only 3 patients for clustering
"""
data_tslearn.SUBJECT_ID.unique()
subject_ids = [98930, 59864, 40227]
print(df_icutime[df_icutime.subject_id.isin(subject_ids)])
"""

intresting_cols = ['SUBJECT_ID',
               
               'HR_MIN','HR_MAX','HR_MEAN','HR_STD','HR_VAR','HR_SKEW','HR_KURT',

               'RESP_MIN','RESP_MAX','RESP_MEAN','RESP_STD','RESP_VAR','RESP_SKEW','RESP_KURT',

               'ABPSYS_MIN','ABPSYS_MAX','ABPSYS_MEAN','ABPSYS_STD','ABPSYS_VAR','ABPSYS_SKEW','ABPSYS_KURT',

               'ABPDIAS_MIN','ABPDIAS_MAX','ABPDIAS_MEAN','ABPDIAS_STD','ABPDIAS_VAR','ABPDIAS_SKEW','ABPDIAS_KURT',

               'ABPMEAN_MIN','ABPMEAN_MAX','ABPMEAN_MEAN','ABPMEAN_STD','ABPMEAN_VAR','ABPMEAN_SKEW','ABPMEAN_KURT',

               'SPO2_MIN','SPO2_MAX','SPO2_MEAN','SPO2_STD','SPO2_VAR','SPO2_SKEW','SPO2_KURT',

               'TEMP_MIN','TEMP_MAX','TEMP_MEAN','TEMP_STD','TEMP_VAR' ,'TEMP_SKEW','TEMP_KURT']

data_tslearn = df_features[intresting_cols]

data_slice = data_tslearn
#data_slice = data_tslearn[data_tslearn.SUBJECT_ID.isin(subject_ids)] #commented code for selecting only 3 patients for clustering
print('before scaling', data_slice)
scaler = MinMaxScaler()

data_slice['HR_MIN'] = scaler.fit_transform(data_slice['HR_MIN'].values.reshape(-1,1))
data_slice['HR_MAX'] = scaler.fit_transform(data_slice['HR_MAX'].values.reshape(-1,1))
data_slice['HR_MEAN'] = scaler.fit_transform(data_slice['HR_MEAN'].values.reshape(-1,1))
data_slice['HR_STD'] = scaler.fit_transform(data_slice['HR_STD'].values.reshape(-1,1))
data_slice['HR_VAR'] = scaler.fit_transform(data_slice['HR_VAR'].values.reshape(-1,1))
#data_slice['HR_AC'] = scaler.fit_transform(data_slice['HR_AC'].values.reshape(-1,1))
data_slice['HR_SKEW'] = scaler.fit_transform(data_slice['HR_SKEW'].values.reshape(-1,1))
data_slice['HR_KURT'] = scaler.fit_transform(data_slice['HR_KURT'].values.reshape(-1,1))


data_slice['RESP_MIN'] = scaler.fit_transform(data_slice['RESP_MIN'].values.reshape(-1,1))
data_slice['RESP_MAX'] = scaler.fit_transform(data_slice['RESP_MAX'].values.reshape(-1,1))
data_slice['RESP_MEAN'] = scaler.fit_transform(data_slice['RESP_MEAN'].values.reshape(-1,1))
data_slice['RESP_STD'] = scaler.fit_transform(data_slice['RESP_STD'].values.reshape(-1,1))
data_slice['RESP_VAR'] = scaler.fit_transform(data_slice['RESP_VAR'].values.reshape(-1,1))
#data_slice['RESP_AC'] = scaler.fit_transform(data_slice['RESP_AC'].values.reshape(-1,1))
data_slice['RESP_SKEW'] = scaler.fit_transform(data_slice['RESP_SKEW'].values.reshape(-1,1))
data_slice['RESP_KURT'] = scaler.fit_transform(data_slice['RESP_KURT'].values.reshape(-1,1))


data_slice['ABPSYS_MIN'] = scaler.fit_transform(data_slice['ABPSYS_MIN'].values.reshape(-1,1))
data_slice['ABPSYS_MAX'] = scaler.fit_transform(data_slice['ABPSYS_MAX'].values.reshape(-1,1))
data_slice['ABPSYS_MEAN'] = scaler.fit_transform(data_slice['ABPSYS_MEAN'].values.reshape(-1,1))
data_slice['ABPSYS_STD'] = scaler.fit_transform(data_slice['ABPSYS_STD'].values.reshape(-1,1))
data_slice['ABPSYS_VAR'] = scaler.fit_transform(data_slice['ABPSYS_VAR'].values.reshape(-1,1))
#data_slice['ABPSYS_AC'] = scaler.fit_transform(data_slice['ABPSYS_AC'].values.reshape(-1,1))
data_slice['ABPSYS_SKEW'] = scaler.fit_transform(data_slice['ABPSYS_SKEW'].values.reshape(-1,1))
data_slice['ABPSYS_KURT'] = scaler.fit_transform(data_slice['ABPSYS_KURT'].values.reshape(-1,1))


data_slice['ABPDIAS_MIN'] = scaler.fit_transform(data_slice['ABPDIAS_MIN'].values.reshape(-1,1))
data_slice['ABPDIAS_MAX'] = scaler.fit_transform(data_slice['ABPDIAS_MAX'].values.reshape(-1,1))
data_slice['ABPDIAS_MEAN'] = scaler.fit_transform(data_slice['ABPDIAS_MEAN'].values.reshape(-1,1))
data_slice['ABPDIAS_STD'] = scaler.fit_transform(data_slice['ABPDIAS_STD'].values.reshape(-1,1))
data_slice['ABPDIAS_VAR'] = scaler.fit_transform(data_slice['ABPDIAS_VAR'].values.reshape(-1,1))
#data_slice['ABPDIAS_AC'] = scaler.fit_transform(data_slice['ABPDIAS_AC'].values.reshape(-1,1))
data_slice['ABPDIAS_SKEW'] = scaler.fit_transform(data_slice['ABPDIAS_SKEW'].values.reshape(-1,1))
data_slice['ABPDIAS_KURT'] = scaler.fit_transform(data_slice['ABPDIAS_KURT'].values.reshape(-1,1))


data_slice['ABPMEAN_MIN'] = scaler.fit_transform(data_slice['ABPMEAN_MIN'].values.reshape(-1,1))
data_slice['ABPMEAN_MAX'] = scaler.fit_transform(data_slice['ABPMEAN_MAX'].values.reshape(-1,1))
data_slice['ABPMEAN_MEAN'] = scaler.fit_transform(data_slice['ABPMEAN_MEAN'].values.reshape(-1,1))
data_slice['ABPMEAN_STD'] = scaler.fit_transform(data_slice['ABPMEAN_STD'].values.reshape(-1,1))
data_slice['ABPMEAN_VAR'] = scaler.fit_transform(data_slice['ABPMEAN_VAR'].values.reshape(-1,1))
#data_slice['ABPMEAN_AC'] = scaler.fit_transform(data_slice['ABPMEAN_AC'].values.reshape(-1,1))
data_slice['ABPMEAN_SKEW'] = scaler.fit_transform(data_slice['ABPMEAN_SKEW'].values.reshape(-1,1))
data_slice['ABPMEAN_KURT'] = scaler.fit_transform(data_slice['ABPMEAN_KURT'].values.reshape(-1,1))




data_slice['SPO2_MIN'] = scaler.fit_transform(data_slice['SPO2_MIN'].values.reshape(-1,1))
data_slice['SPO2_MAX'] = scaler.fit_transform(data_slice['SPO2_MAX'].values.reshape(-1,1))
data_slice['SPO2_MEAN'] = scaler.fit_transform(data_slice['SPO2_MEAN'].values.reshape(-1,1))
data_slice['SPO2_STD'] = scaler.fit_transform(data_slice['SPO2_STD'].values.reshape(-1,1))
data_slice['SPO2_VAR'] = scaler.fit_transform(data_slice['SPO2_VAR'].values.reshape(-1,1))
#data_slice['SPO2_AC'] = scaler.fit_transform(data_slice['SPO2_AC'].values.reshape(-1,1))
data_slice['SPO2_SKEW'] = scaler.fit_transform(data_slice['SPO2_SKEW'].values.reshape(-1,1))
data_slice['SPO2_KURT'] = scaler.fit_transform(data_slice['SPO2_KURT'].values.reshape(-1,1))



data_slice['TEMP_MIN'] = scaler.fit_transform(data_slice['TEMP_MIN'].values.reshape(-1,1))
data_slice['TEMP_MAX'] = scaler.fit_transform(data_slice['TEMP_MAX'].values.reshape(-1,1))
data_slice['TEMP_MEAN'] = scaler.fit_transform(data_slice['TEMP_MEAN'].values.reshape(-1,1))
data_slice['TEMP_STD'] = scaler.fit_transform(data_slice['TEMP_STD'].values.reshape(-1,1))
data_slice['TEMP_VAR'] = scaler.fit_transform(data_slice['TEMP_VAR'].values.reshape(-1,1))
#data_slice['TEMP_AC'] = scaler.fit_transform(data_slice['TEMP_AC'].values.reshape(-1,1))
data_slice['TEMP_SKEW'] = scaler.fit_transform(data_slice['TEMP_SKEW'].values.reshape(-1,1))
data_slice['TEMP_KURT'] = scaler.fit_transform(data_slice['TEMP_KURT'].values.reshape(-1,1))



print('after scaling')
print( data_slice)


arr = data_slice.set_index('SUBJECT_ID').groupby('SUBJECT_ID').apply(pd.DataFrame.to_numpy).to_numpy()
#print('AFTER GROUPING')
#print(arr)

arr= to_time_series_dataset(arr)

print('AFTER CONVERTING TO TIME SERIES DATASET', arr)

from tslearn.metrics import cdist_dtw
from tslearn.clustering import silhouette_score
sum_of_squared_distances = []
K = range(2,20)
for k in K:
    k_means=TimeSeriesKMeans(n_clusters=k, metric="dtw", max_iter=20,
                       random_state=0).fit(arr)
    sum_of_squared_distances.append(k_means.inertia_)
    labels = k_means.labels_
    print('for cluster numbers'+ str(k)+ ' : '+ str(silhouette_score(arr, labels, metric="dtw")))

plt.plot(K, sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('sum_of_squared_distances')
plt.title('elbow method for optimal k')
plt.show()

km = TimeSeriesKMeans(n_clusters=3, metric="dtw", max_iter=20,
                       random_state=0).fit(arr)
print('AFTER CLUSTERING')
print(km.labels_)
df_clustering_result['CLUSTER_LABEL'] = km.labels_

print(df_clustering_result[ df_clustering_result['SEPSIS_ONSETTIME'].notna()].groupby('CLUSTER_LABEL').count()) #septic patients
print(df_clustering_result[ df_clustering_result['SEPSIS_ONSETTIME'].isna()].groupby('CLUSTER_LABEL').count()) # non- septic patients
print(df_clustering_result)
print(df_clustering_result[df_clustering_result['CLUSTER_LABEL']==1])

