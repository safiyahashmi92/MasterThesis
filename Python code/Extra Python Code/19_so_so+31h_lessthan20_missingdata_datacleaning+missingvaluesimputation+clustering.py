# -*- coding: utf-8 -*-
"""19_SO_SO+31h_LessThan20%MissingData_DataCleaning+MissingValuesImputation+clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1orlehq7esKkr4D5rIHIJS3oJJXmCIwiE
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
import pandas as pd
import io
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
import os
import shutil
import posixpath
import urllib.request
import datetime
from collections import namedtuple

drive.mount('/content/gdrive')

# Download All Time series data for 404 patients.
df_all251_withTemp = pd.read_csv('/content/gdrive/My Drive/Master thesis/df_ts_records_all251SepsisPatients(lessThan20%Missing)_sepsisOnsetPlus31h_WITH_TEMP.csv')
df_all251_withTemp['TIME'] =  pd.to_datetime(df_all251_withTemp['TIME'])
print(df_all251_withTemp.shape)

#cleaning data :  removing all negative values

df_all251_withTemp.HR = df_all251_withTemp.HR.mask(df_all251_withTemp.HR < 0)

df_all251_withTemp.RESP = df_all251_withTemp.RESP.mask(df_all251_withTemp.RESP < 0)

df_all251_withTemp.ABPSYS = df_all251_withTemp.ABPSYS.mask(df_all251_withTemp.ABPSYS < 0)

df_all251_withTemp.ABPDIAS = df_all251_withTemp.ABPDIAS.mask(df_all251_withTemp.ABPDIAS < 0)

df_all251_withTemp.ABPMEAN = df_all251_withTemp.ABPMEAN.mask(df_all251_withTemp.ABPMEAN < 0)

df_all251_withTemp.SPO2 = df_all251_withTemp.SPO2.mask(df_all251_withTemp.SPO2 < 0)

df_all251_withTemp.TEMP = df_all251_withTemp.TEMP.mask(df_all251_withTemp.TEMP < 0)

# Missing value imputation by carry forward scheme
df_all251_withTemp_cleaned_MVimputed = df_all251_withTemp.ffill().bfill()

df_all251_withTemp_cleaned_MVimputed.HR = df_all251_withTemp_cleaned_MVimputed.HR.round(decimals=4)
df_all251_withTemp_cleaned_MVimputed.RESP = df_all251_withTemp_cleaned_MVimputed.RESP.round(decimals=4)
df_all251_withTemp_cleaned_MVimputed.ABPSYS = df_all251_withTemp_cleaned_MVimputed.ABPSYS.round(decimals=4)
df_all251_withTemp_cleaned_MVimputed.ABPDIAS = df_all251_withTemp_cleaned_MVimputed.ABPDIAS.round(decimals=4)
df_all251_withTemp_cleaned_MVimputed.ABPMEAN = df_all251_withTemp_cleaned_MVimputed.ABPMEAN.round(decimals=4)
df_all251_withTemp_cleaned_MVimputed.SPO2 = df_all251_withTemp_cleaned_MVimputed.SPO2.round(decimals=4)
df_all251_withTemp_cleaned_MVimputed.TEMP = df_all251_withTemp_cleaned_MVimputed.TEMP.round(decimals=4)

df_all251_withTemp_cleaned_MVimputed['TIME'] =  pd.to_datetime(df_all251_withTemp_cleaned_MVimputed['TIME'])

# Check if any null values exists in the final dataframe after cleaning and imputing missing values.
print(df_all251_withTemp_cleaned_MVimputed[df_all251_withTemp_cleaned_MVimputed.isnull().any(axis=1)])
print(df_all251_withTemp_cleaned_MVimputed)

"""
# **Plotting data for a single SHOCK *patient* to visualize the raw data**"""

import plotly.graph_objects as go
fig = go.Figure()
df_single_subject = df_all251_withTemp_cleaned_MVimputed[df_all251_withTemp_cleaned_MVimputed['SUBJECT_ID']==69272] # 98253
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,4], name = df_single_subject.iloc[:,4].name, line = dict(color = '#17BECF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,5], name = df_single_subject.iloc[:,5].name, line = dict(color = '#CF1717'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,6], name = df_single_subject.iloc[:,6].name, line = dict(color = '#AACF17'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,7], name = df_single_subject.iloc[:,7].name, line = dict(color = '#17CF29'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,8], name = df_single_subject.iloc[:,8].name, line = dict(color = '#1742CF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,9], name = df_single_subject.iloc[:,9].name, line = dict(color = '#B017CF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,10], name = df_single_subject.iloc[:,10].name, line = dict(color = '#CFA417'), opacity = 0.8))
fig.update_layout(title_text=df_single_subject.iloc[:,4].name+', '+ df_single_subject.iloc[:,5].name+', '+df_single_subject.iloc[:,6].name+', '+df_single_subject.iloc[:,7].name+', '+df_single_subject.iloc[:,8].name+', '+
                 df_single_subject.iloc[:,9].name+', '+df_single_subject.iloc[:,10].name)

fig.show()

"""
# **Plotting data for a single NON-SHOCK *patient* to visualize the raw data**


"""

import plotly.graph_objects as go
fig = go.Figure()
df_single_subject = df_all251_withTemp_cleaned_MVimputed[df_all251_withTemp_cleaned_MVimputed['SUBJECT_ID']==98253] # 
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,4], name = df_single_subject.iloc[:,4].name, line = dict(color = '#17BECF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,5], name = df_single_subject.iloc[:,5].name, line = dict(color = '#CF1717'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,6], name = df_single_subject.iloc[:,6].name, line = dict(color = '#AACF17'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,7], name = df_single_subject.iloc[:,7].name, line = dict(color = '#17CF29'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,8], name = df_single_subject.iloc[:,8].name, line = dict(color = '#1742CF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,9], name = df_single_subject.iloc[:,9].name, line = dict(color = '#B017CF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,10], name = df_single_subject.iloc[:,10].name, line = dict(color = '#CFA417'), opacity = 0.8))
fig.update_layout(title_text=df_single_subject.iloc[:,4].name+', '+ df_single_subject.iloc[:,5].name+', '+df_single_subject.iloc[:,6].name+', '+df_single_subject.iloc[:,7].name+', '+df_single_subject.iloc[:,8].name+', '+
                 df_single_subject.iloc[:,9].name+', '+df_single_subject.iloc[:,10].name)

fig.show()

"""# **Heat Map on all 251 patients data to visualize correlation between the raw VITAL SIGNS**

"""

import seaborn as sns
data_corr = df_all251_withTemp_cleaned_MVimputed[['HR', 'SPO2', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'RESP', 'TEMP']]
print(data_corr)
corr = data_corr.corr()# calculating the correlation between the above vital signs
sns.heatmap(corr, square=True) # plotting the correlation

"""#**checking stationairty in the the raw data for a single patient**"""

from statsmodels.tsa.stattools import adfuller
df_stationarity = df_all251_withTemp_cleaned_MVimputed[df_all251_withTemp_cleaned_MVimputed['SUBJECT_ID']==98253] # 
df_stationarity = df_stationarity[ ['TIME','HR', 'SPO2', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'RESP', 'TEMP']]

df_stationarity = df_stationarity.set_index("TIME")
df_stationarity=df_stationarity.dropna()
columns = df_stationarity.columns
print(columns)

index=df_stationarity.index
columns = df_stationarity.columns
df_stats = pd.DataFrame(columns=['P-value'], index=columns)
for col in columns:
  dftest = adfuller(df_stationarity[col], autolag='AIC')
  df_stats.loc[col]=dftest[1]

print('Printing raw data')
print(df_stats)

import plotly.graph_objects as go
fig = go.Figure(data=[go.Bar(
            x=df_stats.index, y=df_stats['P-value'],
            text=df_stats['P-value'],
            textposition='auto',)])

fig.show()

"""# **Taking first order difference and checking stationairty in the the raw data for a single patient**

# **Plotting raw data and first order difference data for each of the vitals**
"""

from statsmodels.tsa.stattools import adfuller
df_before_diff = df_all251_withTemp_cleaned_MVimputed[df_all251_withTemp_cleaned_MVimputed['SUBJECT_ID']==98253] # 
df_before_diff = df_before_diff[ ['TIME','HR', 'SPO2', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'RESP', 'TEMP']]


df_stationarity = df_all251_withTemp_cleaned_MVimputed[df_all251_withTemp_cleaned_MVimputed['SUBJECT_ID']==98253] # 
df_stationarity = df_stationarity[ ['TIME','HR', 'SPO2', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'RESP', 'TEMP']]

df_stationarity = df_stationarity.set_index("TIME")
df_stationarity=df_stationarity.dropna()
columns = df_stationarity.columns
print(columns)


data_diff = pd.DataFrame(columns=columns)
data_diff['TIME']=df_stationarity.index
for col in columns:
  data_diff[col] = pd.DataFrame(np.diff(df_stationarity[col]))

data_diff=data_diff.dropna()
for col in columns:
  dftest = adfuller(data_diff[col], autolag='AIC')
  df_stats.loc[col]=dftest[1]


import plotly.graph_objects as go
fig = go.Figure(data=[go.Bar(
            x=df_stats.index, y=df_stats['P-value'],
            text=df_stats['P-value'],
            textposition='auto',)])

fig.show()
"""
print(df_before_diff)
print(data_diff)
"""
import plotly.graph_objects as go
# plotting TS original and after taking the first order differece for HR
fig = go.Figure()

fig.add_trace(go.Scatter(x=df_before_diff.TIME, y=df_before_diff.iloc[:,1], name = df_before_diff.iloc[:,1].name, line = dict(color = '#CFA417'), opacity = 0.8))

fig.add_trace(go.Scatter(x=data_diff.TIME, y=data_diff.iloc[:,0], name = data_diff.iloc[:,0].name, line = dict(color = '#17BECF'), opacity = 0.8))
fig.show()
# plotting TS original and after taking the first order differece for SPO2

fig = go.Figure()

fig.add_trace(go.Scatter(x=df_before_diff.TIME, y=df_before_diff.iloc[:,2], name = df_before_diff.iloc[:,2].name, line = dict(color = '#CFA417'), opacity = 0.8))

fig.add_trace(go.Scatter(x=data_diff.TIME, y=data_diff.iloc[:,1], name = data_diff.iloc[:,1].name, line = dict(color = '#17BECF'), opacity = 0.8))
fig.show()

# plotting TS original and after taking the first order differece for ABPSYS

fig = go.Figure()

fig.add_trace(go.Scatter(x=df_before_diff.TIME, y=df_before_diff.iloc[:,3], name = df_before_diff.iloc[:,3].name, line = dict(color = '#CFA417'), opacity = 0.8))

fig.add_trace(go.Scatter(x=data_diff.TIME, y=data_diff.iloc[:,2], name = data_diff.iloc[:,2].name, line = dict(color = '#17BECF'), opacity = 0.8))
fig.show()



# plotting TS original and after taking the first order differece for ABPDIAS

fig = go.Figure()

fig.add_trace(go.Scatter(x=df_before_diff.TIME, y=df_before_diff.iloc[:,4], name = df_before_diff.iloc[:,4].name, line = dict(color = '#CFA417'), opacity = 0.8))

fig.add_trace(go.Scatter(x=data_diff.TIME, y=data_diff.iloc[:,3], name = data_diff.iloc[:,3].name, line = dict(color = '#17BECF'), opacity = 0.8))
fig.show()



# plotting TS original and after taking the first order differece for ABPMEAN

fig = go.Figure()

fig.add_trace(go.Scatter(x=df_before_diff.TIME, y=df_before_diff.iloc[:,5], name = df_before_diff.iloc[:,5].name, line = dict(color = '#CFA417'), opacity = 0.8))

fig.add_trace(go.Scatter(x=data_diff.TIME, y=data_diff.iloc[:,4], name = data_diff.iloc[:,4].name, line = dict(color = '#17BECF'), opacity = 0.8))
fig.show()


# plotting TS original and after taking the first order differece for RESP

fig = go.Figure()

fig.add_trace(go.Scatter(x=df_before_diff.TIME, y=df_before_diff.iloc[:,6], name = df_before_diff.iloc[:,6].name, line = dict(color = '#CFA417'), opacity = 0.8))

fig.add_trace(go.Scatter(x=data_diff.TIME, y=data_diff.iloc[:,5], name = data_diff.iloc[:,5].name, line = dict(color = '#17BECF'), opacity = 0.8))
fig.show()



# plotting TS original and after taking the first order differece for TEMP

fig = go.Figure()

fig.add_trace(go.Scatter(x=df_before_diff.TIME, y=df_before_diff.iloc[:,7], name = df_before_diff.iloc[:,7].name, line = dict(color = '#CFA417'), opacity = 0.8))

fig.add_trace(go.Scatter(x=data_diff.TIME, y=data_diff.iloc[:,6], name = data_diff.iloc[:,6].name, line = dict(color = '#17BECF'), opacity = 0.8))
fig.show()

subject_ids = df_all251_withTemp_cleaned_MVimputed.SUBJECT_ID.unique()
print((subject_ids))

from google.colab import files
uploaded = files.upload()

df_icutime = pd.read_csv(io.BytesIO(uploaded['2_Only_AllSepsisPatients_with_%MissingData_sepsisOnset+31_fluid+26.csv']))
print (df_icutime.columns)
#df_icutime = df_icutime[['subject_id','icustay_id','intime','outtime','sepsis_onsettime']]
#print (df_icutime.columns)
"""
df_icutime['intime'] =  pd.to_datetime(df_icutime['intime'])
df_icutime['outtime'] =  pd.to_datetime(df_icutime['outtime'])
"""

"""#**FEATURE EXTRACTION FOR CLUSTERING**"""

"""
feature_cols= ['SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK',
               'HR_MIN','HR_MAX','HR_MEAN','HR_STD','HR_VAR',
               'RESP_MIN','RESP_MAX','RESP_MEAN','RESP_STD','RESP_VAR',
               'ABPSYS_MIN','ABPSYS_MAX','ABPSYS_MEAN','ABPSYS_STD','ABPSYS_VAR',
               'ABPDIAS_MIN','ABPDIAS_MAX','ABPDIAS_MEAN','ABPDIAS_STD','ABPDIAS_VAR',
               'ABPMEAN_MIN','ABPMEAN_MAX','ABPMEAN_MEAN','ABPMEAN_STD','ABPMEAN_VAR',
               'SPO2_MIN','SPO2_MAX','SPO2_MEAN','SPO2_STD','SPO2_VAR',
               'TEMP_MIN','TEMP_MAX','TEMP_MEAN','TEMP_STD','TEMP_VAR' ] 
"""
feature_cols= ['SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK',
               'HR_MEAN_10','HR_MEAN_20','HR_MEAN_31',
               'RESP_MEAN_10','RESP_MEAN_20','RESP_MEAN_31',
               'ABPSYS_MEAN_10' ,'ABPSYS_MEAN_20','ABPSYS_MEAN_31',
               'ABPDIAS_MEAN_10','ABPDIAS_MEAN_20','ABPDIAS_MEAN_31',
               'ABPMEAN_MEAN_10','ABPMEAN_MEAN_20','ABPMEAN_MEAN_31',
               'SPO2_MEAN_10','SPO2_MEAN_20','SPO2_MEAN_31',
               'TEMP_MEAN_10','TEMP_MEAN_20','TEMP_MEAN_31' ] 

df_features =pd.DataFrame(columns=feature_cols);

for subject_id in subject_ids:
  curr_idx = df_features.shape[0]
  #df_icuintime_subjectid = df_icutime[df_icutime['subject_id'] == subject_id] 

  icu_intime = df_icutime.loc[df_icutime.subject_id==subject_id , 'intime'].values[0]
  sepsis_onsettime = df_icutime.loc[df_icutime.subject_id==subject_id , 'sepsis_onsettime'].values[0]
  shock_onsetttime = df_icutime.loc[df_icutime.subject_id==subject_id , 'sepstic_shock_onsettime'].values[0]
  print(shock_onsetttime)

  df_tsdata_subjectid_first_10h = df_all251_withTemp_cleaned_MVimputed[(df_all251_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( ( df_all251_withTemp_cleaned_MVimputed['TIME'] >= datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S')    ) & ( df_all251_withTemp_cleaned_MVimputed['TIME'] <= (datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=10)) )  )]
  
  df_tsdata_subjectid_first_20h = df_all251_withTemp_cleaned_MVimputed[(df_all251_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( ( df_all251_withTemp_cleaned_MVimputed['TIME'] >= datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S')    ) & ( df_all251_withTemp_cleaned_MVimputed['TIME'] <= (datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=20)) )  )]

  df_tsdata_subjectid_entire_31h = df_all251_withTemp_cleaned_MVimputed[(df_all251_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( ( df_all251_withTemp_cleaned_MVimputed['TIME'] >= datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S')    ) & ( df_all251_withTemp_cleaned_MVimputed['TIME'] <= (datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=31)) )  )]
  
  #data_test = df_all404_withTemp_cleaned_MVimputed[(df_all404_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) ]
  #print(data_test[['TIME','HR','RESP','ABPSYS','TEMP']]) # 
  df_features.loc[curr_idx,'SUBJECT_ID'] = subject_id
  df_features.loc[curr_idx,'SEPSIS_ONSETTIME'] = sepsis_onsettime
  df_features.loc[curr_idx,'SEPSIS_SHOCK_ONSETTIME'] = shock_onsetttime
  if str(shock_onsetttime) != 'nan':
    df_features.loc[curr_idx,'HAS_SHOCK'] = 1
  else:
    df_features.loc[curr_idx,'HAS_SHOCK'] = 0

  # CALCULATING MEANS for first 10 hours of data i.e. from sepsis onset time till sepsis onset time + 10 h
  df_features.loc[curr_idx,'HR_MEAN_10'] = df_tsdata_subjectid_first_10h['HR'].mean()
  df_features.loc[curr_idx,'RESP_MEAN_10'] = df_tsdata_subjectid_first_10h['RESP'].mean()
  df_features.loc[curr_idx,'ABPSYS_MEAN_10'] = df_tsdata_subjectid_first_10h['ABPSYS'].mean()
  df_features.loc[curr_idx,'ABPDIAS_MEAN_10'] = df_tsdata_subjectid_first_10h['ABPDIAS'].mean()
  df_features.loc[curr_idx,'ABPMEAN_MEAN_10'] = df_tsdata_subjectid_first_10h['ABPMEAN'].mean()
  df_features.loc[curr_idx,'SPO2_MEAN_10'] = df_tsdata_subjectid_first_10h['SPO2'].mean()
  df_features.loc[curr_idx,'TEMP_MEAN_10'] = df_tsdata_subjectid_first_10h['TEMP'].mean()

  # CALCULATING MEANS for first 20 hours of data i.e. from sepsis onset time till sepsis onset time + 20 h

  df_features.loc[curr_idx,'HR_MEAN_20'] = df_tsdata_subjectid_first_20h['HR'].mean()
  df_features.loc[curr_idx,'RESP_MEAN_20'] = df_tsdata_subjectid_first_20h['RESP'].mean()
  df_features.loc[curr_idx,'ABPSYS_MEAN_20'] = df_tsdata_subjectid_first_20h['ABPSYS'].mean()
  df_features.loc[curr_idx,'ABPDIAS_MEAN_20'] = df_tsdata_subjectid_first_20h['ABPDIAS'].mean()
  df_features.loc[curr_idx,'ABPMEAN_MEAN_20'] = df_tsdata_subjectid_first_20h['ABPMEAN'].mean()
  df_features.loc[curr_idx,'SPO2_MEAN_20'] = df_tsdata_subjectid_first_20h['SPO2'].mean()
  df_features.loc[curr_idx,'TEMP_MEAN_20'] = df_tsdata_subjectid_first_20h['TEMP'].mean()

  # CALCULATING MEANS for ENTIRE 31 hours of data i.e. from sepsis onset time till sepsis onset time + 31 h

  df_features.loc[curr_idx,'HR_MEAN_31'] = df_tsdata_subjectid_entire_31h['HR'].mean()
  df_features.loc[curr_idx,'RESP_MEAN_31'] = df_tsdata_subjectid_entire_31h['RESP'].mean()
  df_features.loc[curr_idx,'ABPSYS_MEAN_31'] = df_tsdata_subjectid_entire_31h['ABPSYS'].mean()
  df_features.loc[curr_idx,'ABPDIAS_MEAN_31'] = df_tsdata_subjectid_entire_31h['ABPDIAS'].mean()
  df_features.loc[curr_idx,'ABPMEAN_MEAN_31'] = df_tsdata_subjectid_entire_31h['ABPMEAN'].mean()
  df_features.loc[curr_idx,'SPO2_MEAN_31'] = df_tsdata_subjectid_entire_31h['SPO2'].mean()
  df_features.loc[curr_idx,'TEMP_MEAN_31'] = df_tsdata_subjectid_entire_31h['TEMP'].mean()


  
  """
  df_features.loc[curr_idx,'HR_MIN'] = df_tsdata_subjectid['HR'].min()
  df_features.loc[curr_idx,'HR_MAX'] = df_tsdata_subjectid['HR'].max()
  df_features.loc[curr_idx,'HR_MEAN'] = df_tsdata_subjectid['HR'].mean()
  df_features.loc[curr_idx,'HR_STD'] = df_tsdata_subjectid['HR'].std()
  df_features.loc[curr_idx,'HR_VAR'] = df_tsdata_subjectid['HR'].var()

  df_features.loc[curr_idx,'RESP_MIN'] = df_tsdata_subjectid['RESP'].min()
  df_features.loc[curr_idx,'RESP_MAX']= df_tsdata_subjectid['RESP'].max()
  df_features.loc[curr_idx,'RESP_MEAN'] = df_tsdata_subjectid['RESP'].mean()
  df_features.loc[curr_idx,'RESP_STD'] = df_tsdata_subjectid['RESP'].std()
  df_features.loc[curr_idx,'RESP_VAR'] = df_tsdata_subjectid['RESP'].var()

  df_features.loc[curr_idx,'ABPSYS_MIN'] = df_tsdata_subjectid['ABPSYS'].min()
  df_features.loc[curr_idx,'ABPSYS_MAX'] = df_tsdata_subjectid['ABPSYS'].max()
  df_features.loc[curr_idx,'ABPSYS_MEAN'] = df_tsdata_subjectid['ABPSYS'].mean()
  df_features.loc[curr_idx,'ABPSYS_STD'] = df_tsdata_subjectid['ABPSYS'].std()
  df_features.loc[curr_idx,'ABPSYS_VAR']  = df_tsdata_subjectid['ABPSYS'].var()

  df_features.loc[curr_idx,'ABPDIAS_MIN']  = df_tsdata_subjectid['ABPDIAS'].min()
  df_features.loc[curr_idx,'ABPDIAS_MAX'] = df_tsdata_subjectid['ABPDIAS'].max()
  df_features.loc[curr_idx,'ABPDIAS_MEAN'] = df_tsdata_subjectid['ABPDIAS'].mean()
  df_features.loc[curr_idx,'ABPDIAS_STD'] = df_tsdata_subjectid['ABPDIAS'].std()
  df_features.loc[curr_idx,'ABPDIAS_VAR'] = df_tsdata_subjectid['ABPDIAS'].var()

  df_features.loc[curr_idx,'ABPMEAN_MIN']= df_tsdata_subjectid['ABPMEAN'].min()
  df_features.loc[curr_idx,'ABPMEAN_MAX']= df_tsdata_subjectid['ABPMEAN'].max()
  df_features.loc[curr_idx,'ABPMEAN_MEAN']= df_tsdata_subjectid['ABPMEAN'].mean()
  df_features.loc[curr_idx,'ABPMEAN_STD']= df_tsdata_subjectid['ABPMEAN'].std()
  df_features.loc[curr_idx,'ABPMEAN_VAR']= df_tsdata_subjectid['ABPMEAN'].var()

  df_features.loc[curr_idx,'SPO2_MIN']= df_tsdata_subjectid['SPO2'].min()
  df_features.loc[curr_idx,'SPO2_MAX']= df_tsdata_subjectid['SPO2'].max()
  df_features.loc[curr_idx,'SPO2_MEAN']= df_tsdata_subjectid['SPO2'].mean()
  df_features.loc[curr_idx,'SPO2_STD']= df_tsdata_subjectid['SPO2'].std()
  df_features.loc[curr_idx,'SPO2_VAR']= df_tsdata_subjectid['SPO2'].var()

  df_features.loc[curr_idx,'TEMP_MIN'] = df_tsdata_subjectid['TEMP'].min()
  df_features.loc[curr_idx,'TEMP_MAX']= df_tsdata_subjectid['TEMP'].max()
  df_features.loc[curr_idx,'TEMP_MEAN']= df_tsdata_subjectid['TEMP'].mean()
  df_features.loc[curr_idx,'TEMP_STD']= df_tsdata_subjectid['TEMP'].std()
  df_features.loc[curr_idx,'TEMP_VAR']= df_tsdata_subjectid['TEMP'].var()
  """

  icu_intime='';
  sepsis_onsettime='';
  shock_onsetttime = '';
  df_tsdata_subjectid_first_10h.drop(df_tsdata_subjectid_first_10h.index, inplace=True)
  df_tsdata_subjectid_first_20h.drop(df_tsdata_subjectid_first_20h.index, inplace=True) 
  df_tsdata_subjectid_entire_31h.drop(df_tsdata_subjectid_entire_31h.index, inplace=True)  
  #print(df_features)

pd.set_option('display.max_rows', 50000)
pd.set_option('display.max_columns', 50)
pd.set_option('display.width', 1000)
pd.set_option('max_colwidth', 800)
print(df_features.columns)

"""#**NORMAL K-MEANS CLUSTERING USING THE EXTRACTED FEATURES**"""

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA

cols_of_interest = ['HR_MEAN_10','HR_MEAN_20','HR_MEAN_31',
               'RESP_MEAN_10','RESP_MEAN_20','RESP_MEAN_31',
               'ABPSYS_MEAN_10' ,'ABPSYS_MEAN_20','ABPSYS_MEAN_31',
               'ABPDIAS_MEAN_10','ABPDIAS_MEAN_20','ABPDIAS_MEAN_31',
               'ABPMEAN_MEAN_10','ABPMEAN_MEAN_20','ABPMEAN_MEAN_31',
               'SPO2_MEAN_10','SPO2_MEAN_20','SPO2_MEAN_31',
               'TEMP_MEAN_10','TEMP_MEAN_20','TEMP_MEAN_31' ];
               

data = df_features[cols_of_interest]
scaled_data = MinMaxScaler().fit_transform(data)
print(scaled_data)

#CHECKING THE ELBOW PLOT
"""
from sklearn.cluster import KMeans
sum_of_squared_distances = []
K = range(2,20)
for k in K:
    k_means = KMeans(n_clusters=k)
    model = k_means.fit(scaled_data)
    sum_of_squared_distances.append(k_means.inertia_)
    print(metrics.silhouette_score(scaled_data, k_means.labels_, metric = 'euclidean'))


plt.plot(K, sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('sum_of_squared_distances')
plt.title('elbow method for optimal k')
plt.show()
"""

import seaborn as sns
import sklearn
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn import metrics
k_means = KMeans(n_clusters=2, max_iter=100)
#Run the clustering algorithm
model = k_means.fit(scaled_data)
#Generate cluster predictions and store in y_hat
y_hat = k_means.predict(scaled_data)
df_features['CLUSTER'] = k_means.labels_

#print(df_features)


print('Printing metircs for 2 clusters')
print(metrics.silhouette_score(scaled_data,  k_means.labels_, metric = 'euclidean'))
print(metrics.calinski_harabasz_score(scaled_data,   k_means.labels_))#
hom = metrics.homogeneity_score(df_features.HAS_SHOCK, df_features.CLUSTER)
print(hom)

com = metrics.completeness_score(df_features.HAS_SHOCK, df_features.CLUSTER)
print(com)

v_meas = metrics.v_measure_score(df_features.HAS_SHOCK, df_features.CLUSTER)
print(v_meas)



y_true = df_features['HAS_SHOCK']
y_true = y_true.apply(pd.to_numeric).to_numpy()

y_pred = df_features['CLUSTER']
y_pred = y_pred.apply(pd.to_numeric).to_numpy()

precision = metrics.precision_score(y_true, y_pred)
print(precision)

accuracy = metrics.accuracy_score(y_true, y_pred)
print(accuracy)

"""#**K-MEANS CLUSTERING APLYING PCA ON SCALED FEATURES EXTRACTED ABOVE**"""

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA

cols_of_interest = ['HR_MEAN_10','HR_MEAN_20','HR_MEAN_31',
               'RESP_MEAN_10','RESP_MEAN_20','RESP_MEAN_31',
               'ABPSYS_MEAN_10' ,'ABPSYS_MEAN_20','ABPSYS_MEAN_31',
               'ABPDIAS_MEAN_10','ABPDIAS_MEAN_20','ABPDIAS_MEAN_31',
               'ABPMEAN_MEAN_10','ABPMEAN_MEAN_20','ABPMEAN_MEAN_31',
               'SPO2_MEAN_10','SPO2_MEAN_20','SPO2_MEAN_31',
               'TEMP_MEAN_10','TEMP_MEAN_20','TEMP_MEAN_31' ];

data = df_features[cols_of_interest]
scaled_data = MinMaxScaler().fit_transform(data)
print('scaled data dataframe')
print(scaled_data)

#plotting PCA components
pca = PCA(n_components=10)
principalComponents = pca.fit_transform(scaled_data)
# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='black')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)

# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)
df_features['PCA_1'] = PCA_components[0]
df_features['PCA_2'] = PCA_components[1]

print(df_features.columns)

#print('Printing PCA dataframe')
#print(PCA_components)

# PLOTTING FIRST 2 PCA
plt.scatter(PCA_components[0], PCA_components[1], alpha=.1, color='black')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')

import seaborn as sns
import sklearn
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn import metrics
import matplotlib.pyplot as plt

k_means = KMeans(n_clusters=2, max_iter=100)
#Run the clustering algorithm
model = k_means.fit(PCA_components.iloc[:,:2])
#Generate cluster predictions and store in y_hat
#y_hat = k_means.predict(PCA_components.iloc[:,:2])
df_features['CLUSTER'] = k_means.labels_

print(df_features.columns)


print('Printing metircs for 2 clusters')
print(metrics.silhouette_score(scaled_data,  k_means.labels_, metric = 'euclidean'))
print(metrics.calinski_harabasz_score(scaled_data,   k_means.labels_))#
hom = metrics.homogeneity_score(df_features.HAS_SHOCK, df_features.CLUSTER)
print(hom)

com = metrics.completeness_score(df_features.HAS_SHOCK, df_features.CLUSTER)
print(com)

v_meas = metrics.v_measure_score(df_features.HAS_SHOCK, df_features.CLUSTER)
print(v_meas)


y_true = df_features['HAS_SHOCK']
y_true = y_true.apply(pd.to_numeric).to_numpy()

y_pred = df_features['CLUSTER']
y_pred = y_pred.apply(pd.to_numeric).to_numpy()

precision = metrics.precision_score(y_true, y_pred)
print(precision)

accuracy = metrics.accuracy_score(y_true, y_pred)
print(accuracy)

"""
# plotting clusters
pl.figure('K-means with 3 clusters')
pl.scatter(PCA_components.iloc[:,0] , PCA_components.iloc[:,1] , c=k_means.labels_)
pl.show()
"""
x_axis = df_features['PCA_1']
y_axis = df_features['PCA_2']
plt.figure(figsize=(8,8))
sns.scatterplot(x_axis , y_axis, hue= df_features['CLUSTER'])
plt.title('Clusters by PCA components')
plt.show()


print(df_features[df_features['SEPSIS_SHOCK_ONSETTIME'].notna()].groupby('CLUSTER').count()) #shock patients :cluster 1
print(df_features[df_features['SEPSIS_SHOCK_ONSETTIME'].isna()].groupby('CLUSTER').count()) # non- shock patients : cluster 0

"""#**TRYING RANDOM FOREST CLASSIFIER TO KNOW IMPORTANT TOP FEATURES**

"""

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MinMaxScaler

print(df_features.columns)
cols_of_interest = ['HR_MEAN_10','HR_MEAN_20','HR_MEAN_31',
               'RESP_MEAN_10','RESP_MEAN_20','RESP_MEAN_31',
               'ABPSYS_MEAN_10' ,'ABPSYS_MEAN_20','ABPSYS_MEAN_31',
               'ABPDIAS_MEAN_10','ABPDIAS_MEAN_20','ABPDIAS_MEAN_31',
               'ABPMEAN_MEAN_10','ABPMEAN_MEAN_20','ABPMEAN_MEAN_31',
               'SPO2_MEAN_10','SPO2_MEAN_20','SPO2_MEAN_31',
               'TEMP_MEAN_10','TEMP_MEAN_20','TEMP_MEAN_31' ];

feat_labels = ['HR_MEAN_10','HR_MEAN_20','HR_MEAN_31',
               'RESP_MEAN_10','RESP_MEAN_20','RESP_MEAN_31',
               'ABPSYS_MEAN_10' ,'ABPSYS_MEAN_20','ABPSYS_MEAN_31',
               'ABPDIAS_MEAN_10','ABPDIAS_MEAN_20','ABPDIAS_MEAN_31',
               'ABPMEAN_MEAN_10','ABPMEAN_MEAN_20','ABPMEAN_MEAN_31',
               'SPO2_MEAN_10','SPO2_MEAN_20','SPO2_MEAN_31',
               'TEMP_MEAN_10','TEMP_MEAN_20','TEMP_MEAN_31' ]

X = df_features[cols_of_interest]

X = X.apply(pd.to_numeric)
X = MinMaxScaler().fit_transform(X)

y= df_features['CLUSTER']
y = y.apply(pd.to_numeric).to_numpy()

#print(X)
#print(y)

#splitting data into test and train
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)


# Create a random forest classifier
clf = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)

# Train the classifier
clf.fit(X_train, y_train)

# Print the name and gini importance of each feature
for feature in zip(feat_labels, clf.feature_importances_):
    print(feature)

import plotly.graph_objects as go
names = feat_labels
fig = go.Figure(data=[go.Bar(
            x=feat_labels, y=clf.feature_importances_,
            text=clf.feature_importances_,
            textposition='auto',)])

fig.show()

#clustering using only most important features, i.e. value > 0.05
# 'ABPSYS_MEAN_10','ABPSYS_MEAN_20' , 'ABPSYS_MEAN_31'

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA

RF_cols_of_interest = ['ABPSYS_MEAN_10','ABPSYS_MEAN_20' , 'ABPSYS_MEAN_31'];
               

data = df_features[RF_cols_of_interest]
scaled_data = MinMaxScaler().fit_transform(data)


#plotting PCA components
pca = PCA(n_components=3)
principalComponents = pca.fit_transform(scaled_data)
# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='black')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)

# Save components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)
df_features['PCA_1'] = PCA_components[0]
df_features['PCA_2'] = PCA_components[1]

print(df_features.columns)

from sklearn.metrics import accuracy_score

import seaborn as sns
import sklearn
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn import metrics
import matplotlib.pyplot as plt

k_means = KMeans(n_clusters=2, max_iter=100)
#Run the clustering algorithm
model = k_means.fit(PCA_components.iloc[:,:1])
#Generate cluster predictions and store in y_hat
#y_hat = k_means.predict(PCA_components.iloc[:,:2])
df_features['CLUSTER'] = k_means.labels_

print(df_features.columns)


print('Printing metircs for 2 clusters')
print(metrics.silhouette_score(scaled_data,  k_means.labels_, metric = 'euclidean'))
print(metrics.calinski_harabasz_score(scaled_data,   k_means.labels_))#
hom = metrics.homogeneity_score(df_features.HAS_SHOCK, df_features.CLUSTER)
print(hom)

com = metrics.completeness_score(df_features.HAS_SHOCK, df_features.CLUSTER)
print(com)

v_meas = metrics.v_measure_score(df_features.HAS_SHOCK, df_features.CLUSTER)
print(v_meas)

y_true = df_features['HAS_SHOCK']
y_true = y_true.apply(pd.to_numeric).to_numpy()

y_pred = df_features['CLUSTER']
y_pred = y_pred.apply(pd.to_numeric).to_numpy()

precision = metrics.precision_score(y_true, y_pred)
print(precision)

accuracy = metrics.accuracy_score(y_true, y_pred)
print(accuracy)

"""
# plotting clusters
pl.figure('K-means with 3 clusters')
pl.scatter(PCA_components.iloc[:,0] , PCA_components.iloc[:,1] , c=k_means.labels_)
pl.show()
"""
x_axis = df_features['PCA_1']
y_axis = df_features['PCA_2']
plt.figure(figsize=(8,8))
sns.scatterplot(x_axis , y_axis, hue= df_features['CLUSTER'])
plt.title('Clusters by PCA components')
plt.show()


print(df_features[df_features['SEPSIS_SHOCK_ONSETTIME'].notna()].groupby('CLUSTER').count()) #shock patients :cluster 1
print(df_features[df_features['SEPSIS_SHOCK_ONSETTIME'].isna()].groupby('CLUSTER').count()) # non- shock patients : cluster 0

# post clustering, building classification models

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MinMaxScaler

print(df_features.columns)
cols_of_interest = ['HR_MEAN_10','HR_MEAN_20','HR_MEAN_31',
               'RESP_MEAN_10','RESP_MEAN_20','RESP_MEAN_31',
               'ABPSYS_MEAN_10' ,'ABPSYS_MEAN_20','ABPSYS_MEAN_31',
               'ABPDIAS_MEAN_10','ABPDIAS_MEAN_20','ABPDIAS_MEAN_31',
               'ABPMEAN_MEAN_10','ABPMEAN_MEAN_20','ABPMEAN_MEAN_31',
               'SPO2_MEAN_10','SPO2_MEAN_20','SPO2_MEAN_31',
               'TEMP_MEAN_10','TEMP_MEAN_20','TEMP_MEAN_31' ];

feat_labels = ['HR_MEAN_10','HR_MEAN_20','HR_MEAN_31',
               'RESP_MEAN_10','RESP_MEAN_20','RESP_MEAN_31',
               'ABPSYS_MEAN_10' ,'ABPSYS_MEAN_20','ABPSYS_MEAN_31',
               'ABPDIAS_MEAN_10','ABPDIAS_MEAN_20','ABPDIAS_MEAN_31',
               'ABPMEAN_MEAN_10','ABPMEAN_MEAN_20','ABPMEAN_MEAN_31',
               'SPO2_MEAN_10','SPO2_MEAN_20','SPO2_MEAN_31',
               'TEMP_MEAN_10','TEMP_MEAN_20','TEMP_MEAN_31' ]

X = df_features[cols_of_interest]

X = X.apply(pd.to_numeric)
X = MinMaxScaler().fit_transform(X)

y= df_features['CLUSTER']
y = y.apply(pd.to_numeric).to_numpy()

y1 = df_features['HAS_SHOCK']
y1 = y1.apply(pd.to_numeric).to_numpy()

#print(X)
#print(y)

#splitting data into test and train
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

X1_train, X1_test, y1_train, y1_test = train_test_split(X, y1, test_size=0.3, random_state=0)


# Create a random forest classifier
clf = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)

# Train the classifier
clf.fit(X_train, y_train)

# Print the name and gini importance of each feature
for feature in zip(feat_labels, clf.feature_importances_):
    print(feature)

import plotly.graph_objects as go
names = feat_labels
fig = go.Figure(data=[go.Bar(
            x=feat_labels, y=clf.feature_importances_,
            text=clf.feature_importances_,
            textposition='auto',)])

fig.show()

# Create a selector object that will use the random forest classifier to identify
# features that have an importance of more than 0.15
sfm = SelectFromModel(clf, threshold=0.1)

# Train the selector
sfm.fit(X_train, y_train)

# Print the names of the most important features
for feature_list_index in sfm.get_support(indices=True):
    print(feat_labels[feature_list_index])

# Transform the data to create a new dataset containing only the most important features
# Note: We have to apply the transform to both the training X and test X data.
X_important_train = sfm.transform(X_train)
X_important_test = sfm.transform(X1_test)

# Create a new random forest classifier for the most important features
clf_important = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)

# Train the new classifier on the new dataset containing the most important features
clf_important.fit(X_important_train, y1_train)

# Apply The Full Featured Classifier To The Test Data
y_pred = clf.predict(X1_test)
# View The Accuracy Of Our Full Feature (4 Features) Model
accuracy_score(y1_test, y_pred)

##### TODO
# TO COMPARE WITH THE ORIGINAL HAS_SHOCK LABEL AND NOT THE ASSIGNED CLUSTER.1

"""LETS TRY SOM CLASSIFICATION

"""

!pip install minisom
from minisom import MiniSom
import numpy as np
import pandas as pd


# Initialization and training
som_shape = (1, 2)
som = MiniSom(som_shape[0], som_shape[1], data.shape[1], sigma=.5, learning_rate=.6,
              neighborhood_function='gaussian', random_seed=50)

som.train_batch(scaled_data, 500, verbose=True)

# each neuron represents a cluster
winner_coordinates = np.array([som.winner(x) for x in scaled_data]).T
# with np.ravel_multi_index we convert the bidimensional
# coordinates to a monodimensional index
cluster_index = np.ravel_multi_index(winner_coordinates, som_shape)
print(cluster_index)
df_features['CLUSTER_SOM'] = cluster_index

hom = metrics.homogeneity_score(df_features.HAS_SHOCK, df_features.CLUSTER_SOM)
print(hom)

com = metrics.completeness_score(df_features.HAS_SHOCK, df_features.CLUSTER_SOM)
print(com)

v_meas = metrics.v_measure_score(df_features.HAS_SHOCK, df_features.CLUSTER_SOM)
print(v_meas)

fow_mal = metrics.fowlkes_mallows_score(df_features.HAS_SHOCK, df_features.CLUSTER_SOM)
print(fow_mal)


y_true = df_features['HAS_SHOCK']
y_true = y_true.apply(pd.to_numeric).to_numpy()

y_pred = df_features['CLUSTER']
y_pred = y_pred.apply(pd.to_numeric).to_numpy()

precision = metrics.precision_score(y_true, y_pred)
print(precision)

accuracy = metrics.accuracy_score(y_true, y_pred)
print(accuracy)