# -*- coding: utf-8 -*-
"""30_256SepsisOnset_ShockOnSet_OR_SepsisOnsetPlus31h_PatientsLessThan20%Missing_DataCleaning+MissingValuesImputation+experiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pNk5UWGisf1OgQEb7bY_Bw-LDSMVe-Yp
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
import pandas as pd
import io
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
import os
import shutil
import posixpath
import urllib.request
import datetime
from collections import namedtuple

drive.mount('/content/gdrive')

# Download All Time series data for 404 patients.
df_all256_withTemp = pd.read_csv('/content/gdrive/My Drive/Master thesis/df_ts_records_all256SepsisPatients(lessThan20%Missing)_fromSepsisOnset_ShockOnset_or_sepsisOnsetPlus31h_WITH_TEMP_SOFA.csv')
df_all256_withTemp['TIME'] =  pd.to_datetime(df_all256_withTemp['TIME'])
print(df_all256_withTemp.shape)

#cleaning data :  removing all negative values

df_all256_withTemp.HR = df_all256_withTemp.HR.mask(df_all256_withTemp.HR < 0)

df_all256_withTemp.RESP = df_all256_withTemp.RESP.mask(df_all256_withTemp.RESP < 0)

df_all256_withTemp.ABPSYS = df_all256_withTemp.ABPSYS.mask(df_all256_withTemp.ABPSYS < 0)

df_all256_withTemp.ABPDIAS = df_all256_withTemp.ABPDIAS.mask(df_all256_withTemp.ABPDIAS < 0)

df_all256_withTemp.ABPMEAN = df_all256_withTemp.ABPMEAN.mask(df_all256_withTemp.ABPMEAN < 0)

df_all256_withTemp.SPO2 = df_all256_withTemp.SPO2.mask(df_all256_withTemp.SPO2 < 0)

df_all256_withTemp.TEMP = df_all256_withTemp.TEMP.mask(df_all256_withTemp.TEMP < 0)


df_all256_withTemp.SOFA_SCORE = df_all256_withTemp.SOFA_SCORE.mask(df_all256_withTemp.SOFA_SCORE < 0)


df_all256_withTemp.RESP_SOFA = df_all256_withTemp.RESP_SOFA.mask(df_all256_withTemp.RESP_SOFA < 0)


df_all256_withTemp.LIVER_SOFA = df_all256_withTemp.LIVER_SOFA.mask(df_all256_withTemp.LIVER_SOFA < 0)


df_all256_withTemp.RENAL_SOFA = df_all256_withTemp.RENAL_SOFA.mask(df_all256_withTemp.RENAL_SOFA < 0)


df_all256_withTemp.CARDIO_SOFA = df_all256_withTemp.CARDIO_SOFA.mask(df_all256_withTemp.CARDIO_SOFA < 0)


df_all256_withTemp.CNS_SOFA = df_all256_withTemp.CNS_SOFA.mask(df_all256_withTemp.CNS_SOFA < 0)


df_all256_withTemp.COAG_SOFA = df_all256_withTemp.COAG_SOFA.mask(df_all256_withTemp.COAG_SOFA < 0)

# Missing value imputation by carry forward scheme
df_all256_withTemp_cleaned_MVimputed = df_all256_withTemp.ffill().bfill()

df_all256_withTemp_cleaned_MVimputed.HR = df_all256_withTemp_cleaned_MVimputed.HR.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.RESP = df_all256_withTemp_cleaned_MVimputed.RESP.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.ABPSYS = df_all256_withTemp_cleaned_MVimputed.ABPSYS.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.ABPDIAS = df_all256_withTemp_cleaned_MVimputed.ABPDIAS.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.ABPMEAN = df_all256_withTemp_cleaned_MVimputed.ABPMEAN.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.SPO2 = df_all256_withTemp_cleaned_MVimputed.SPO2.round(decimals=4)
df_all256_withTemp_cleaned_MVimputed.TEMP = df_all256_withTemp_cleaned_MVimputed.TEMP.round(decimals=4)


df_all256_withTemp_cleaned_MVimputed.SOFA_SCORE = df_all256_withTemp_cleaned_MVimputed.SOFA_SCORE.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.RENAL_SOFA = df_all256_withTemp_cleaned_MVimputed.RENAL_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.LIVER_SOFA = df_all256_withTemp_cleaned_MVimputed.LIVER_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.COAG_SOFA = df_all256_withTemp_cleaned_MVimputed.COAG_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.CARDIO_SOFA = df_all256_withTemp_cleaned_MVimputed.CARDIO_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.CNS_SOFA = df_all256_withTemp_cleaned_MVimputed.CNS_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed.RESP_SOFA = df_all256_withTemp_cleaned_MVimputed.RESP_SOFA.round(decimals=4)

df_all256_withTemp_cleaned_MVimputed['TIME'] =  pd.to_datetime(df_all256_withTemp_cleaned_MVimputed['TIME'])

# Check if any null values exists in the final dataframe after cleaning and imputing missing values.
print(df_all256_withTemp_cleaned_MVimputed[df_all256_withTemp_cleaned_MVimputed.isnull().any(axis=1)])

import plotly.graph_objects as go
fig = go.Figure()
df_single_subject = df_all256_withTemp_cleaned_MVimputed[df_all256_withTemp_cleaned_MVimputed['SUBJECT_ID']==69272] # 98253
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,4], name = df_single_subject.iloc[:,4].name, line = dict(color = '#17BECF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,5], name = df_single_subject.iloc[:,5].name, line = dict(color = '#CF1717'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,6], name = df_single_subject.iloc[:,6].name, line = dict(color = '#AACF17'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,7], name = df_single_subject.iloc[:,7].name, line = dict(color = '#17CF29'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,8], name = df_single_subject.iloc[:,8].name, line = dict(color = '#1742CF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,9], name = df_single_subject.iloc[:,9].name, line = dict(color = '#B017CF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,10], name = df_single_subject.iloc[:,10].name, line = dict(color = '#CFA417'), opacity = 0.8))
fig.update_layout(title_text=df_single_subject.iloc[:,4].name+', '+ df_single_subject.iloc[:,5].name+', '+df_single_subject.iloc[:,6].name+', '+df_single_subject.iloc[:,7].name+', '+df_single_subject.iloc[:,8].name+', '+
                 df_single_subject.iloc[:,9].name+', '+df_single_subject.iloc[:,10].name)

fig.show()

import plotly.graph_objects as go
fig = go.Figure()
df_single_subject = df_all256_withTemp_cleaned_MVimputed[df_all256_withTemp_cleaned_MVimputed['SUBJECT_ID']==98253] # 
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,4], name = df_single_subject.iloc[:,4].name, line = dict(color = '#17BECF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,5], name = df_single_subject.iloc[:,5].name, line = dict(color = '#CF1717'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,6], name = df_single_subject.iloc[:,6].name, line = dict(color = '#AACF17'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,7], name = df_single_subject.iloc[:,7].name, line = dict(color = '#17CF29'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,8], name = df_single_subject.iloc[:,8].name, line = dict(color = '#1742CF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,9], name = df_single_subject.iloc[:,9].name, line = dict(color = '#B017CF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=df_single_subject.TIME, y=df_single_subject.iloc[:,10], name = df_single_subject.iloc[:,10].name, line = dict(color = '#CFA417'), opacity = 0.8))
fig.update_layout(title_text=df_single_subject.iloc[:,4].name+', '+ df_single_subject.iloc[:,5].name+', '+df_single_subject.iloc[:,6].name+', '+df_single_subject.iloc[:,7].name+', '+df_single_subject.iloc[:,8].name+', '+
                 df_single_subject.iloc[:,9].name+', '+df_single_subject.iloc[:,10].name)

fig.show()

import seaborn as sns
data_corr = df_all256_withTemp_cleaned_MVimputed[['HR', 'SPO2', 'ABPSYS', 'ABPDIAS', 'ABPMEAN', 'RESP', 'TEMP','SOFA_SCORE']]
print(data_corr)
corr = data_corr.corr()# calculating the correlation between the above vital signs
sns.heatmap(corr, square=True) # plotting the correlation

subject_ids = df_all256_withTemp_cleaned_MVimputed.SUBJECT_ID.unique()
print((subject_ids))

from google.colab import files
uploaded = files.upload()

df_icutime = pd.read_csv(io.BytesIO(uploaded['Only_AllSepsisPatients_with_MissingData_FromSepsisOnset_ToShockOnset_or_SepsisOnset+31h.csv']))
print (df_icutime.columns)
#df_icutime = df_icutime[['subject_id','icustay_id','intime','outtime','sepsis_onsettime']]
#print (df_icutime.columns)
"""
df_icutime['intime'] =  pd.to_datetime(df_icutime['intime'])
df_icutime['outtime'] =  pd.to_datetime(df_icutime['outtime'])
"""

"""# **steps for post GLM classification**"""

from sklearn import metrics
from matplotlib import pyplot
from numpy import sqrt
from numpy import argmax
from sklearn.metrics import precision_recall_curve


fpr, tpr, thresholds = metrics.roc_curve(y_test, xg_preds)
pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')
pyplot.plot(fpr, tpr, marker='.', label='Logistic')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
pyplot.legend()
# show the plot
pyplot.show()
gmeans = sqrt(tpr * (1-fpr))
ix = argmax(gmeans)
print('Best Threshold according to the G-Mean=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))

########## calculating best threshold according to the best F1-score:

precision, recall, thresholds = precision_recall_curve(y_test, xg_preds)
# convert to f score
fscore = (2 * precision * recall) / (precision + recall)
# locate the index of the largest f score
ix = argmax(fscore)
print('Best Threshold according to the F-Score=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))


########## calculating best threshold according to the  Youden’s J statistic.:
fpr, tpr, thresholds = metrics.roc_curve(y_test, xg_preds)
J = tpr - fpr
ix = argmax(J)
best_thresh = thresholds[ix]
print('Best Threshold according to the Youden’s J statistic=%f' % (best_thresh))

from sklearn.metrics import confusion_matrix, classification_report
y_pred = [ 0 if x < 0.38 else 1 for x in xg_preds]
print(classification_report(y_test, 
                            y_pred, 
                            digits = 3))
print(y_test)
print(y_pred)

from sklearn import metrics
import seaborn as sns
cnf_matrix = metrics.confusion_matrix(y_test, y_pred)

class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')


print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("Precision:",metrics.precision_score(y_test, y_pred))
print("Recall:",metrics.recall_score(y_test, y_pred))
print("F-Score:",metrics.f1_score(y_test, y_pred))

"""# **Restart the session to test with more different features**"""

"""
feature_cols= ['SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK',
               'HR_MEAN',
               'RESP_MEAN',
               'ABPSYS_MEAN',
               'ABPDIAS_MEAN',
               'ABPMEAN_MEAN',
               'SPO2_MEAN',
               'TEMP_MEAN' ,
               'SOFA_MEAN',
               'HR_VAR',
               'RESP_VAR',
               'SPO2_VAR',
               'TEMP_VAR'] 
"""               

feature_cols= ['SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK',
               'HR_MEAN',
               'RESP_MEAN',
               'ABPSYS_MEAN',
               'ABPDIAS_MEAN',
               'ABPMEAN_MEAN',
               'SPO2_MEAN',
               'TEMP_MEAN' ,
               'SOFA_MEAN',
               'HR_VAR',
               'RESP_VAR',
               'SPO2_VAR',
               'ABPSYS_VAR',
               'ABPDIAS_VAR',
               'ABPMEAN_VAR',
               'TEMP_VAR',
               'HR_MIN',
               'HR_MAX',
               'RESP_MIN',
               'RESP_MAX',
               'SPO2_MIN',
               'SPO2_MAX',
               'TEMP_MIN',
               'TEMP_MAX',
               'ABPSYS_MIN',
               'ABPSYS_MAX',
               'ABPDIAS_MIN',
               'ABPDIAS_MAX',
               'ABPMEAN_MIN',
               'ABPMEAN_MAX'] 


"""
feature_cols= ['SUBJECT_ID','SEPSIS_ONSETTIME','SEPSIS_SHOCK_ONSETTIME','HAS_SHOCK',
               'HR_MEAN',
               'RESP_MEAN',
               'ABPSYS_MEAN',
               'ABPDIAS_MEAN',
               'ABPMEAN_MEAN',
               'SPO2_MEAN',
               'TEMP_MEAN' ,
               'SOFA_MEAN',
               'HR_MIN',
               'HR_MAX',
               'RESP_MIN',
               'RESP_MAX',
               'SPO2_MIN',
               'SPO2_MAX',
               'TEMP_MIN',
               'TEMP_MAX'] 
"""
try:
  df_features.drop(df_features.index, inplace=True)
except:
  print('df_features does not exists')

df_features =pd.DataFrame(columns=feature_cols);

for subject_id in subject_ids:
  curr_idx = df_features.shape[0]
  #df_icuintime_subjectid = df_icutime[df_icutime['subject_id'] == subject_id] 

  icu_intime = df_icutime.loc[df_icutime.subject_id==subject_id , 'intime'].values[0]
  sepsis_onsettime = df_icutime.loc[df_icutime.subject_id==subject_id , 'sepsis_onsettime'].values[0]
  shock_onsetttime = df_icutime.loc[df_icutime.subject_id==subject_id , 'sepstic_shock_onsettime'].values[0]
  #print(shock_onsetttime)
  
  if str(shock_onsetttime) != 'nan':
    df_tsdata_subjectid_first_1h = df_all256_withTemp_cleaned_MVimputed[(df_all256_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( ( df_all256_withTemp_cleaned_MVimputed['TIME'] >= (datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S') - datetime.timedelta(hours=8))   ) & ( df_all256_withTemp_cleaned_MVimputed['TIME'] <= (datetime.datetime.strptime(shock_onsetttime,'%Y-%m-%d %H:%M:%S') - datetime.timedelta(hours=2)) )  )]
  else:
    df_tsdata_subjectid_first_1h = df_all256_withTemp_cleaned_MVimputed[(df_all256_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( ( df_all256_withTemp_cleaned_MVimputed['TIME'] >= (datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=23))    ) & ( df_all256_withTemp_cleaned_MVimputed['TIME'] <= (datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=29)) )  )]
  
  if df_tsdata_subjectid_first_1h.shape[0]!=0 :
    #below data selection  from sepsis onset till sepsis onset + 1 hour
    #df_tsdata_subjectid_first_1h = df_all256_withTemp_cleaned_MVimputed[(df_all256_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( ( df_all256_withTemp_cleaned_MVimputed['TIME'] >= datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S')    ) & ( df_all256_withTemp_cleaned_MVimputed['TIME'] <= (datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=1)) )  )]
    


    #df_tsdata_subjectid_first_20h = df_all251_withTemp_cleaned_MVimputed[(df_all251_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( ( df_all251_withTemp_cleaned_MVimputed['TIME'] >= datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S')    ) & ( df_all251_withTemp_cleaned_MVimputed['TIME'] <= (datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=20)) )  )]

    #df_tsdata_subjectid_entire_31h = df_all251_withTemp_cleaned_MVimputed[(df_all251_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) & ( ( df_all251_withTemp_cleaned_MVimputed['TIME'] >= datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S')    ) & ( df_all251_withTemp_cleaned_MVimputed['TIME'] <= (datetime.datetime.strptime(sepsis_onsettime,'%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=31)) )  )]
    
    #data_test = df_all404_withTemp_cleaned_MVimputed[(df_all404_withTemp_cleaned_MVimputed['SUBJECT_ID'] == subject_id) ]
    #print(data_test[['TIME','HR','RESP','ABPSYS','TEMP']]) # 


    df_features.loc[curr_idx,'SUBJECT_ID'] = subject_id
    df_features.loc[curr_idx,'SEPSIS_ONSETTIME'] = sepsis_onsettime
    df_features.loc[curr_idx,'SEPSIS_SHOCK_ONSETTIME'] = shock_onsetttime

    if str(shock_onsetttime) != 'nan':
      df_features.loc[curr_idx,'HAS_SHOCK'] = 1
    else:
      df_features.loc[curr_idx,'HAS_SHOCK'] = 0

    # CALCULATING MEANS for first 10 hours of data i.e. from sepsis onset time till sepsis onset time + 10 h
    df_features.loc[curr_idx,'HR_MEAN'] = df_tsdata_subjectid_first_1h['HR'].mean()
    df_features.loc[curr_idx,'RESP_MEAN'] = df_tsdata_subjectid_first_1h['RESP'].mean()
    df_features.loc[curr_idx,'ABPSYS_MEAN'] = df_tsdata_subjectid_first_1h['ABPSYS'].mean()
    df_features.loc[curr_idx,'ABPDIAS_MEAN'] = df_tsdata_subjectid_first_1h['ABPDIAS'].mean()
    df_features.loc[curr_idx,'ABPMEAN_MEAN'] = df_tsdata_subjectid_first_1h['ABPMEAN'].mean()
    df_features.loc[curr_idx,'SPO2_MEAN'] = df_tsdata_subjectid_first_1h['SPO2'].mean()
    df_features.loc[curr_idx,'TEMP_MEAN'] = df_tsdata_subjectid_first_1h['TEMP'].mean()
    df_features.loc[curr_idx,'SOFA_MEAN'] = df_tsdata_subjectid_first_1h['SOFA_SCORE'].mean()
    
    df_features.loc[curr_idx,'HR_VAR'] = df_tsdata_subjectid_first_1h['HR'].var()
    df_features.loc[curr_idx,'RESP_VAR'] = df_tsdata_subjectid_first_1h['RESP'].var()
    df_features.loc[curr_idx,'SPO2_VAR'] = df_tsdata_subjectid_first_1h['SPO2'].var()
    df_features.loc[curr_idx,'TEMP_VAR'] = df_tsdata_subjectid_first_1h['TEMP'].var()
    df_features.loc[curr_idx,'ABPSYS_VAR'] = df_tsdata_subjectid_first_1h['ABPSYS'].var()
    df_features.loc[curr_idx,'ABPDIAS_VAR'] = df_tsdata_subjectid_first_1h['ABPDIAS'].var()
    df_features.loc[curr_idx,'ABPMEAN_VAR'] = df_tsdata_subjectid_first_1h['ABPMEAN'].var()
    
    
    df_features.loc[curr_idx,'HR_MIN'] = df_tsdata_subjectid_first_1h['HR'].min()
    df_features.loc[curr_idx,'RESP_MIN'] = df_tsdata_subjectid_first_1h['RESP'].min()
    df_features.loc[curr_idx,'SPO2_MIN'] = df_tsdata_subjectid_first_1h['SPO2'].min()
    df_features.loc[curr_idx,'TEMP_MIN'] = df_tsdata_subjectid_first_1h['TEMP'].min()
    df_features.loc[curr_idx,'ABPSYS_MIN'] = df_tsdata_subjectid_first_1h['ABPSYS'].min()
    df_features.loc[curr_idx,'ABPDIAS_MIN'] = df_tsdata_subjectid_first_1h['ABPDIAS'].min()
    df_features.loc[curr_idx,'ABPMEAN_MIN'] = df_tsdata_subjectid_first_1h['ABPMEAN'].min()


    df_features.loc[curr_idx,'HR_MAX'] = df_tsdata_subjectid_first_1h['HR'].max()
    df_features.loc[curr_idx,'RESP_MAX'] = df_tsdata_subjectid_first_1h['RESP'].max()
    df_features.loc[curr_idx,'SPO2_MAX'] = df_tsdata_subjectid_first_1h['SPO2'].max()
    df_features.loc[curr_idx,'TEMP_MAX'] = df_tsdata_subjectid_first_1h['TEMP'].max()
    df_features.loc[curr_idx,'ABPSYS_MAX'] = df_tsdata_subjectid_first_1h['ABPSYS'].max()
    df_features.loc[curr_idx,'ABPDIAS_MAX'] = df_tsdata_subjectid_first_1h['ABPDIAS'].max()
    df_features.loc[curr_idx,'ABPMEAN_MAX'] = df_tsdata_subjectid_first_1h['ABPMEAN'].max()
    

    icu_intime='';
    sepsis_onsettime='';
    shock_onsetttime = '';

    df_tsdata_subjectid_first_1h.drop(df_tsdata_subjectid_first_1h.index, inplace = True)
    #df_tsdata_subjectid_first_10h.drop(df_tsdata_subjectid_first_10h.index, inplace=True)
    #df_tsdata_subjectid_first_20h.drop(df_tsdata_subjectid_first_20h.index, inplace=True) 
    #df_tsdata_subjectid_entire_31h.drop(df_tsdata_subjectid_entire_31h.index, inplace=True)  
    #print(df_features)

"""# **PLOTTING HEATMMAP FOPR ALL FEATURES IE. MEAN, VAR, MIN, MAX**"""

import seaborn as sns
data_corr = df_features[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'SOFA_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]

data_corr = data_corr.apply(pd.to_numeric)

#print(data_corr)
corr = data_corr.corr()# calculating the correlation between the above vital signs

sns.heatmap(corr,xticklabels=data_corr.columns, yticklabels=data_corr.columns) # plotting the correlation

"""# **DOING CORRELATION ANALYSIS TO FIND THE TOP FEATURES TO BE USED FOR REGRESSION**"""

# example of correlation feature selection for numerical data
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression
from matplotlib import pyplot

x_feat_imp = df_features[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]

x_feat_imp = x_feat_imp.apply(pd.to_numeric)

y_feat_imp = df_features['SOFA_MEAN']
y_feat_imp = y_feat_imp.apply(pd.to_numeric)

fs = SelectKBest(score_func=f_regression, k='all')
fs.fit(x_feat_imp, y_feat_imp)
for i in range(len(fs.scores_)):
	print('Feature %d: %f' % (i, fs.scores_[i]))
# plot the scores
pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)
pyplot.show()


"""
TOP 5: 
ABPSYS_MEAN + ABPDIAS_MEAN + ABPMEAN_MEAN +RESP_VAR +ABPMEAN_MIN 

TOP 7:
ABPSYS_MEAN + ABPDIAS_MEAN + ABPMEAN_MEAN + RESP_VAR + ABPMEAN_MIN  + SPO2_MEAN + ABPDIAS_MIN

'HR_MEAN', # 1
 'RESP_MEAN', # 2

 'ABPSYS_MEAN',# 3
  'ABPDIAS_MEAN',# 4
   'ABPMEAN_MEAN',# 5

      'SPO2_MEAN',# 6
       'TEMP_MEAN', # 7
       'HR_VAR', # 8

       'RESP_VAR', # 9

       'SPO2_VAR', # 10
       'ABPSYS_VAR', # 11
        'ABPDIAS_VAR', # 12
        'ABPMEAN_VAR',# 13
         'TEMP_VAR',# 14
          'HR_MIN',# 15
       'HR_MAX', # 16
       'RESP_MIN', # 17
       'RESP_MAX', # 18
       'SPO2_MIN', # 19
       'SPO2_MAX', # 20
       'TEMP_MIN',# 21
       'TEMP_MAX', # 22
       'ABPSYS_MIN', # 23
       'ABPSYS_MAX',# 24
       
        'ABPDIAS_MIN', # 25
        'ABPDIAS_MAX',# 26
       'ABPMEAN_MIN', # 27
       'ABPMEAN_MAX'# 28

"""

"""# **Trying mmutual informaiton gain for feature selection**"""

# example of correlation feature selection for numerical data
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression, mutual_info_regression
from matplotlib import pyplot

x_feat_imp = df_features[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]

x_feat_imp = x_feat_imp.apply(pd.to_numeric)

y_feat_imp = df_features['SOFA_MEAN']
y_feat_imp = y_feat_imp.apply(pd.to_numeric)


"""
x_feat_train, x_feat_test, y_feat_train, y_feat_test = train_test_split(x_feat_imp, y_feat_imp, test_size=0.3,random_state=42)

def select_features(x_feat_train, y_feat_train, x_feat_test):
	# configure to select all features
	fs = SelectKBest(score_func=mutual_info_regression, k='all')
	# learn relationship from training data

	fs.fit(x_feat_train, y_feat_train)
	
	return x_feat_train, x_feat_test, fs


x_feat_train_fs, x_feat_test_fs, fs = select_features(x_feat_train, y_feat_train, x_feat_test)
# what are scores for the features

for i in range(len(fs.scores_)):
	print('Feature %d: %f' % (i, fs.scores_[i]))
# plot the scores
pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)
pyplot.show()
"""
fs = SelectKBest(score_func=mutual_info_regression, k='all')
fs.fit(x_feat_imp, y_feat_imp)
for i in range(len(fs.scores_)):
	print('Feature %d: %f' % (i, fs.scores_[i]))
# plot the scores
pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)
pyplot.show()
"""
#top 6 features:

RESP_MEAN + ABPMEAN_MEAN + TEMP_MEAN + SPO2_MIN + ABPDIAS_MIN +ABPDIAS_MAX

'HR_MEAN', # 1
 'RESP_MEAN', # 2

 'ABPSYS_MEAN',# 3
  'ABPDIAS_MEAN',# 4
   'ABPMEAN_MEAN',# 5

      'SPO2_MEAN',# 6
       'TEMP_MEAN', # 7
       'HR_VAR', # 8

       'RESP_VAR', # 9

       'SPO2_VAR', # 10
       'ABPSYS_VAR', # 11
        'ABPDIAS_VAR', # 12
        'ABPMEAN_VAR',# 13
         'TEMP_VAR',# 14
          'HR_MIN',# 15
       'HR_MAX', # 16
       'RESP_MIN', # 17
       'RESP_MAX', # 18
       'SPO2_MIN', # 19
       'SPO2_MAX', # 20
       'TEMP_MIN',# 21
       'TEMP_MAX', # 22
       'ABPSYS_MIN', # 23
       'ABPSYS_MAX',# 24
       
        'ABPDIAS_MIN', # 25
        'ABPDIAS_MAX',# 26
       'ABPMEAN_MIN', # 27
       'ABPMEAN_MAX'# 28

"""

"""# **CHECKING FEATURE IMPORTANCE USING ALL +VE COEFFICIENTS for a GLM Model**"""

from sklearn.model_selection import train_test_split
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.metrics import mean_squared_error


x = df_features[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'SOFA_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]
    
x = x.apply(pd.to_numeric)

y = df_features['HAS_SHOCK']


glm = smf.glm(formula='SOFA_MEAN ~ HR_MEAN + RESP_MEAN + ABPSYS_MEAN + ABPDIAS_MEAN + ABPMEAN_MEAN + SPO2_MEAN + TEMP_MEAN + HR_VAR + RESP_VAR + SPO2_VAR + TEMP_VAR + ABPSYS_VAR + ABPDIAS_VAR + ABPMEAN_VAR+ HR_MIN +HR_MAX + RESP_MIN + RESP_MAX + SPO2_MIN + SPO2_MAX + TEMP_MIN + TEMP_MAX + ABPSYS_MIN + ABPSYS_MAX + ABPDIAS_MIN + ABPDIAS_MAX + ABPMEAN_MIN + ABPMEAN_MAX ', data = x, family = sm.families.Poisson())


glm_regression_model = glm.fit()

print(glm_regression_model.summary())


# all features with +ve coefficients:
#HR_MEAN + RESP_MEAN+ RESP_VAR + ABPDIAS_VAR + ABPMEAN_VAR + HR_MAX + SPO2_MIN + SPO2_MAX + TEMP_MAX + ABPSYS_MIN + ABPMEAN_MIN

"""# **trying xgboost regressor feature immportance**"""

# xgboost for feature importance on a regression problem
from sklearn.datasets import make_regression
from xgboost import XGBRegressor
from matplotlib import pyplot


X = df_features[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]

X = X.apply(pd.to_numeric)

y = df_features['SOFA_MEAN']
y = y.apply(pd.to_numeric)

# define the model
model = XGBRegressor(objecttive='reg:linear')
# fit the model
model.fit(X, y)
# get importance
importance = model.feature_importances_
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
pyplot.bar([x for x in range(len(importance))], importance)
pyplot.show()

"""
#top 5 features:

ABPSYS_MEAN , RESP_VAR , TEMP_VAR , ABPSYS_MIN , ABPMEAN_MIN

#top 5+8 = 13 features:
ABPSYS_MEAN , RESP_VAR , TEMP_VAR , ABPSYS_MIN , ABPMEAN_MIN,
RESP_MEAN ,HR_VAR, ABPMEAN_VAR, RESP_MIN, SPO2_MIN,  ABPSYS_MAX, ABPDIAS_MIN, ABPDIAS_MAX



'HR_MEAN', # 1
 'RESP_MEAN', # 2

 'ABPSYS_MEAN',# 3
  'ABPDIAS_MEAN',# 4
   'ABPMEAN_MEAN',# 5

      'SPO2_MEAN',# 6
       'TEMP_MEAN', # 7
       'HR_VAR', # 8

       'RESP_VAR', # 9

       'SPO2_VAR', # 10
       'ABPSYS_VAR', # 11
        'ABPDIAS_VAR', # 12
        'ABPMEAN_VAR',# 13
         'TEMP_VAR',# 14
          'HR_MIN',# 15
       'HR_MAX', # 16
       'RESP_MIN', # 17
       'RESP_MAX', # 18
       'SPO2_MIN', # 19
       'SPO2_MAX', # 20
       'TEMP_MIN',# 21
       'TEMP_MAX', # 22
       'ABPSYS_MIN', # 23
       'ABPSYS_MAX',# 24
       
        'ABPDIAS_MIN', # 25
        'ABPDIAS_MAX',# 26
       'ABPMEAN_MIN', # 27
       'ABPMEAN_MAX'# 28

"""

from sklearn.model_selection import train_test_split
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor


"""
For top 5 features  : 'ABPSYS_MEAN', 'RESP_VAR',  'TEMP_VAR', 'ABPSYS_MIN' ,'ABPMEAN_MIN'
ROOT MEAN SQUARE ERROR :  1.1458710519554571
RSS =  101.10257601363195
R2 =  -0.10808386560390293

"""

"""
for top 13 features : 'ABPSYS_MEAN', 'RESP_VAR',  'TEMP_VAR', 'ABPSYS_MIN' ,'ABPMEAN_MIN', 'RESP_MEAN','HR_VAR', 'ABPMEAN_VAR', 'RESP_MIN', 'SPO2_MIN',  'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX'

ROOT MEAN SQUARE ERROR :  1.144962312506987
RSS =  100.94227967372376
R2 =  -0.10920652972127298
"""

x = df_features[['ABPSYS_MEAN', 'RESP_VAR',  'TEMP_VAR', 'ABPSYS_MIN' ,'ABPMEAN_MIN', 'RESP_MEAN','HR_VAR', 'ABPMEAN_VAR', 'RESP_MIN', 'SPO2_MIN',  'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX']]
y = df_features['SOFA_MEAN']   

x = x.apply(pd.to_numeric)

y = y.apply(pd.to_numeric)


#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=42)



model = XGBRegressor(objective='reg:linear',learning_rate = 0.2,max_depth = 5,  n_estimators = 100)
# fit the model
XGB_model = model.fit(x, y)

#testing on sofa regression 
#regression_test = x_test[['ABPSYS_MEAN', 'RESP_VAR',  'TEMP_VAR', 'ABPSYS_MIN' ,'ABPMEAN_MIN']]


predictions = XGB_model.predict(x)
#print(predictions)



rms = np.sqrt(mean_squared_error(y, predictions))
print('ROOT MEAN SQUARE ERROR : ',rms )

x['PRED_SOFA_MEAN'] = predictions

print("RSS = ", ((y - predictions)**2).sum())
#print("R2 = ", glm_regression_model.rsquared)


SS_Residual = sum((y - predictions)**2)       
SS_Total = sum((y -np.mean(predictions))**2)     
r_squared = 1 - (float(SS_Residual))/SS_Total
print("R2 = ", r_squared)

"""# **to check permuation combo with XGBOOST model**"""

# permutation feature importance with knn for regression
from sklearn.datasets import make_regression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.inspection import permutation_importance
from matplotlib import pyplot

X_KNN = df_features[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]
    
X_KNN = X_KNN.apply(pd.to_numeric)
print(X_KNN.shape)
y_KNN = df_features['SOFA_MEAN']

# define the model
model = XGBRegressor()

# fit the model
model.fit(X_KNN, y_KNN)
# perform permutation importance
results = permutation_importance(model, X_KNN, y_KNN, scoring='neg_mean_squared_error')
# get importance
importance_KNN = results.importances_mean

print(results)
# summarize feature importance
for i,v in enumerate(importance_KNN):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
pyplot.bar([x for x in range(len(importance_KNN))], importance_KNN)
pyplot.show()

#RESP_MEAN ,  ABPSYS_MEAN,  ABPMEAN_MEAN,  RESP_VAR, TEMP_VAR, ABPDIAS_MIN, ABPMEAN_MIN

"""

'HR_MEAN', # 1
 'RESP_MEAN', # 2

 'ABPSYS_MEAN',# 3
  'ABPDIAS_MEAN',# 4
   'ABPMEAN_MEAN',# 5

      'SPO2_MEAN',# 6
       'TEMP_MEAN', # 7
       'HR_VAR', # 8

       'RESP_VAR', # 9

       'SPO2_VAR', # 10
       'ABPSYS_VAR', # 11
        'ABPDIAS_VAR', # 12
        'ABPMEAN_VAR',# 13
         'TEMP_VAR',# 14
          'HR_MIN',# 15
       'HR_MAX', # 16
       'RESP_MIN', # 17
       'RESP_MAX', # 18
       'SPO2_MIN', # 19
       'SPO2_MAX', # 20
       'TEMP_MIN',# 21
       'TEMP_MAX', # 22
       'ABPSYS_MIN', # 23
       'ABPSYS_MAX',# 24
       
        'ABPDIAS_MIN', # 25
        'ABPDIAS_MAX',# 26
       'ABPMEAN_MIN', # 27
       'ABPMEAN_MAX'# 28

"""

from sklearn.model_selection import train_test_split
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.metrics import mean_squared_error

"""
For top 5 features  : 'RESP_MEAN' ,  'ABPSYS_MEAN',  'ABPMEAN_MEAN',  'RESP_VAR', 'TEMP_VAR', 'ABPDIAS_MIN', 'ABPMEAN_MIN'
ROOT MEAN SQUARE ERROR :  1.140812866793415
RSS =  100.21195777218855
R2 =  -0.10173093641567599
"""

x = df_features[['RESP_MEAN' ,  'ABPSYS_MEAN',  'ABPMEAN_MEAN',  'RESP_VAR', 'TEMP_VAR', 'ABPDIAS_MIN', 'ABPMEAN_MIN']]
y = df_features['SOFA_MEAN']   

x = x.apply(pd.to_numeric)

y = y.apply(pd.to_numeric)


#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=42)


model = XGBRegressor(objective='reg:linear',learning_rate = 0.2,max_depth = 5,  n_estimators = 100)
# fit the model
XGB_model = model.fit(x, y)

#testing on sofa regression 
#regression_test = x_test[['ABPSYS_MEAN', 'RESP_VAR',  'TEMP_VAR', 'ABPSYS_MIN' ,'ABPMEAN_MIN']]


predictions = XGB_model.predict(x)
#print(predictions)



rms = np.sqrt(mean_squared_error(y, predictions))
print('ROOT MEAN SQUARE ERROR : ',rms )

x['PRED_SOFA_MEAN'] = predictions

print("RSS = ", ((y - predictions)**2).sum())
#print("R2 = ", glm_regression_model.rsquared)


SS_Residual = sum((y - predictions)**2)       
SS_Total = sum((y -np.mean(predictions))**2)     
r_squared = 1 - (float(SS_Residual))/SS_Total
print("R2 = ", r_squared)

"""# **checking permuation with GLM Model**"""

# permutation feature importance with knn for regression
from sklearn.datasets import make_regression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.inspection import permutation_importance
from matplotlib import pyplot
"""
X_KNN = df_features[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]
    """

X_KNN = df_features[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'SOFA_MEAN','HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]

X_KNN = X_KNN.apply(pd.to_numeric)
print(X_KNN.shape)

y_KNN = df_features['SOFA_MEAN']

# define the model
#model = XGBRegressor()

glm = smf.glm(formula='SOFA_MEAN ~ HR_MEAN + RESP_MEAN + ABPSYS_MEAN + ABPDIAS_MEAN + ABPMEAN_MEAN + SPO2_MEAN + TEMP_MEAN + HR_VAR + RESP_VAR + SPO2_VAR + TEMP_VAR + ABPSYS_VAR + ABPDIAS_VAR + ABPMEAN_VAR+ HR_MIN +HR_MAX + RESP_MIN + RESP_MAX + SPO2_MIN + SPO2_MAX + TEMP_MIN + TEMP_MAX + ABPSYS_MIN + ABPSYS_MAX + ABPDIAS_MIN + ABPDIAS_MAX + ABPMEAN_MIN + ABPMEAN_MAX ', data = X_KNN, family = sm.families.Poisson())


X_KNN = df_features[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN','HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]
X_KNN = X_KNN.apply(pd.to_numeric)


# fit the model
glm.fit()
# perform permutation importance
results = permutation_importance(model, X_KNN, y_KNN, scoring='neg_mean_squared_error')
# get importance
importance_KNN = results.importances_mean
print(results)
# summarize feature importance
for i,v in enumerate(importance_KNN):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
pyplot.bar([x for x in range(len(importance_KNN))], importance_KNN)
pyplot.show()

#RESP_MEAN ,  ABPSYS_MEAN,  ABPMEAN_MEAN,  RESP_VAR, TEMP_VAR, ABPDIAS_MIN, ABPMEAN_MIN

"""

'HR_MEAN', # 1
 'RESP_MEAN', # 2

 'ABPSYS_MEAN',# 3
  'ABPDIAS_MEAN',# 4
   'ABPMEAN_MEAN',# 5

      'SPO2_MEAN',# 6
       'TEMP_MEAN', # 7
       'HR_VAR', # 8

       'RESP_VAR', # 9

       'SPO2_VAR', # 10
       'ABPSYS_VAR', # 11
        'ABPDIAS_VAR', # 12
        'ABPMEAN_VAR',# 13
         'TEMP_VAR',# 14
          'HR_MIN',# 15
       'HR_MAX', # 16
       'RESP_MIN', # 17
       'RESP_MAX', # 18
       'SPO2_MIN', # 19
       'SPO2_MAX', # 20
       'TEMP_MIN',# 21
       'TEMP_MAX', # 22
       'ABPSYS_MIN', # 23
       'ABPSYS_MAX',# 24
       
        'ABPDIAS_MIN', # 25
        'ABPDIAS_MAX',# 26
       'ABPMEAN_MIN', # 27
       'ABPMEAN_MAX'# 28

"""

from sklearn.model_selection import train_test_split
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.metrics import mean_squared_error


x = df_features[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'SOFA_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]     
""" 
x = df_features[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN','SPO2_MEAN', 'TEMP_MEAN','SOFA_MEAN',
                'HR_MIN',
               'HR_MAX',
               'RESP_MIN',
               'RESP_MAX',
               'SPO2_MIN',
               'SPO2_MAX',
               'TEMP_MIN',
               'TEMP_MAX']]
"""
x = x.apply(pd.to_numeric)

y = df_features['HAS_SHOCK']

"""
FOR TOP IMP FEATURES : 'RESP_MEAN' ,'ABPSYS_MEAN',  'ABPMEAN_MEAN',  'RESP_VAR', 'TEMP_VAR', 'ABPDIAS_MIN', 'ABPMEAN_MIN'
ROOT MEAN SQUARE ERROR :  1.0215071180086743
RSS =  80.34771299496384
R2 =  0.1161361861275032
"""
glm = smf.glm(formula='SOFA_MEAN ~  RESP_MEAN + ABPSYS_MEAN + ABPMEAN_MEAN +  RESP_VAR +  TEMP_VAR + ABPDIAS_MIN + ABPMEAN_MIN', data = x, family = sm.families.Poisson())


glm_regression_model = glm.fit()

print(glm_regression_model.summary())

#print(glm_regression_model.params)


""""

importance = glm_regression_model.params
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
pyplot.bar([x for x in range(len(importance))], importance)
pyplot.show()
"""



predictions = glm_regression_model.predict(x)
#print(predictions)


actual_sofa = x['SOFA_MEAN']

rms = np.sqrt(mean_squared_error(actual_sofa, predictions))
print('ROOT MEAN SQUARE ERROR : ',rms )
x['PRED_SOFA_MEAN'] = predictions

print("RSS = ", ((x.SOFA_MEAN - x.PRED_SOFA_MEAN)**2).sum())
#print("R2 = ", glm_regression_model.rsquared)


SS_Residual = sum((x.SOFA_MEAN - x.PRED_SOFA_MEAN)**2)       
SS_Total = sum((x.SOFA_MEAN-np.mean(x.SOFA_MEAN))**2)     
r_squared = 1 - (float(SS_Residual))/SS_Total
print("R2 = ", r_squared)

"""# **TO CHECK WHICH FEATURES ARE BEST. ---  to ignore**"""

from sklearn.model_selection import train_test_split
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.metrics import mean_squared_error

"""
x = df_features[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN','SPO2_MEAN', 'TEMP_MEAN','SOFA_MEAN','HR_VAR','RESP_VAR','SPO2_VAR','TEMP_VAR']]
"""

x = df_features[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'SOFA_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]
     
""" 
x = df_features[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN','SPO2_MEAN', 'TEMP_MEAN','SOFA_MEAN',
                'HR_MIN',
               'HR_MAX',
               'RESP_MIN',
               'RESP_MAX',
               'SPO2_MIN',
               'SPO2_MAX',
               'TEMP_MIN',
               'TEMP_MAX']]
"""
x = x.apply(pd.to_numeric)

y = df_features['HAS_SHOCK']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=42)

# building GLM Bionomial model and training it
#HR_MEAN + RESP_MEAN + ABPSYS_MEAN + ABPDIAS_MEAN + ABPMEAN_MEAN + SPO2_MEAN + TEMP_MEAN + HR_VAR + RESP_VAR + SPO2_VAR + TEMP_VAR + ABPSYS_VAR + ABPDIAS_VAR + ABPMEAN_VAR+ HR_MIN +HR_MAX + RESP_MIN + RESP_MAX + SPO2_MIN + SPO2_MAX + TEMP_MIN + TEMP_MAX + ABPSYS_MIN + ABPSYS_MAX + ABPDIAS_MIN + ABPDIAS_MAX + ABPMEAN_MIN + ABPMEAN_MAX

#best :
glm = smf.glm(formula='SOFA_MEAN ~  SPO2_VAR +  RESP_MEAN + ABPSYS_MEAN + ABPDIAS_MEAN + ABPMEAN_MEAN + SPO2_MEAN  +HR_MAX + RESP_MIN  + SPO2_MIN  + TEMP_MIN  + ABPSYS_MIN + ABPSYS_MAX + ABPDIAS_MIN + ABPDIAS_MAX + ABPMEAN_MIN + ABPMEAN_MAX', data = x_train, family = sm.families.Poisson())

"""
BELOW FOR BEST:
AFTER REMOVING spo2 max, resp max, hr_mean and adding spo2 var
ROOT MEAN SQUARE ERROR :  1.0038023794888875
RSS =  77.58667971420155
R2 =  0.14650889139562173

after removing temp_max
ROOT MEAN SQUARE ERROR :  1.0033907110004499
RSS =  77.5230547569931
R2 =  0.14720879678483045

after removing hr min
ROOT MEAN SQUARE ERROR :  1.000606804309567
RSS =  77.09347621595651
R2 =  0.151934369094053

after removing temp_ mean
ROOT MEAN SQUARE ERROR :  0.982116312030626
RSS =  74.27053867746112
R2 =  0.18298805122256334

"""
#ALL VARS : glm = smf.glm(formula='SOFA_MEAN ~ HR_MEAN + RESP_MEAN + ABPSYS_MEAN + ABPDIAS_MEAN + ABPMEAN_MEAN + SPO2_MEAN + TEMP_MEAN + HR_VAR + RESP_VAR + SPO2_VAR + TEMP_VAR + ABPSYS_VAR + ABPDIAS_VAR + ABPMEAN_VAR+ HR_MIN +HR_MAX + RESP_MIN + RESP_MAX + SPO2_MIN + SPO2_MAX + TEMP_MIN + TEMP_MAX + ABPSYS_MIN + ABPSYS_MAX + ABPDIAS_MIN + ABPDIAS_MAX + ABPMEAN_MIN + ABPMEAN_MAX ', data = x_train, family = sm.families.Poisson())



#below according to correlation ABOVE analysis :
#glm = smf.glm(formula='SOFA_MEAN ~ ABPSYS_MEAN + ABPDIAS_MEAN + ABPMEAN_MEAN + RESP_VAR + ABPMEAN_MIN  + SPO2_MEAN + ABPDIAS_MIN', data = x_train, family = sm.families.Poisson())
"""
FOR CORRELATTION ANALYSIS : 

TOP 5 FEATURES : ABPSYS_MEAN + ABPDIAS_MEAN + ABPMEAN_MEAN +RESP_VAR +ABPMEAN_MIN
ROOT MEAN SQUARE ERROR :  1.0272619302520087
RSS =  81.25556464757138
R2 =  0.10614937761492671

TOP 7 FEATURES : ABPSYS_MEAN + ABPDIAS_MEAN + ABPMEAN_MEAN + RESP_VAR + ABPMEAN_MIN  + SPO2_MEAN + ABPDIAS_MIN
ROOT MEAN SQUARE ERROR :  1.0293526022160202
RSS =  81.58664203604471
R2 =  0.1025073657597626


"""


#below according to MUTUAL INFORMATION GAIN ABOVE analysis :
#glm = smf.glm(formula='SOFA_MEAN ~ RESP_MEAN + ABPMEAN_MEAN + TEMP_MEAN + SPO2_MIN + ABPDIAS_MIN +ABPDIAS_MAX', data = x_train, family = sm.families.Poisson())
"""
FOR TOP 6 FEATURES : RESP_MEAN + ABPMEAN_MEAN + TEMP_MEAN + SPO2_MIN + ABPDIAS_MIN +ABPDIAS_MAX
ROOT MEAN SQUARE ERROR :  1.0627290365628288
RSS =  86.96326139683937
R2 =  0.04336194497758605

"""

#Below for all +ve coefficents:
#glm = smf.glm(formula='SOFA_MEAN ~  HR_MEAN + RESP_MEAN+ RESP_VAR + ABPDIAS_VAR + ABPMEAN_VAR + HR_MAX + SPO2_MIN + SPO2_MAX + TEMP_MAX + ABPSYS_MIN + ABPMEAN_MIN', data = x_train, family = sm.families.Poisson())
"""
all +ve features: HR_MEAN + RESP_MEAN+ RESP_VAR + ABPDIAS_VAR + ABPMEAN_VAR + HR_MAX + SPO2_MIN + SPO2_MAX + TEMP_MAX + ABPSYS_MIN + ABPMEAN_MIN

ROOT MEAN SQUARE ERROR :  1.0778894778948676
RSS =  89.46212094484821
R2 =  0.015873277931478302
"""


glm_regression_model = glm.fit()

print(glm_regression_model.summary())

#print(glm_regression_model.params)


""""

importance = glm_regression_model.params
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
pyplot.bar([x for x in range(len(importance))], importance)
pyplot.show()
"""

#testing on sofa regression 
regression_test = x_test[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'SOFA_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]


predictions = glm_regression_model.predict(regression_test)
#print(predictions)


actual_sofa = x_test['SOFA_MEAN']

rms = np.sqrt(mean_squared_error(actual_sofa, predictions))
print('ROOT MEAN SQUARE ERROR : ',rms )
x_test['PRED_SOFA_MEAN'] = predictions

print("RSS = ", ((x_test.SOFA_MEAN - x_test.PRED_SOFA_MEAN)**2).sum())
#print("R2 = ", glm_regression_model.rsquared)


SS_Residual = sum((x_test.SOFA_MEAN - x_test.PRED_SOFA_MEAN)**2)       
SS_Total = sum((x_test.SOFA_MEAN-np.mean(x_test.SOFA_MEAN))**2)     
r_squared = 1 - (float(SS_Residual))/SS_Total
print("R2 = ", r_squared)

"""# **TO PERFORM RECURSIVE FEATURE ELIMINATION ON XGBOOST AND THEN USE THE MOST IMPORTANT FEATURES TO DO A REGRESSION + BIONOMIAL CLASSIFICATION** """

# do a sofa_mean regression on entire dataset and attach replace it with the original sofa_mmean

import pandas
from sklearn.model_selection import KFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVR
import numpy as np

X = df_features[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]
     

X = X.apply(pd.to_numeric)
X = np.asarray(X)
X =  X.astype(int)


y = df_features['SOFA_MEAN']
y =  y.apply(pd.to_numeric)
y =  y.astype(int)

y = np.asarray(y)

from sklearn.feature_selection import RFECV

estimator = XGBRegressor()
selector = RFECV(estimator, step=1, cv=5,scoring='neg_root_mean_squared_error')

selector = selector.fit(X, y)
print(selector.support_ )
print("Optimal number of features : %d" % selector.n_features_)


print(selector.ranking_ ) 
print(selector)
"""
#[5 1 1 1 1 1 4 1 1 3 2 1 1 6 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
1, 7,10,11, 14

df_features[['RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN',
        'HR_VAR', 'RESP_VAR',
       'ABPDIAS_VAR', 'ABPMEAN_VAR', 
       'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]

"""

# performin xg_boost with top 23 on training and test dataset

from sklearn.model_selection import train_test_split
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor


"""
For top 5 features  : 'ABPSYS_MEAN', 'RESP_VAR',  'TEMP_VAR', 'ABPSYS_MIN' ,'ABPMEAN_MIN'
ROOT MEAN SQUARE ERROR :  1.1458710519554571
RSS =  101.10257601363195
R2 =  -0.10808386560390293

"""

"""
for top 13 features : 'ABPSYS_MEAN', 'RESP_VAR',  'TEMP_VAR', 'ABPSYS_MIN' ,'ABPMEAN_MIN', 'RESP_MEAN','HR_VAR', 'ABPMEAN_VAR', 'RESP_MIN', 'SPO2_MIN',  'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX'

ROOT MEAN SQUARE ERROR :  1.144962312506987
RSS =  100.94227967372376
R2 =  -0.10920652972127298
"""
x = df_features[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'SOFA_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]


y = df_features['HAS_SHOCK']   

x = x.apply(pd.to_numeric)

y = y.apply(pd.to_numeric)



x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=42)

x_train_xgb = x_train[['RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN',
        'HR_VAR', 'RESP_VAR',
       'ABPDIAS_VAR', 'ABPMEAN_VAR', 
       'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]

x_test_xgb = x_test[['RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN',
        'HR_VAR', 'RESP_VAR',
       'ABPDIAS_VAR', 'ABPMEAN_VAR', 
       'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]




model = XGBRegressor(objective='reg:linear')
# fit the model
XGB_model = model.fit(x_train_xgb, y_train)

#testing on sofa regression 
#regression_test = x_test[['ABPSYS_MEAN', 'RESP_VAR',  'TEMP_VAR', 'ABPSYS_MIN' ,'ABPMEAN_MIN']]


predictions = XGB_model.predict(x_test_xgb)
#print(predictions)


actual_sofa = x_test['SOFA_MEAN']
rms = np.sqrt(mean_squared_error(actual_sofa, predictions))
print('ROOT MEAN SQUARE ERROR : ',rms )


print("RSS = ", ((actual_sofa - predictions)**2).sum())
#print("R2 = ", glm_regression_model.rsquared)


SS_Residual = sum((actual_sofa - predictions)**2)       
SS_Total = sum((actual_sofa - np.mean(predictions))**2)     
r_squared = 1 - (float(SS_Residual))/SS_Total
print("R2 = ", r_squared)

print(x_train.columns)

# building GLM Bionomial model and training it
import statsmodels.api as sm

y_train =  y_train.astype(int)
"""
x_train = x_train[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'SOFA_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]
"""
x_train = x_train[['RESP_MEAN', 'SOFA_MEAN', 'TEMP_VAR','RESP_MIN','SPO2_MAX']]
x_train = np.asarray(x_train)
#x_train = MinMaxScaler().fit_transform(x_train)


glm_bionomial = sm.GLM(y_train, x_train, family=sm.families.Binomial())
glm_bio_model = glm_bionomial.fit()

print(glm_bio_model.summary())

"""
importance = glm_bio_model.params
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
pyplot.bar([x for x in range(len(importance))], importance)
pyplot.show()
"""
#IMP FEATURES : 'RESP_MEAN', 'SOFA_MEAN', 'TEMP_VAR','RESP_MIN','SPO2_MAX'
"""
'HR_MEAN',  --1 
'RESP_MEAN', --2
'ABPSYS_MEAN', --3 
'ABPDIAS_MEAN', --4
'ABPMEAN_MEAN',--5
'SPO2_MEAN',--6
 'TEMP_MEAN', --7
 'SOFA_MEAN', --8
 'HR_VAR', --9
 'RESP_VAR', --10
 'SPO2_VAR',--11
'ABPSYS_VAR', --12
'ABPDIAS_VAR',  ---13
'ABPMEAN_VAR', --14
'TEMP_VAR', --15
'HR_MIN',--16
'HR_MAX', --17
'RESP_MIN',--18
'RESP_MAX', ---19
'SPO2_MIN', --20
'SPO2_MAX', --21
'TEMP_MIN', --22
'TEMP_MAX', --23
'ABPSYS_MIN', --24
'ABPSYS_MAX', --25
'ABPDIAS_MIN', --26
'ABPDIAS_MAX',--27
 'ABPMEAN_MIN', --28
  'ABPMEAN_MAX' --29

"""

x_test['SOFA_MEAN'] = predictions

print(x_test)

print(x_test.shape)
print(x_train.shape)


#x_test['PREDCITED_SOFA'] = predictions

x_test = x_test[['HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'SOFA_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]

x_test = x_test[['RESP_MEAN', 'SOFA_MEAN', 'TEMP_VAR','RESP_MIN','SPO2_MAX']]
               

#x_test = MinMaxScaler().fit_transform(x_test)

y_test= y_test.astype(int)
print(y_test.shape)

#testing on bionomial regression for classification
Class_predictions = glm_bio_model.predict(x_test)
print(glm_bio_model.model.endog_names)

print(Class_predictions)
print(y_test)

from sklearn import metrics
from matplotlib import pyplot
from numpy import sqrt
from numpy import argmax
from sklearn.metrics import precision_recall_curve
print(y_test)
print(Class_predictions)
y_test= y_test.astype(int)

fpr, tpr, thresholds = metrics.roc_curve(y_test, Class_predictions)
pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')
pyplot.plot(fpr, tpr, marker='.', label='Logistic')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
pyplot.legend()
# show the plot
pyplot.show()
gmeans = sqrt(tpr * (1-fpr))
ix = argmax(gmeans)
print('Best Threshold according to the G-Mean=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))

########## calculating best threshold according to the best F1-score:

precision, recall, thresholds = precision_recall_curve(y_test, Class_predictions)
# convert to f score
fscore = (2 * precision * recall) / (precision + recall)
# locate the index of the largest f score
ix = argmax(fscore)
print('Best Threshold according to the F-Score=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))


########## calculating best threshold according to the  Youden’s J statistic.:
fpr, tpr, thresholds = metrics.roc_curve(y_test, Class_predictions)
J = tpr - fpr
ix = argmax(J)
best_thresh = thresholds[ix]
print('Best Threshold according to the Youden’s J statistic=%f' % (best_thresh))

from sklearn.metrics import confusion_matrix, classification_report
y_pred = [ 0 if x < 0.07706 else 1 for x in Class_predictions]
print(classification_report(y_test, 
                            y_pred, 
                            digits = 3))
print(y_test)
print(y_pred)

from sklearn import metrics
import seaborn as sns
cnf_matrix = metrics.confusion_matrix(y_test, y_pred)

class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("Precision:",metrics.precision_score(y_test, y_pred))
print("Recall:",metrics.recall_score(y_test, y_pred))
print("F-Score:",metrics.f1_score(y_test, y_pred))

"""# **trying different features with different models on 10 fold cross validation**


1.   Best match in GLM according to backward eng.
2.   Correlation analysis (top 5 and 7 features)
3.   Mutual info analysis 
4.   all positive
5.   XGBoost (top 5)
6.   XGBoost (top 13)
7.   Permutation importance with XGBOOST
8.   Permutation importance with GLM




"""

# trying out Best match in GLM according to backward eng. Correlation analysis (top 5 and 7 features) , Mutual info analysis, 
# all positive. , Permutation importance with GLM

from sklearn.model_selection import KFold
import statsmodels.api as sm
import statsmodels.formula.api as smf

from sklearn.model_selection import train_test_split

from sklearn.metrics import mean_squared_error


x = df_features[['RESP_MEAN' ,  'ABPSYS_MEAN',  'ABPMEAN_MEAN',  'RESP_VAR', 'TEMP_VAR', 'ABPDIAS_MIN', 'ABPMEAN_MIN']]

y = df_features['SOFA_MEAN']


rms_list = []
rss_list = [] ;
r2_list= [];


x = x.apply(pd.to_numeric)
x = np.asarray(x)

y = y.apply(pd.to_numeric)
y = np.asarray(y)

kf = KFold(n_splits=10)

KFold(n_splits=10, shuffle=False)
for train_index, test_index in kf.split(x):
    x_train, x_test = x[train_index], x[test_index]
    y_train, y_test = y[train_index], y[test_index]
    actual_sofa.append( y_train)
    #best :

    glm = sm.GLM(y_train, x_train, families= sm.families.Poisson() )
    glm_Reg_model= glm.fit() 
    predictions = glm_Reg_model.predict(x_test)
    

    
    rms = np.sqrt(mean_squared_error(y_test, predictions))
    rms_list.append(rms)


    #print("R2 = ", glm_regression_model.rsquared)
    rss_list.append(((y_test - predictions)**2).sum())

    SS_Residual = sum((y_test - predictions)**2)       
    SS_Total = sum((y_test - np.mean(predictions))**2)     
    r_squared = 1 - (float(SS_Residual))/SS_Total
    r2_list.append(r_squared)    

  

rms_list = np.asarray(rms_list) 
rss_list = np.asarray(rss_list) 
r2_list = np.asarray(r2_list) 


print('RMS : ' , rms_list.mean())
print('RSS : ' , rss_list.mean())
print('R2 : ' , r2_list.mean())

# trying out XGBoost (top 5) , XGBoost (top 13) , Permutation importance with XGBOOST

from sklearn.model_selection import KFold
from xgboost import XGBRegressor
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

"""
"ABPSYS_MEAN , RESP_VAR , TEMP_VAR , ABPSYS_MIN , ABPMEAN_MIN,
RESP_MEAN ,HR_VAR, ABPMEAN_VAR, RESP_MIN, SPO2_MIN,  ABPSYS_MAX, ABPDIAS_MIN, ABPDIAS_MAX"

RMS :  1.1996537549937705
RSS :  38.20318959790511
R2 :  -0.14192783816779714


RMS :  1.0638164040843112
RSS :  30.06284363929759
R2 :  0.11298394966493794
"""




x = df_features[['ABPSYS_MEAN' , 'RESP_VAR' , 'TEMP_VAR' , 'ABPSYS_MIN' , 'ABPMEAN_MIN',
'RESP_MEAN' ,'HR_VAR','ABPMEAN_VAR', 'RESP_MIN', 'SPO2_MIN',  'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX']]

y = df_features['SOFA_MEAN']


rms_list = []
rss_list = [] ;
r2_list= [];


x = x.apply(pd.to_numeric)
x = np.asarray(x)

y = y.apply(pd.to_numeric)
y = np.asarray(y)

kf = KFold(n_splits=10)
print(kf)  

KFold(n_splits=10, shuffle=False)
for train_index, test_index in kf.split(x):
    x_train, x_test = x[train_index], x[test_index]
    y_train, y_test = y[train_index], y[test_index]
    #best :

    xgb = XGBRegressor()
    xgb_model= xgb.fit(x_train,y_train) 
    predictions = xgb_model.predict(x_test)
    

    
    rms = np.sqrt(mean_squared_error(y_test, predictions))
    rms_list.append(rms)


    #print("R2 = ", glm_regression_model.rsquared)
    rss_list.append(((y_test - predictions)**2).sum())

    SS_Residual = sum((y_test - predictions)**2)       
    SS_Total = sum((y_test - np.mean(predictions))**2)     
    r_squared = 1 - (float(SS_Residual))/SS_Total
    r2_list.append(r_squared)    

  

rms_list = np.asarray(rms_list) 
rss_list = np.asarray(rss_list) 
r2_list = np.asarray(r2_list) 


print('RMS : ' , rms_list.mean())
print('RSS : ' , rss_list.mean())
print('R2 : ' , r2_list.mean())

import pandas as pd
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

print(df_features)

"""# **K-FOLD CROSS VALIDATIONt**

# **knowing the fact that xgboost with tp 13 features performs the best**
# **test GLM and XGBOOST classification algos with XGBOOST regression**
"""

# trying out XGBoost Regression + GLM classification

from sklearn.model_selection import KFold
from xgboost import XGBRegressor
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn import metrics
from matplotlib import pyplot
from numpy import sqrt
from numpy import argmax
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import confusion_matrix, classification_report

import pandas as pd
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

x = df_features[['SOFA_MEAN','HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]

y = df_features['HAS_SHOCK']

x_regression = x[['ABPSYS_MEAN' , 'RESP_VAR' , 'TEMP_VAR' , 'ABPSYS_MIN' , 'ABPMEAN_MIN',
'RESP_MEAN' ,'HR_VAR','ABPMEAN_VAR', 'RESP_MIN', 'SPO2_MIN',  'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX']]

y_regression = x['SOFA_MEAN']

# for regression
rms_list = []
rss_list = [] ;
r2_list= [];

#for classification:
acucracy_list = []
precision_list = [] ;
recall_list= [];
fscore_list = [];


x_regression = x_regression.apply(pd.to_numeric)
x_regression = np.asarray(x_regression)

y_regression = y_regression.apply(pd.to_numeric)
y_regression = np.asarray(y_regression)



x = x.apply(pd.to_numeric)
x = np.asarray(x)

y = y.apply(pd.to_numeric)
y = np.asarray(y)


kf = KFold(n_splits=10)
#print(kf)  

KFold(n_splits=2, shuffle=False)
for train_index, test_index in kf.split(x):
    x_train_reg, x_test_reg = x_regression[train_index], x_regression[test_index]
    y_train_reg, y_test_reg = y_regression[train_index], y_regression[test_index]

    x_train_class, x_test_class = x[train_index], x[test_index]
    y_train_class, y_test_class = y[train_index], y[test_index]
    

    #best :

    xgb = XGBRegressor()
    xgb_model= xgb.fit(x_train_reg,y_train_reg) 
    predictions = xgb_model.predict(x_test_reg)

    
    
    rms = np.sqrt(mean_squared_error(y_test_reg, predictions))
    rms_list.append(rms)


    #print("R2 = ", glm_regression_model.rsquared)
    rss_list.append(((y_test_reg - predictions)**2).sum())

    SS_Residual = sum((y_test_reg - predictions)**2)       
    SS_Total = sum((y_test_reg - np.mean(predictions))**2)     
    r_squared = 1 - (float(SS_Residual))/SS_Total
    r2_list.append(r_squared)   

    #print('printing X TEST')
    #print(x_test_class)
    #print(x_test_class.shape)
    x_test_class = np.array(x_test_class)
    x_test_class[:,0] = predictions
    #print(x_test_class)
    #print(x_test_class[:,0]) 

    #print(x_train_class)
    
    #GLM classification 
    glm_bionomial = sm.GLM(y_train_class, x_train_class, family=sm.families.Binomial())
    glm_bio_model = glm_bionomial.fit()
    #x_test_class =  np.asarray(x_test_class)
    #print(x_test_class.shape)
    class_pred_prob = glm_bio_model.predict(x_test_class)
    
    #############
    
    y_test_class = y_test_class.astype(int)

    fpr, tpr, thresholds = metrics.roc_curve(y_test_class, class_pred_prob)
    gmeans = sqrt(tpr * (1-fpr))
    ix = argmax(gmeans)
    #print('Best Threshold according to the G-Mean=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))

    y_pred_class = [ 0 if x < thresholds[ix] else 1 for x in class_pred_prob]

    acucracy_list.append(metrics.accuracy_score(y_test_class, y_pred_class))
    precision_list.append(metrics.precision_score(y_test_class, y_pred_class))
    recall_list.append(metrics.recall_score(y_test_class, y_pred_class))
    fscore_list.append(metrics.f1_score(y_test_class, y_pred_class))

  
rms_list = np.asarray(rms_list) 
rss_list = np.asarray(rss_list) 
r2_list = np.asarray(r2_list) 

acucracy_list = np.asarray(acucracy_list) 
precision_list = np.asarray(precision_list) 
recall_list = np.asarray(recall_list) 
fscore_list = np.asarray(fscore_list) 



print('RMS : ' , rms_list.mean())
print('RSS : ' , rss_list.mean())
print('R2 : ' , r2_list.mean())

print("Accuracy: ", acucracy_list.mean())
print("Precision: " , precision_list.mean())
print("Recall: " , recall_list.mean())
print("F-Score: ", fscore_list.mean())
   
###### result
"""
RMS :  1.5965682434393136
RSS :  63.89951721995595
R2 :  -0.08214921245562042
Accuracy:  0.7378333333333333
Precision:  0.6590404040404041
Recall:  0.633430735930736
F-Score:  0.6214946894089443



RMS :  1.6006011836589713
RSS :  63.7920592619145
R2 :  0.014668786177857784
Accuracy:  0.693
Precision:  0.5975974025974027
Recall:  0.5909271284271285
F-Score:  0.5680162080781276
"""

# trying out XGBoost regression + XGBOOST CLASSIFICATION

from sklearn.model_selection import KFold
from xgboost import XGBRegressor, XGBClassifier, plot_importance
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn import metrics
from matplotlib import pyplot
from numpy import sqrt
from numpy import argmax
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.feature_selection import SelectFromModel
from matplotlib import pyplot




x = df_features[['SOFA_MEAN','HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]

y = df_features['HAS_SHOCK']

x_regression = x[['ABPSYS_MEAN' , 'RESP_VAR' , 'TEMP_VAR' , 'ABPSYS_MIN' , 'ABPMEAN_MIN',
'RESP_MEAN' ,'HR_VAR','ABPMEAN_VAR', 'RESP_MIN', 'SPO2_MIN',  'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX']]

y_regression = x['SOFA_MEAN']

# for regression
rms_list = []
rss_list = [] ;
r2_list= [];

#for classification:
acucracy_list = []
precision_list = [] ;
recall_list= [];
fscore_list = [];


x_regression = x_regression.apply(pd.to_numeric)
x_regression = np.asarray(x_regression)

y_regression = y_regression.apply(pd.to_numeric)
y_regression = np.asarray(y_regression)



x = x.apply(pd.to_numeric)
x = np.asarray(x)

y = y.apply(pd.to_numeric)
y = np.asarray(y)


kf = KFold(n_splits=10)
#print(kf)  

KFold(n_splits=2, shuffle=False)
for train_index, test_index in kf.split(x):
    x_train_reg, x_test_reg = x_regression[train_index], x_regression[test_index]
    y_train_reg, y_test_reg = y_regression[train_index], y_regression[test_index]

    x_train_class, x_test_class = x[train_index], x[test_index]
    y_train_class, y_test_class = y[train_index], y[test_index]
    

    #best :

    xgb = XGBRegressor()
    xgb_model= xgb.fit(x_train_reg,y_train_reg) 
    predictions = xgb_model.predict(x_test_reg)

    
    
    rms = np.sqrt(mean_squared_error(y_test_reg, predictions))
    rms_list.append(rms)


    #print("R2 = ", glm_regression_model.rsquared)
    rss_list.append(((y_test_reg - predictions)**2).sum())

    SS_Residual = sum((y_test_reg - predictions)**2)       
    SS_Total = sum((y_test_reg - np.mean(predictions))**2)     
    r_squared = 1 - (float(SS_Residual))/SS_Total
    r2_list.append(r_squared)   

    #print('printing X TEST')
    #print(x_test_class)
    #print(x_test_class.shape)
    x_test_class = np.array(x_test_class)
    x_test_class[:,0] = predictions
    #print(x_test_class)
    #print(x_test_class[:,0]) 

    
    #XGBOOST classification 
    xgb = XGBClassifier()
    xgb_model = xgb.fit(x_train_class, y_train_class)  
    #print('PRINTING PROBABILITIES') 
    #print(xgb_model.predict_proba(x_test_class))

    y_pred_class = xgb_model.predict(x_test_class)
    """
    print(xgb.feature_importances_)
    plot_importance(xgb)
    pyplot.show()
    # select features using threshold
    selection = SelectFromModel(xgb, threshold=0.025, prefit=True)
    select_x_train_class = selection.transform(x_train_class)

    # train model with important features
    selection_model = XGBClassifier()
    selection_model.fit(select_x_train_class, y_train_class)
    # eval model with important features
    select_x_test_class = selection.transform(x_test_class)
    y_pred_class = selection_model.predict(select_x_test_class) 

    """
    #print('PRINTING PREDICTED CLASS')
    #print(class_pred)
    
    y_test_class = y_test_class.astype(int)
    y_pred_class = y_pred_class.astype(int)

    acucracy_list.append(metrics.accuracy_score(y_test_class, y_pred_class))
    precision_list.append(metrics.precision_score(y_test_class, y_pred_class))
    recall_list.append(metrics.recall_score(y_test_class, y_pred_class))
    fscore_list.append(metrics.f1_score(y_test_class, y_pred_class))

  
rms_list = np.asarray(rms_list) 
rss_list = np.asarray(rss_list) 
r2_list = np.asarray(r2_list) 

acucracy_list = np.asarray(acucracy_list) 
precision_list = np.asarray(precision_list) 
recall_list = np.asarray(recall_list) 
fscore_list = np.asarray(fscore_list) 



print('RMS : ' , rms_list.mean())
print('RSS : ' , rss_list.mean())
print('R2 : ' , r2_list.mean())

print("Accuracy: ", acucracy_list.mean())
print("Precision: " , precision_list.mean())
print("Recall: " , recall_list.mean())
print("F-Score: ", fscore_list.mean())
"""
###### result
without selectinng immportant features

RMS :  1.0638164040843112
RSS :  30.06284363929759
R2 :  0.11298394966493794
Accuracy:  0.7653846153846153
Precision:  0.7431457431457431
Recall:  0.5633674658674658
F-Score:  0.6305116520905993


with selecting top 13 features that are used in regression

RMS :  1.0638164040843112
RSS :  30.06284363929759
R2 :  0.11298394966493794
Accuracy:  0.7696923076923077
Precision:  0.7483333333333333
Recall:  0.5970604395604395
F-Score:  0.651086194507247


"""



"""# **truing out BEST GLM regressor with GLM CLassification and XGBoost classification**"""

# trying out GLM Regression + GLM classification

from sklearn.model_selection import KFold
from xgboost import XGBRegressor
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn import metrics
from matplotlib import pyplot
from numpy import sqrt
from numpy import argmax
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import confusion_matrix, classification_report

import pandas as pd
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)


###########



##########

x = df_features[['SOFA_MEAN','HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]

y = df_features['HAS_SHOCK']

x_regression = x[['SPO2_VAR' ,  'RESP_MEAN' , 'ABPSYS_MEAN' , 'ABPDIAS_MEAN' ,'ABPMEAN_MEAN' ,'SPO2_MEAN' ,'HR_MAX' ,'RESP_MIN', 'SPO2_MIN' ,'TEMP_MIN', 'ABPSYS_MIN', 'ABPSYS_MAX','ABPDIAS_MIN','ABPDIAS_MAX', 'ABPMEAN_MIN', 'ABPMEAN_MAX']]

y_regression = x['SOFA_MEAN']

# for regression
rms_list = []
rss_list = [] ;
r2_list= [];

#for classification:
acucracy_list = []
precision_list = [] ;
recall_list= [];
fscore_list = [];


x_regression = x_regression.apply(pd.to_numeric)
x_regression = np.asarray(x_regression)

y_regression = y_regression.apply(pd.to_numeric)
y_regression = np.asarray(y_regression)



x = x.apply(pd.to_numeric)
x = np.asarray(x)

y = y.apply(pd.to_numeric)
y = np.asarray(y)


kf = KFold(n_splits=10)
#print(kf)  

KFold(n_splits=2, shuffle=False)
for train_index, test_index in kf.split(x):
    x_train_reg, x_test_reg = x_regression[train_index], x_regression[test_index]
    y_train_reg, y_test_reg = y_regression[train_index], y_regression[test_index]

    x_train_class, x_test_class = x[train_index], x[test_index]
    y_train_class, y_test_class = y[train_index], y[test_index]
    

    #best :

    glm = sm.GLM(y_train_reg, x_train_reg, families= sm.families.Poisson() )
    glm_Reg_model= glm.fit() 
    predictions = glm_Reg_model.predict(x_test_reg)

    
    
    rms = np.sqrt(mean_squared_error(y_test_reg, predictions))
    rms_list.append(rms)


    #print("R2 = ", glm_regression_model.rsquared)
    rss_list.append(((y_test_reg - predictions)**2).sum())

    SS_Residual = sum((y_test_reg - predictions)**2)       
    SS_Total = sum((y_test_reg - np.mean(predictions))**2)     
    r_squared = 1 - (float(SS_Residual))/SS_Total
    r2_list.append(r_squared)   

    #print('printing X TEST')
    #print(x_test_class)
    #print(x_test_class.shape)
    x_test_class = np.array(x_test_class)
    x_test_class[:,0] = predictions
    #print(x_test_class)
    #print(x_test_class[:,0]) 

    #print(x_train_class)
    
    #GLM classification 
    glm_bionomial = sm.GLM(y_train_class, x_train_class, family=sm.families.Binomial())
    glm_bio_model = glm_bionomial.fit()
    #x_test_class =  np.asarray(x_test_class)
    #print(x_test_class.shape)
    class_pred_prob = glm_bio_model.predict(x_test_class)
    
    #############
    
    y_test_class = y_test_class.astype(int)

    fpr, tpr, thresholds = metrics.roc_curve(y_test_class, class_pred_prob)
    gmeans = sqrt(tpr * (1-fpr))
    ix = argmax(gmeans)
    #print('Best Threshold according to the G-Mean=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))

    y_pred_class = [ 0 if x < thresholds[ix] else 1 for x in class_pred_prob]

    acucracy_list.append(metrics.accuracy_score(y_test_class, y_pred_class))
    precision_list.append(metrics.precision_score(y_test_class, y_pred_class))
    recall_list.append(metrics.recall_score(y_test_class, y_pred_class))
    fscore_list.append(metrics.f1_score(y_test_class, y_pred_class))

  
rms_list = np.asarray(rms_list) 
rss_list = np.asarray(rss_list) 
r2_list = np.asarray(r2_list) 

acucracy_list = np.asarray(acucracy_list) 
precision_list = np.asarray(precision_list) 
recall_list = np.asarray(recall_list) 
fscore_list = np.asarray(fscore_list) 



print('RMS : ' , rms_list.mean())
print('RSS : ' , rss_list.mean())
print('R2 : ' , r2_list.mean())

print("Accuracy: ", acucracy_list.mean())
print("Precision: " , precision_list.mean())
print("Recall: " , recall_list.mean())
print("F-Score: ", fscore_list.mean())
   
###### result
"""
RMS :  1.0638164040843112
RSS :  30.06284363929759
R2 :  0.11298394966493794
Accuracy:  0.7775384615384616
Precision:  0.7191117216117215
Recall:  0.7268581418581418
F-Score:  0.7053614522732169

"""

# trying out GLM Regression + XGBOOST classification



from sklearn.model_selection import KFold
from xgboost import XGBRegressor, XGBClassifier, plot_importance
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn import metrics
from matplotlib import pyplot
from numpy import sqrt
from numpy import argmax
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.feature_selection import SelectFromModel
from matplotlib import pyplot




x = df_features[['SOFA_MEAN','HR_MEAN', 'RESP_MEAN', 'ABPSYS_MEAN', 'ABPDIAS_MEAN', 'ABPMEAN_MEAN',
       'SPO2_MEAN', 'TEMP_MEAN', 'HR_VAR', 'RESP_VAR', 'SPO2_VAR',
       'ABPSYS_VAR', 'ABPDIAS_VAR', 'ABPMEAN_VAR', 'TEMP_VAR', 'HR_MIN',
       'HR_MAX', 'RESP_MIN', 'RESP_MAX', 'SPO2_MIN', 'SPO2_MAX', 'TEMP_MIN',
       'TEMP_MAX', 'ABPSYS_MIN', 'ABPSYS_MAX', 'ABPDIAS_MIN', 'ABPDIAS_MAX',
       'ABPMEAN_MIN', 'ABPMEAN_MAX']]

y = df_features['HAS_SHOCK']

x_regression = x[['SPO2_VAR' ,  'RESP_MEAN' , 'ABPSYS_MEAN' , 'ABPDIAS_MEAN' ,'ABPMEAN_MEAN' ,'SPO2_MEAN' ,'HR_MAX' ,'RESP_MIN', 'SPO2_MIN' ,'TEMP_MIN', 'ABPSYS_MIN', 'ABPSYS_MAX','ABPDIAS_MIN','ABPDIAS_MAX', 'ABPMEAN_MIN', 'ABPMEAN_MAX']]


y_regression = x['SOFA_MEAN']

# for regression
rms_list = []
rss_list = [] ;
r2_list= [];

#for classification:
acucracy_list = []
precision_list = [] ;
recall_list= [];
fscore_list = [];


x_regression = x_regression.apply(pd.to_numeric)
x_regression = np.asarray(x_regression)

y_regression = y_regression.apply(pd.to_numeric)
y_regression = np.asarray(y_regression)



x = x.apply(pd.to_numeric)
x = np.asarray(x)

y = y.apply(pd.to_numeric)
y = np.asarray(y)


kf = KFold(n_splits=10)
#print(kf)  

KFold(n_splits=2, shuffle=False)
for train_index, test_index in kf.split(x):
    x_train_reg, x_test_reg = x_regression[train_index], x_regression[test_index]
    y_train_reg, y_test_reg = y_regression[train_index], y_regression[test_index]

    x_train_class, x_test_class = x[train_index], x[test_index]
    y_train_class, y_test_class = y[train_index], y[test_index]
    

    #best :

    glm = sm.GLM(y_train_reg, x_train_reg, families= sm.families.Poisson() )
    glm_Reg_model= glm.fit() 
    predictions = glm_Reg_model.predict(x_test_reg)
    
    
    rms = np.sqrt(mean_squared_error(y_test_reg, predictions))
    rms_list.append(rms)


    #print("R2 = ", glm_regression_model.rsquared)
    rss_list.append(((y_test_reg - predictions)**2).sum())

    SS_Residual = sum((y_test_reg - predictions)**2)       
    SS_Total = sum((y_test_reg - np.mean(predictions))**2)     
    r_squared = 1 - (float(SS_Residual))/SS_Total
    r2_list.append(r_squared)   

    #print('printing X TEST')
    #print(x_test_class)
    #print(x_test_class.shape)
    x_test_class = np.array(x_test_class)
    x_test_class[:,0] = predictions
    #print(x_test_class)
    #print(x_test_class[:,0]) 

    
    #XGBOOST classification 
    xgb = XGBClassifier()
    xgb_model = xgb.fit(x_train_class, y_train_class)  
    #print('PRINTING PROBABILITIES') 
    #print(xgb_model.predict_proba(x_test_class))

    y_pred_class = xgb_model.predict(x_test_class)
    """
    print(xgb.feature_importances_)
    plot_importance(xgb)
    pyplot.show()
    # select features using threshold
    selection = SelectFromModel(xgb, threshold=0.025, prefit=True)
    select_x_train_class = selection.transform(x_train_class)

    # train model with important features
    selection_model = XGBClassifier()
    selection_model.fit(select_x_train_class, y_train_class)
    # eval model with important features
    select_x_test_class = selection.transform(x_test_class)
    y_pred_class = selection_model.predict(select_x_test_class) 

    """
    #print('PRINTING PREDICTED CLASS')
    #print(class_pred)
    
    y_test_class = y_test_class.astype(int)
    y_pred_class = y_pred_class.astype(int)

    acucracy_list.append(metrics.accuracy_score(y_test_class, y_pred_class))
    precision_list.append(metrics.precision_score(y_test_class, y_pred_class))
    recall_list.append(metrics.recall_score(y_test_class, y_pred_class))
    fscore_list.append(metrics.f1_score(y_test_class, y_pred_class))

  
rms_list = np.asarray(rms_list) 
rss_list = np.asarray(rss_list) 
r2_list = np.asarray(r2_list) 

acucracy_list = np.asarray(acucracy_list) 
precision_list = np.asarray(precision_list) 
recall_list = np.asarray(recall_list) 
fscore_list = np.asarray(fscore_list) 



print('RMS : ' , rms_list.mean())
print('RSS : ' , rss_list.mean())
print('R2 : ' , r2_list.mean())

print("Accuracy: ", acucracy_list.mean())
print("Precision: " , precision_list.mean())
print("Recall: " , recall_list.mean())
print("F-Score: ", fscore_list.mean())
"""
###### result
without selectinng immportant features

RMS :  1.0638164040843112
RSS :  30.06284363929759
R2 :  0.11298394966493794
Accuracy:  0.7653846153846153
Precision:  0.7431457431457431
Recall:  0.5633674658674658
F-Score:  0.6305116520905993


with selecting top 13 features that are used in regression

RMS :  1.0638164040843112
RSS :  30.06284363929759
R2 :  0.11298394966493794
Accuracy:  0.7696923076923077
Precision:  0.7483333333333333
Recall:  0.5970604395604395
F-Score:  0.651086194507247


"""